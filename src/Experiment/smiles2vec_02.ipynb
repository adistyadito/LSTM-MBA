{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9049658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 14:21:38.491555: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-29 14:21:38.550788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 14:21:40.315853: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, \n",
    "    BatchNormalization, Dropout, Lambda, LSTM, Reshape\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dc40d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label:\n",
      "target\n",
      "0    321\n",
      "1    315\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv')\n",
    "df['smiles'] = df['smiles'].str.upper()\n",
    "\n",
    "def encode_label(cat):\n",
    "    if cat == \"inhibitor\":\n",
    "        return 1\n",
    "    elif cat == \"neutral\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(f\"Label tidak dikenal: {cat}\")\n",
    "\n",
    "df['target'] = df['categories'].apply(encode_label)\n",
    "\n",
    "print(\"Distribusi label:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1500f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")\n",
    "y_train = df_train['target'].values.astype(np.float32)\n",
    "y_test = df_test['target'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f23b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters in test set: 0\n",
      "Truncated test SMILES: 0\n"
     ]
    }
   ],
   "source": [
    "all_chars = set(''.join(df_train['smiles']))\n",
    "charset = ['!', 'E', '?', '_'] + sorted(all_chars)\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "vocab_size = len(charset)\n",
    "max_len = max(len(s) for s in df_train['smiles']) + 2\n",
    "\n",
    "def encode_smiles(smiles_list):\n",
    "    sequences = []\n",
    "    for s in smiles_list:\n",
    "        seq = [char_to_int['!']]\n",
    "        for c in s:\n",
    "            seq.append(char_to_int.get(c, char_to_int['?']))\n",
    "        seq.append(char_to_int['E'])\n",
    "        if len(seq) < max_len:\n",
    "            seq += [char_to_int['_']] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences, dtype=np.int32)\n",
    "\n",
    "X_train = encode_smiles(df_train['smiles'])\n",
    "X_test = encode_smiles(df_test['smiles'])\n",
    "\n",
    "unknown_count = np.sum(X_test == char_to_int['?'])\n",
    "print(f\"Unknown characters in test set: {unknown_count}\")\n",
    "\n",
    "truncated = sum(len(s) + 2 > max_len for s in df_test['smiles'])\n",
    "print(f\"Truncated test SMILES: {truncated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'conv1d_8' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_28\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_28\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m20,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m20,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,008</span> (246.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,008\u001b[0m (246.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,880</span> (245.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,880\u001b[0m (245.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = Input(shape=(max_len,))\n",
    "x = Embedding(vocab_size, 50, mask_zero=True)(inputs)\n",
    "x = Conv1D(64, 10, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv1D(64, 5, activation='relu', padding='same')(x)\n",
    "x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "embedding = Dense(128, activation='relu')(x)\n",
    "\n",
    "encoder = Model(inputs=inputs, outputs=embedding)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a836e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "train_emb = encoder.predict(X_train, verbose=1)\n",
    "test_emb = encoder.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce664083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4980 - auc_5: 0.5000 - loss: 0.7258 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7251 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7246 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7239 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7233 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7227 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7221 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7215 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7210 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7203 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7198 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7192 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7188 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7182 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7177 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7172 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5020 - auc_5: 0.5000 - loss: 0.7167 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7162 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7158 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7153 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7149 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7144 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7139 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7135 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7131 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7126 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7123 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7118 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7115 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7111 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7107 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7103 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7100 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7096 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7093 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7089 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7086 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7083 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7080 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7076 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7074 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7070 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7068 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7064 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7062 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7059 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7056 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7053 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7051 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7048 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7046 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7043 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7041 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7039 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7036 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7034 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7032 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7030 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7028 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7025 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7023 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7021 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7020 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7017 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7016 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7014 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7012 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7010 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7009 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7007 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7006 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7003 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.7002 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.7000 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6999 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6997 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6996 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6994 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6994 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6992 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6990 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6989 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6988 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6986 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6985 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6984 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6983 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6982 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6981 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6979 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6979 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6977 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6977 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6975 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6974 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6973 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6973 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6971 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6971 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6969 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6969 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6968 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6968 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6966 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6966 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6964 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6964 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6963 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5020 - loss: 0.6963 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6961 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5039 - auc_5: 0.5038 - loss: 0.6961 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6960 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.5058 - loss: 0.6960 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6959 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5019 - loss: 0.6959 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6958 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4940 - loss: 0.6958 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6956 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5096 - loss: 0.6956 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6955 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.5075 - loss: 0.6955 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6954 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4958 - loss: 0.6954 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6953 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.5038 - loss: 0.6953 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6952 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5000 - loss: 0.6952 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6951 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4999 - loss: 0.6951 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6950 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4920 - loss: 0.6950 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6949 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5057 - loss: 0.6949 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6948 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.4980 - loss: 0.6948 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6948 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5039 - auc_5: 0.4997 - loss: 0.6948 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6947 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5053 - loss: 0.6947 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6946 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4977 - loss: 0.6946 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6945 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5019 - loss: 0.6945 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6945 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5147 - loss: 0.6945 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6944 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4821 - loss: 0.6944 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6943 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4761 - loss: 0.6943 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6943 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4846 - loss: 0.6943 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6942 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5334 - loss: 0.6942 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6942 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.4988 - loss: 0.6942 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6941 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4841 - loss: 0.6941 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6941 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5366 - loss: 0.6941 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6940 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5039 - auc_5: 0.4921 - loss: 0.6941 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6940 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4620 - loss: 0.6940 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6939 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.4768 - loss: 0.6940 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6939 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5366 - loss: 0.6939 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6939 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5034 - loss: 0.6939 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6938 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5139 - loss: 0.6938 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6938 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.4870 - loss: 0.6938 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6937 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4497 - loss: 0.6938 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6937 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4807 - loss: 0.6938 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6937 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5039 - auc_5: 0.4905 - loss: 0.6937 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6937 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5136 - loss: 0.6937 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6936 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5039 - auc_5: 0.4521 - loss: 0.6937 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6936 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5039 - auc_5: 0.4924 - loss: 0.6936 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6936 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4834 - loss: 0.6936 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5090 - loss: 0.6936 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5090 - loss: 0.6935 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4665 - loss: 0.6936 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5284 - loss: 0.6935 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.4963 - loss: 0.6935 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6934 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - auc_5: 0.5349 - loss: 0.6934 - val_accuracy: 0.5078 - val_auc_5: 0.5000 - val_loss: 0.6934 - learning_rate: 1.0000e-04\n",
      "✅ Test Accuracy: 0.5078, AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "units = [128, 32]  \n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "l2_reg = 1e-4\n",
    "\n",
    "model_baseline_5 = Sequential()\n",
    "model_baseline_5.add(tf.keras.Input(shape=(128,)))\n",
    "model_baseline_5.add(tf.keras.layers.Reshape((1, 128)))\n",
    "model_baseline_5.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(Dropout(dropout_rate))\n",
    "model_baseline_5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_baseline_5.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "history_baseline_5 = model_baseline_5.fit(\n",
    "    train_emb, y_train,\n",
    "    validation_data=(test_emb, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, acc, auc = model_baseline_5.evaluate(test_emb, y_test, verbose=0)\n",
    "print(f\"✅ Test Accuracy: {acc:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491de04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [128, 32]  \n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "l2_reg = 1e-4\n",
    "\n",
    "model_baseline_5 = Sequential()\n",
    "model_baseline_5.add(tf.keras.Input(shape=(128,)))\n",
    "model_baseline_5.add(tf.keras.layers.Reshape((1, 128)))\n",
    "model_baseline_5.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(Dropout(dropout_rate))\n",
    "model_baseline_5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_baseline_5.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "history_baseline_5 = model_baseline_5.fit(\n",
    "    train_emb, y_train,\n",
    "    validation_data=(test_emb, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, acc, auc = model_baseline_5.evaluate(test_emb, y_test, verbose=0)\n",
    "print(f\"✅ Test Accuracy: {acc:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c27e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 11:21:59.265209: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'conv1d' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">459,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m270\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m270\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │         \u001b[38;5;34m2,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m270\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m28,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m270\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m459,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,016,837</span> (3.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,016,837\u001b[0m (3.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,016,837</span> (3.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,016,837\u001b[0m (3.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, LSTM, Dropout, Dense\n",
    "# Impor lain yang dibutuhkan: BatchNormalization, Regularizers, dll.\n",
    "\n",
    "def build_smiles2vec_unidirectional_lstm(\n",
    "    max_len, \n",
    "    vocab_size, \n",
    "    embedding_dim, \n",
    "    lstm_units, \n",
    "    dropout_rate, \n",
    "    use_cnn=True,\n",
    "    num_lstm_layers=2 # Bisa diubah/di-tune\n",
    "):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    \n",
    "    # Layer 1: Embedding (Input sekuensial)\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs) # Misalnya, 50 dim\n",
    "    \n",
    "    # Layer 2: CNN Pre-processing (Opsional, tapi ada di varian terbaik SMILES2vec)\n",
    "    if use_cnn:\n",
    "        # Lapisan Conv1D tunggal (atau bisa jadi dua lapisan Conv seperti di paper \n",
    "        # CNN-GRU/LSTM, tapi dengan GlobalMaxPooling dihapus)\n",
    "        # Contoh: filter 192, kernel 3 (berdasarkan paper )\n",
    "        x = Conv1D(filters=192, kernel_size=3, activation='relu', padding='same')(x)\n",
    "        \n",
    "    # Layer 3: JANTUNG TA - LSTM Unidirectional\n",
    "    for i in range(num_lstm_layers):\n",
    "        is_last_layer = (i == num_lstm_layers - 1)\n",
    "        \n",
    "        # return_sequences=True untuk semua layer kecuali layer terakhir\n",
    "        x = LSTM(\n",
    "            lstm_units, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='sigmoid', \n",
    "            return_sequences=(not is_last_layer)\n",
    "        )(x)\n",
    "        \n",
    "    # Layer 4: Dropout (Diterapkan setelah LSTM terakhir)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Layer 5: Dense Output (Klasifikasi Biner)\n",
    "    output = Dense(1, activation='sigmoid')(x) # Output aktivitas antiproliferatif (Inhibitor/Neutral)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# --- PENGGUNAAN DALAM TA LO (Contoh Nilai yang Akan Di-tune MBA) ---\n",
    "# Misalnya, kita asumsikan lo pakai 2 lapisan LSTM biasa (seperti yang diuji di paper [cite: 182])\n",
    "MAX_LEN = 270 \n",
    "VOCAB_SIZE = 50 \n",
    "EMB_DIM = 50 \n",
    "LSTM_UNITS_PER_LAYER = 256 # Ini adalah parameter utama MBA lo!\n",
    "DROPOUT = 0.4\n",
    "NUM_LAYERS = 2 # Ini juga bisa jadi parameter MBA lo\n",
    "\n",
    "model_ta_unidir = build_smiles2vec_unidirectional_lstm(\n",
    "    MAX_LEN, VOCAB_SIZE, EMB_DIM, LSTM_UNITS_PER_LAYER, DROPOUT, use_cnn=True, num_lstm_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "model_ta_unidir.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8104a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'conv1d_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np # Anggap data sudah ada dan siap\n",
    "\n",
    "# --- HYPERPARAMETER AWAL (Akan di-tune oleh MBA lo) ---\n",
    "MAX_LEN = 270 \n",
    "VOCAB_SIZE = 50 \n",
    "EMB_DIM = 50 \n",
    "LSTM_UNITS_PER_LAYER = 256\n",
    "DROPOUT = 0.4\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 1e-3 # Diambil dari setting default yang umum\n",
    "BATCH_SIZE = 32 # Diambil dari setting di paper SMILES2vec\n",
    "L2_REG = 1e-4\n",
    "\n",
    "# --- Panggil Fungsi Pembuat Model ---\n",
    "model_ta_unidir = build_smiles2vec_unidirectional_lstm(\n",
    "    MAX_LEN, VOCAB_SIZE, EMB_DIM, LSTM_UNITS_PER_LAYER, DROPOUT, \n",
    "    use_cnn=True, num_lstm_layers=NUM_LAYERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4d74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label:\n",
      "target\n",
      "0    321\n",
      "1    315\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Max SMILES Length: 165\n",
      "Vocabulary Size: 31\n",
      "\n",
      "Unknown characters in test set: 0\n",
      "Truncated test SMILES: 0\n",
      "\n",
      "--- RINGKASAN MODEL ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'conv1d_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">459,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │         \u001b[38;5;34m1,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m28,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m459,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,015,887</span> (3.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,015,887\u001b[0m (3.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,015,887</span> (3.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,015,887\u001b[0m (3.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START TRAINING (Maksimum 250 Epoch) ---\n",
      "Epoch 1/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 639ms/step - accuracy: 0.5059 - auc: 0.4819 - loss: 0.6947 - val_accuracy: 0.5078 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 613ms/step - accuracy: 0.5157 - auc: 0.5124 - loss: 0.6940 - val_accuracy: 0.4922 - val_auc: 0.5162 - val_loss: 0.6969 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 633ms/step - accuracy: 0.5157 - auc: 0.4845 - loss: 0.6951 - val_accuracy: 0.5078 - val_auc: 0.5159 - val_loss: 0.6932 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 650ms/step - accuracy: 0.4488 - auc: 0.4538 - loss: 0.6945 - val_accuracy: 0.5234 - val_auc: 0.5322 - val_loss: 0.6928 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 644ms/step - accuracy: 0.4980 - auc: 0.4887 - loss: 0.6931 - val_accuracy: 0.5234 - val_auc: 0.5109 - val_loss: 0.6926 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 631ms/step - accuracy: 0.4882 - auc: 0.4828 - loss: 0.6936 - val_accuracy: 0.5156 - val_auc: 0.5006 - val_loss: 0.6929 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 653ms/step - accuracy: 0.4961 - auc: 0.4950 - loss: 0.6938 - val_accuracy: 0.5312 - val_auc: 0.5274 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 649ms/step - accuracy: 0.5098 - auc: 0.5058 - loss: 0.6950 - val_accuracy: 0.5234 - val_auc: 0.5287 - val_loss: 0.6928 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m 1/16\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 650ms/step - accuracy: 0.5000 - auc: 0.6806 - loss: 0.6911"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- START TRAINING (Maksimum 250 Epoch) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Proses Training (menggunakan X_test sebagai validation_data)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m history = \u001b[43mmodel_ta_unidir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m     \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m     \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Jumlah maksimum epoch [cite: 161]\u001b[39;49;00m\n\u001b[32m    181\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    184\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- HASIL AKHIR (TEST SET) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Evaluasi di Test Set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, LSTM, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# --- 1. DATA PREPARATION DAN ENCODING ---\n",
    "\n",
    "# Load Data (Pastikan path file-nya benar)\n",
    "# Catatan: Lo perlu ganti '/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv' dengan path yang benar di sistem lo.\n",
    "df = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv')\n",
    "df['smiles'] = df['smiles'].str.upper()\n",
    "\n",
    "# Fungsi Encoding Label (Target)\n",
    "def encode_label(cat):\n",
    "    if cat == \"inhibitor\":\n",
    "        return 1\n",
    "    elif cat == \"neutral\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(f\"Label tidak dikenal: {cat}\")\n",
    "\n",
    "df['target'] = df['categories'].apply(encode_label)\n",
    "\n",
    "print(\"Distribusi label:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Data Splitting (Train/Test)\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")\n",
    "y_train = df_train['target'].values.astype(np.float32)\n",
    "y_test = df_test['target'].values.astype(np.float32)\n",
    "\n",
    "# Vokabulari (Char-to-Int Mapping)\n",
    "all_chars = set(''.join(df_train['smiles']))\n",
    "# Token khusus: ! (Start), E (End), ? (Unknown), _ (Padding)\n",
    "charset = ['!', 'E', '?', '_'] + sorted(all_chars) \n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "vocab_size = len(charset)\n",
    "max_len = max(len(s) for s in df_train['smiles']) + 2 # +2 untuk token Start dan End\n",
    "\n",
    "print(f\"\\nMax SMILES Length: {max_len}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# Fungsi Encoding SMILES (dengan Padding dan Start/End Token)\n",
    "def encode_smiles(smiles_list, max_len, char_to_int):\n",
    "    sequences = []\n",
    "    # Ambil nilai int untuk token khusus dari dictionary\n",
    "    int_start = char_to_int['!']\n",
    "    int_end = char_to_int['E']\n",
    "    int_unk = char_to_int['?']\n",
    "    int_pad = char_to_int['_']\n",
    "    \n",
    "    for s in smiles_list:\n",
    "        # Tambahkan Start token\n",
    "        seq = [int_start]\n",
    "        # Encode karakter\n",
    "        for c in s:\n",
    "            seq.append(char_to_int.get(c, int_unk))\n",
    "        # Tambahkan End token\n",
    "        seq.append(int_end)\n",
    "        \n",
    "        # Padding\n",
    "        if len(seq) < max_len:\n",
    "            seq += [int_pad] * (max_len - len(seq))\n",
    "        # Truncation (Seharusnya tidak terjadi jika max_len dihitung dari max train)\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "            \n",
    "        sequences.append(seq)\n",
    "        \n",
    "    return np.array(sequences, dtype=np.int32)\n",
    "\n",
    "# Encode Data Training dan Testing\n",
    "X_train = encode_smiles(df_train['smiles'], max_len, char_to_int)\n",
    "X_test = encode_smiles(df_test['smiles'], max_len, char_to_int)\n",
    "\n",
    "# Cek hasil encoding\n",
    "unknown_count = np.sum(X_test == char_to_int['?'])\n",
    "print(f\"\\nUnknown characters in test set: {unknown_count}\")\n",
    "truncated = sum(len(s) + 2 > max_len for s in df_test['smiles'])\n",
    "print(f\"Truncated test SMILES: {truncated}\")\n",
    "\n",
    "\n",
    "# --- 2. MODEL ARCHITECTURE (SMILES2vec Unidirectional LSTM) ---\n",
    "\n",
    "# --- HYPERPARAMETER AWAL ---\n",
    "# Ini adalah nilai-nilai yang akan lo ganti dan dioptimasi oleh MBA!\n",
    "MAX_LEN = X_train.shape[1] # Ambil max_len yang sudah dihitung\n",
    "VOCAB_SIZE = vocab_size\n",
    "EMB_DIM = 50 \n",
    "LSTM_UNITS_PER_LAYER = 256\n",
    "DROPOUT = 0.4\n",
    "NUM_LAYERS = 2 \n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "L2_REG = 1e-4\n",
    "\n",
    "# Fungsi Pembuat Model\n",
    "def build_smiles2vec_unidirectional_lstm(\n",
    "    max_len, vocab_size, embedding_dim, lstm_units, dropout_rate, use_cnn=True, num_lstm_layers=2\n",
    "):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    \n",
    "    # Layer 1: Embedding (dengan mask_zero=True untuk mengabaikan padding)\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs) \n",
    "    \n",
    "    # Layer 2: CNN Pre-processing (CNN-LSTM di paper SMILES2vec)\n",
    "    if use_cnn:\n",
    "        # [cite_start]Contoh: filter 192, kernel 3 (berdasarkan varian terbaik di paper SMILES2vec) [cite: 197, 208, 227]\n",
    "        x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "        \n",
    "    # Layer 3: JANTUNG TA - LSTM Unidirectional\n",
    "    for i in range(num_lstm_layers):\n",
    "        is_last_layer = (i == num_lstm_layers - 1)\n",
    "        \n",
    "        # return_sequences=True untuk semua layer kecuali layer terakhir\n",
    "        x = LSTM(\n",
    "            lstm_units, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='sigmoid', \n",
    "            return_sequences=(not is_last_layer)\n",
    "            # Opsional: tambahkan kernel_regularizer=l2(L2_REG) jika dioptimasi\n",
    "        )(x)\n",
    "        \n",
    "    # Layer 4: Dropout (Diterapkan setelah LSTM terakhir)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Layer 5: Dense Output (Klasifikasi Biner)\n",
    "    output = Dense(1, activation='sigmoid')(x) \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Panggil Model\n",
    "model_ta_unidir = build_smiles2vec_unidirectional_lstm(\n",
    "    MAX_LEN, VOCAB_SIZE, EMB_DIM, LSTM_UNITS_PER_LAYER, DROPOUT, \n",
    "    use_cnn=True, num_lstm_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "# --- 3. KOMPILASI DAN TRAINING ---\n",
    "\n",
    "# Kompilasi\n",
    "model_ta_unidir.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')] \n",
    ")\n",
    "\n",
    "# [cite_start]Callbacks (Early Stopping dan ReduceLROnPlateau) [cite: 162, 163]\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10, \n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n--- RINGKASAN MODEL ---\")\n",
    "model_ta_unidir.summary()\n",
    "\n",
    "print(\"\\n--- START TRAINING (Maksimum 250 Epoch) ---\")\n",
    "# Proses Training (menggunakan X_test sebagai validation_data)\n",
    "history = model_ta_unidir.fit(\n",
    "     X_train, y_train,\n",
    "     validation_data=(X_test, y_test),\n",
    "     epochs=250, # Jumlah maksimum epoch [cite: 161]\n",
    "     batch_size=BATCH_SIZE,\n",
    "     callbacks=callbacks_list,\n",
    "     verbose=1\n",
    " )\n",
    "\n",
    "print(\"\\n--- HASIL AKHIR (TEST SET) ---\")\n",
    "# Evaluasi di Test Set\n",
    "loss, acc, auc = model_ta_unidir.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"✅ Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6301b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def build_smiles2vec_lstm_light(\n",
    "    max_len, vocab_size, embedding_dim, lstm_units, dropout_rate, \n",
    "    cnn_filters=64, num_lstm_layers=1 # Default 1 layer\n",
    "):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    \n",
    "    # Layer 1: Embedding (Ringan)\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs) \n",
    "    \n",
    "    # Layer 2: CNN Pre-processing (Ringan + BN)\n",
    "    # Kita hanya pakai 1 lapisan CNN\n",
    "    x = Conv1D(filters=cnn_filters, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x) # Penting untuk data kecil/batch kecil\n",
    "        \n",
    "    # Layer 3: JANTUNG TA - LSTM Unidirectional (Ringan)\n",
    "    for i in range(num_lstm_layers):\n",
    "        is_last_layer = (i == num_lstm_layers - 1)\n",
    "        \n",
    "        x = LSTM(\n",
    "            lstm_units, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='sigmoid', \n",
    "            return_sequences=(not is_last_layer)\n",
    "        )(x)\n",
    "        \n",
    "    # Layer 4: Dropout\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Layer 5: Dense Output\n",
    "    output = Dense(1, activation='sigmoid')(x) \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f016262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label:\n",
      "target\n",
      "0    321\n",
      "1    315\n",
      "Name: count, dtype: int64\n",
      "Input shape: (508, 163, 27)\n",
      "Vocab size: 27, Max length: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m15,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,985</span> (367.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,985\u001b[0m (367.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,985</span> (367.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,985\u001b[0m (367.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - accuracy: 0.4902 - loss: 0.6983 - val_accuracy: 0.4922 - val_loss: 0.6928\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.4980 - loss: 0.6929 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5079 - loss: 0.6928 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5236 - loss: 0.6933 - val_accuracy: 0.5156 - val_loss: 0.6932\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5118 - loss: 0.6925 - val_accuracy: 0.5156 - val_loss: 0.6933\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4469 - loss: 0.6945 - val_accuracy: 0.5156 - val_loss: 0.6935\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5118 - loss: 0.6925 - val_accuracy: 0.5156 - val_loss: 0.6930\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5157 - loss: 0.6916 - val_accuracy: 0.5156 - val_loss: 0.6950\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.5276 - loss: 0.6912 - val_accuracy: 0.5391 - val_loss: 0.6916\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5335 - loss: 0.6946 - val_accuracy: 0.5469 - val_loss: 0.6891\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5256 - loss: 0.6893 - val_accuracy: 0.5312 - val_loss: 0.6883\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5197 - loss: 0.6872 - val_accuracy: 0.5312 - val_loss: 0.6980\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5217 - loss: 0.6883 - val_accuracy: 0.5156 - val_loss: 0.6828\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5118 - loss: 0.6869 - val_accuracy: 0.5391 - val_loss: 0.6791\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5256 - loss: 0.6838 - val_accuracy: 0.5391 - val_loss: 0.6864\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5118 - loss: 0.6827 - val_accuracy: 0.5391 - val_loss: 0.6840\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5571 - loss: 0.6792 - val_accuracy: 0.5234 - val_loss: 0.6882\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5256 - loss: 0.6944 - val_accuracy: 0.5156 - val_loss: 0.6933\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5256 - loss: 0.6900 - val_accuracy: 0.5156 - val_loss: 0.6978\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5217 - loss: 0.6916 - val_accuracy: 0.5312 - val_loss: 0.6953\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5079 - loss: 0.6911 - val_accuracy: 0.5156 - val_loss: 0.6901\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5079 - loss: 0.6895 - val_accuracy: 0.5547 - val_loss: 0.6895\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5472 - loss: 0.6856 - val_accuracy: 0.5547 - val_loss: 0.6863\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5413 - loss: 0.6819 - val_accuracy: 0.5469 - val_loss: 0.6855\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5354 - loss: 0.6794 - val_accuracy: 0.5547 - val_loss: 0.6831\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5177 - loss: 0.6774 - val_accuracy: 0.5078 - val_loss: 0.6828\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5354 - loss: 0.6739 - val_accuracy: 0.5781 - val_loss: 0.6812\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5768 - loss: 0.6786 - val_accuracy: 0.6094 - val_loss: 0.6676\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5256 - loss: 0.6704 - val_accuracy: 0.5547 - val_loss: 0.6839\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5728 - loss: 0.6641 - val_accuracy: 0.5703 - val_loss: 0.6763\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6024 - loss: 0.6556 - val_accuracy: 0.6328 - val_loss: 0.6637\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5906 - loss: 0.6548 - val_accuracy: 0.5469 - val_loss: 0.6629\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5512 - loss: 0.6576 - val_accuracy: 0.6094 - val_loss: 0.6586\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6004 - loss: 0.6470 - val_accuracy: 0.6875 - val_loss: 0.6259\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5827 - loss: 0.6590 - val_accuracy: 0.5547 - val_loss: 0.6659\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6043 - loss: 0.6309 - val_accuracy: 0.6016 - val_loss: 0.6474\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6457 - loss: 0.6190 - val_accuracy: 0.5547 - val_loss: 0.7243\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6496 - loss: 0.6213 - val_accuracy: 0.6484 - val_loss: 0.6339\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6417 - loss: 0.6180 - val_accuracy: 0.6719 - val_loss: 0.6212\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6339 - loss: 0.6283 - val_accuracy: 0.5234 - val_loss: 0.6894\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6299 - loss: 0.6380 - val_accuracy: 0.6641 - val_loss: 0.6450\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6929 - loss: 0.5951 - val_accuracy: 0.5938 - val_loss: 0.6704\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6476 - loss: 0.6255 - val_accuracy: 0.6562 - val_loss: 0.6447\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6417 - loss: 0.6136 - val_accuracy: 0.6328 - val_loss: 0.6223\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6929 - loss: 0.5786 - val_accuracy: 0.6250 - val_loss: 0.6461\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6969 - loss: 0.5643 - val_accuracy: 0.6406 - val_loss: 0.6323\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6988 - loss: 0.5611 - val_accuracy: 0.6328 - val_loss: 0.6696\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6988 - loss: 0.5554 - val_accuracy: 0.6797 - val_loss: 0.6363\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7224 - loss: 0.5451 - val_accuracy: 0.6953 - val_loss: 0.6284\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7244 - loss: 0.5432 - val_accuracy: 0.6875 - val_loss: 0.6400\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7146 - loss: 0.5472 - val_accuracy: 0.6094 - val_loss: 0.7231\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7165 - loss: 0.5313 - val_accuracy: 0.6328 - val_loss: 0.6546\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7224 - loss: 0.5381 - val_accuracy: 0.7266 - val_loss: 0.6032\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7087 - loss: 0.5431 - val_accuracy: 0.5938 - val_loss: 0.7316\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7421 - loss: 0.5295 - val_accuracy: 0.6797 - val_loss: 0.6143\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7224 - loss: 0.5249 - val_accuracy: 0.6328 - val_loss: 0.6771\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7500 - loss: 0.5144 - val_accuracy: 0.7344 - val_loss: 0.5829\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7283 - loss: 0.5457 - val_accuracy: 0.6641 - val_loss: 0.6523\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7382 - loss: 0.5125 - val_accuracy: 0.6641 - val_loss: 0.6274\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7677 - loss: 0.4865 - val_accuracy: 0.6875 - val_loss: 0.6233\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7677 - loss: 0.4784 - val_accuracy: 0.6875 - val_loss: 0.6377\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7657 - loss: 0.4915 - val_accuracy: 0.7031 - val_loss: 0.6104\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7874 - loss: 0.4653 - val_accuracy: 0.6641 - val_loss: 0.6570\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7776 - loss: 0.4493 - val_accuracy: 0.6641 - val_loss: 0.6819\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7933 - loss: 0.4463 - val_accuracy: 0.6875 - val_loss: 0.6820\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7933 - loss: 0.4529 - val_accuracy: 0.7266 - val_loss: 0.5939\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8012 - loss: 0.4305 - val_accuracy: 0.7422 - val_loss: 0.5891\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7894 - loss: 0.4617 - val_accuracy: 0.6484 - val_loss: 0.7080\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7894 - loss: 0.4532 - val_accuracy: 0.7266 - val_loss: 0.5826\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8189 - loss: 0.4204 - val_accuracy: 0.7266 - val_loss: 0.5995\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8110 - loss: 0.4181 - val_accuracy: 0.6719 - val_loss: 0.6880\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8248 - loss: 0.3977 - val_accuracy: 0.7578 - val_loss: 0.5775\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8268 - loss: 0.4009 - val_accuracy: 0.6797 - val_loss: 0.6920\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8228 - loss: 0.4025 - val_accuracy: 0.6953 - val_loss: 0.6659\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8307 - loss: 0.3808 - val_accuracy: 0.7422 - val_loss: 0.5765\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8287 - loss: 0.4033 - val_accuracy: 0.7188 - val_loss: 0.6459\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8386 - loss: 0.3839 - val_accuracy: 0.7188 - val_loss: 0.6209\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8504 - loss: 0.3582 - val_accuracy: 0.7344 - val_loss: 0.6132\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8150 - loss: 0.4284 - val_accuracy: 0.7344 - val_loss: 0.6168\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7854 - loss: 0.4658 - val_accuracy: 0.6641 - val_loss: 0.6918\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8425 - loss: 0.3733 - val_accuracy: 0.7188 - val_loss: 0.6690\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8543 - loss: 0.3509 - val_accuracy: 0.7031 - val_loss: 0.6790\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8819 - loss: 0.3198 - val_accuracy: 0.7344 - val_loss: 0.6292\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8583 - loss: 0.3538 - val_accuracy: 0.7266 - val_loss: 0.6922\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8524 - loss: 0.3407 - val_accuracy: 0.7422 - val_loss: 0.6592\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8681 - loss: 0.3360 - val_accuracy: 0.7344 - val_loss: 0.6714\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8878 - loss: 0.2923 - val_accuracy: 0.7656 - val_loss: 0.6114\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8642 - loss: 0.3155 - val_accuracy: 0.7188 - val_loss: 0.6904\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8524 - loss: 0.3388 - val_accuracy: 0.7656 - val_loss: 0.6408\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8701 - loss: 0.3181 - val_accuracy: 0.7344 - val_loss: 0.7106\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8760 - loss: 0.2873 - val_accuracy: 0.7344 - val_loss: 0.6996\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8799 - loss: 0.2897 - val_accuracy: 0.7188 - val_loss: 0.7859\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8917 - loss: 0.2809 - val_accuracy: 0.7344 - val_loss: 0.7150\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8760 - loss: 0.2999 - val_accuracy: 0.7266 - val_loss: 0.7205\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8917 - loss: 0.2773 - val_accuracy: 0.7109 - val_loss: 0.7750\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9075 - loss: 0.2477 - val_accuracy: 0.7188 - val_loss: 0.7798\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8839 - loss: 0.2739 - val_accuracy: 0.7500 - val_loss: 0.6910\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8858 - loss: 0.2569 - val_accuracy: 0.6875 - val_loss: 0.8798\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8661 - loss: 0.3265 - val_accuracy: 0.7109 - val_loss: 0.7569\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8740 - loss: 0.3038 - val_accuracy: 0.7344 - val_loss: 0.6811\n",
      "\n",
      "Test Accuracy: 0.7344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer, Conv1D, LSTM, Dense, Dropout, Activation\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Load dan Preprocess Data ---\n",
    "df = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv')\n",
    "df['smiles'] = df['smiles'].str.upper()\n",
    "\n",
    "def encode_label(cat):\n",
    "    return 1 if cat == \"inhibitor\" else 0\n",
    "\n",
    "df['target'] = df['categories'].apply(encode_label)\n",
    "print(\"Distribusi label:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Split data (stratified)\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")\n",
    "\n",
    "# --- 2. Bangun vocabulary dan encode SMILES ---\n",
    "all_chars = set(''.join(df_train['smiles']))\n",
    "charset = sorted(all_chars)\n",
    "# Tambahkan token khusus jika perlu, tapi SMILES2vec asli TIDAK pakai embedding/token khusus\n",
    "# Jadi kita langsung one-hot encode karakter asli\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "vocab_size = len(charset)\n",
    "max_len = max(len(s) for s in df['smiles'])  # gunakan seluruh data untuk max_len\n",
    "# Sesuai makalah: max_len = 250, padding ±10 di kiri/kanan → kita sederhanakan jadi padding kanan saja\n",
    "MAX_LEN = min(max_len, 250)  # batasi maks 250\n",
    "\n",
    "def smiles_to_onehot(smiles_list, char_to_int, max_len):\n",
    "    \"\"\"\n",
    "    Konversi list SMILES ke one-hot tensor: (n, max_len, vocab_size)\n",
    "    \"\"\"\n",
    "    n = len(smiles_list)\n",
    "    X = np.zeros((n, max_len, len(char_to_int)), dtype=np.float32)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        s = s[:max_len]  # truncate jika terlalu panjang\n",
    "        for t, char in enumerate(s):\n",
    "            if char in char_to_int:\n",
    "                X[i, t, char_to_int[char]] = 1.0\n",
    "            # else: biarkan 0 (tidak ada karakter unknown karena pakai train set untuk vocab)\n",
    "    return X\n",
    "\n",
    "X_train = smiles_to_onehot(df_train['smiles'], char_to_int, MAX_LEN)\n",
    "X_test = smiles_to_onehot(df_test['smiles'], char_to_int, MAX_LEN)\n",
    "y_train = df_train['target'].values.astype(np.float32)\n",
    "y_test = df_test['target'].values.astype(np.float32)\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Vocab size: {vocab_size}, Max length: {MAX_LEN}\")\n",
    "\n",
    "# --- 3. Fungsi Build Model (untuk MBO) ---\n",
    "def build_model(\n",
    "    use_conv=True,\n",
    "    conv_filters=192,\n",
    "    conv_kernel_size=3,\n",
    "    lstm_units_1=128,\n",
    "    lstm_units_2=64,\n",
    "    lstm_units_3=32,\n",
    "    dropout_rate=0.4,\n",
    "    learning_rate=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Bangun model Conv1D → LSTM (atau LSTM saja) untuk prediksi biner.\n",
    "    Input: one-hot encoded SMILES (shape: [None, MAX_LEN, vocab_size])\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(MAX_LEN, vocab_size)))\n",
    "\n",
    "    # Opsional: Conv1D layer\n",
    "    if use_conv:\n",
    "        model.add(Conv1D(\n",
    "            filters=conv_filters,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "\n",
    "    # LSTM layers\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    model.add(LSTM(lstm_units_2, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # binary classification\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 4. Contoh Penggunaan Model ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Contoh: build model dengan Conv1D\n",
    "    model = build_model(\n",
    "        use_conv=True,\n",
    "        conv_filters=192,\n",
    "        conv_kernel_size=3,\n",
    "        lstm_units_1=64,\n",
    "        lstm_units_2=32,\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # Latih model (hanya contoh, nanti diganti oleh MBO loop)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluasi\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c159e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label:\n",
      "target\n",
      "0    321\n",
      "1    315\n",
      "Name: count, dtype: int64\n",
      "Input shape: (508, 163, 27)\n",
      "Vocab size: 27, Max length: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m15,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m28,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,697</span> (186.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,697\u001b[0m (186.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,697</span> (186.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,697\u001b[0m (186.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.4961 - loss: 0.6935 - val_accuracy: 0.4922 - val_loss: 0.6935\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.4882 - loss: 0.6935 - val_accuracy: 0.5078 - val_loss: 0.6937\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.5098 - loss: 0.6930 - val_accuracy: 0.5156 - val_loss: 0.6935\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.4705 - loss: 0.6938 - val_accuracy: 0.5156 - val_loss: 0.6935\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.5059 - loss: 0.6930 - val_accuracy: 0.5156 - val_loss: 0.6934\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.4606 - loss: 0.6932 - val_accuracy: 0.4922 - val_loss: 0.6945\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.4980 - loss: 0.6929 - val_accuracy: 0.4922 - val_loss: 0.6943\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5059 - loss: 0.6925 - val_accuracy: 0.4922 - val_loss: 0.6935\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.4921 - loss: 0.6923 - val_accuracy: 0.5156 - val_loss: 0.6939\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5374 - loss: 0.6913 - val_accuracy: 0.4922 - val_loss: 0.6967\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5059 - loss: 0.6912 - val_accuracy: 0.4922 - val_loss: 0.6936\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5118 - loss: 0.6900 - val_accuracy: 0.5312 - val_loss: 0.6921\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.5157 - loss: 0.6915 - val_accuracy: 0.5391 - val_loss: 0.6925\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.5256 - loss: 0.6897 - val_accuracy: 0.5391 - val_loss: 0.6927\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5335 - loss: 0.6847 - val_accuracy: 0.5469 - val_loss: 0.6879\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.5335 - loss: 0.6855 - val_accuracy: 0.5469 - val_loss: 0.6864\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5000 - loss: 0.6804 - val_accuracy: 0.5469 - val_loss: 0.6916\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5531 - loss: 0.6740 - val_accuracy: 0.5391 - val_loss: 0.6933\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5512 - loss: 0.6757 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.5433 - loss: 0.6767 - val_accuracy: 0.5312 - val_loss: 0.6777\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.5453 - loss: 0.6801 - val_accuracy: 0.5391 - val_loss: 0.6842\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.5787 - loss: 0.6665 - val_accuracy: 0.5078 - val_loss: 0.6775\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5906 - loss: 0.6607 - val_accuracy: 0.5703 - val_loss: 0.6628\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.5906 - loss: 0.6594 - val_accuracy: 0.5859 - val_loss: 0.6509\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6260 - loss: 0.6492 - val_accuracy: 0.6016 - val_loss: 0.6614\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5886 - loss: 0.6492 - val_accuracy: 0.6172 - val_loss: 0.6251\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.6457 - loss: 0.6184 - val_accuracy: 0.6172 - val_loss: 0.6781\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.6693 - loss: 0.6154 - val_accuracy: 0.6328 - val_loss: 0.6264\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.6535 - loss: 0.6074 - val_accuracy: 0.5312 - val_loss: 0.7153\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.6024 - loss: 0.6538 - val_accuracy: 0.6172 - val_loss: 0.6498\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.7205 - loss: 0.5989 - val_accuracy: 0.6562 - val_loss: 0.6445\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.6772 - loss: 0.6166 - val_accuracy: 0.6094 - val_loss: 0.6685\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.6909 - loss: 0.5990 - val_accuracy: 0.6406 - val_loss: 0.6382\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7008 - loss: 0.5572 - val_accuracy: 0.6094 - val_loss: 0.7193\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7165 - loss: 0.5609 - val_accuracy: 0.6562 - val_loss: 0.6547\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7264 - loss: 0.5498 - val_accuracy: 0.6953 - val_loss: 0.6103\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7205 - loss: 0.5561 - val_accuracy: 0.6797 - val_loss: 0.6268\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7185 - loss: 0.5496 - val_accuracy: 0.6797 - val_loss: 0.6185\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.7539 - loss: 0.5213 - val_accuracy: 0.6562 - val_loss: 0.6076\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.7303 - loss: 0.5486 - val_accuracy: 0.7188 - val_loss: 0.5996\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.7402 - loss: 0.5242 - val_accuracy: 0.6641 - val_loss: 0.6208\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.7520 - loss: 0.5211 - val_accuracy: 0.7344 - val_loss: 0.5855\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.7461 - loss: 0.5268 - val_accuracy: 0.6641 - val_loss: 0.6762\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.7382 - loss: 0.5412 - val_accuracy: 0.7109 - val_loss: 0.5878\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7717 - loss: 0.5071 - val_accuracy: 0.6875 - val_loss: 0.6525\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.7402 - loss: 0.5409 - val_accuracy: 0.6797 - val_loss: 0.6826\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7303 - loss: 0.5491 - val_accuracy: 0.6562 - val_loss: 0.6676\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7165 - loss: 0.5425 - val_accuracy: 0.7109 - val_loss: 0.5781\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.7579 - loss: 0.5234 - val_accuracy: 0.6641 - val_loss: 0.6477\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7402 - loss: 0.5158 - val_accuracy: 0.6719 - val_loss: 0.6861\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.7638 - loss: 0.4906 - val_accuracy: 0.6953 - val_loss: 0.6171\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.7795 - loss: 0.4847 - val_accuracy: 0.6797 - val_loss: 0.6827\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.7756 - loss: 0.4934 - val_accuracy: 0.7266 - val_loss: 0.5768\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7933 - loss: 0.4646 - val_accuracy: 0.7344 - val_loss: 0.5780\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7776 - loss: 0.4859 - val_accuracy: 0.6641 - val_loss: 0.6655\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7618 - loss: 0.4989 - val_accuracy: 0.6797 - val_loss: 0.6017\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8071 - loss: 0.4720 - val_accuracy: 0.7422 - val_loss: 0.5820\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.7874 - loss: 0.4722 - val_accuracy: 0.6719 - val_loss: 0.6476\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.7776 - loss: 0.4774 - val_accuracy: 0.7422 - val_loss: 0.5682\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7953 - loss: 0.4542 - val_accuracy: 0.7422 - val_loss: 0.5523\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.7559 - loss: 0.5168 - val_accuracy: 0.7188 - val_loss: 0.5916\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.7717 - loss: 0.4764 - val_accuracy: 0.7500 - val_loss: 0.5346\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8189 - loss: 0.4285 - val_accuracy: 0.6953 - val_loss: 0.6154\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7992 - loss: 0.4457 - val_accuracy: 0.7266 - val_loss: 0.5924\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7323 - loss: 0.5618 - val_accuracy: 0.6797 - val_loss: 0.6177\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8091 - loss: 0.4474 - val_accuracy: 0.7031 - val_loss: 0.5822\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.8071 - loss: 0.4483 - val_accuracy: 0.7344 - val_loss: 0.5445\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7894 - loss: 0.4653 - val_accuracy: 0.7578 - val_loss: 0.5674\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.8346 - loss: 0.4189 - val_accuracy: 0.6953 - val_loss: 0.6091\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8366 - loss: 0.4047 - val_accuracy: 0.6875 - val_loss: 0.5884\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8209 - loss: 0.4030 - val_accuracy: 0.7109 - val_loss: 0.5893\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.8386 - loss: 0.3905 - val_accuracy: 0.7344 - val_loss: 0.5827\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8248 - loss: 0.4330 - val_accuracy: 0.5000 - val_loss: 1.0047\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5020 - loss: 0.9263 - val_accuracy: 0.4922 - val_loss: 0.6964\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5059 - loss: 0.7020 - val_accuracy: 0.5391 - val_loss: 0.6889\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5354 - loss: 0.6945 - val_accuracy: 0.5391 - val_loss: 0.6903\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.5492 - loss: 0.6900 - val_accuracy: 0.5469 - val_loss: 0.6884\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.4626 - loss: 0.6950 - val_accuracy: 0.5547 - val_loss: 0.6881\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.5571 - loss: 0.6834 - val_accuracy: 0.5469 - val_loss: 0.6870\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.4862 - loss: 0.6961 - val_accuracy: 0.5547 - val_loss: 0.6859\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.5098 - loss: 0.6869 - val_accuracy: 0.5391 - val_loss: 0.6812\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4961 - loss: 0.6871 - val_accuracy: 0.5391 - val_loss: 0.6736\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.5335 - loss: 0.6818 - val_accuracy: 0.5469 - val_loss: 0.6669\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5256 - loss: 0.6785 - val_accuracy: 0.5469 - val_loss: 0.6639\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.5551 - loss: 0.6729 - val_accuracy: 0.5391 - val_loss: 0.6643\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5256 - loss: 0.6745 - val_accuracy: 0.5391 - val_loss: 0.6640\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.5433 - loss: 0.6721 - val_accuracy: 0.5469 - val_loss: 0.6623\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5571 - loss: 0.6729 - val_accuracy: 0.5469 - val_loss: 0.6626\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5256 - loss: 0.6671 - val_accuracy: 0.5391 - val_loss: 0.6643\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5433 - loss: 0.6681 - val_accuracy: 0.5469 - val_loss: 0.6614\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.5276 - loss: 0.6679 - val_accuracy: 0.5469 - val_loss: 0.6598\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5138 - loss: 0.6675 - val_accuracy: 0.6250 - val_loss: 0.6578\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5807 - loss: 0.6554 - val_accuracy: 0.6797 - val_loss: 0.6412\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.6909 - loss: 0.6223 - val_accuracy: 0.6719 - val_loss: 0.6505\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.6929 - loss: 0.6119 - val_accuracy: 0.7031 - val_loss: 0.5842\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.7165 - loss: 0.5750 - val_accuracy: 0.6797 - val_loss: 0.5892\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7421 - loss: 0.5345 - val_accuracy: 0.6875 - val_loss: 0.5622\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7657 - loss: 0.5050 - val_accuracy: 0.7266 - val_loss: 0.5739\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7913 - loss: 0.4653 - val_accuracy: 0.7422 - val_loss: 0.5876\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7776 - loss: 0.4796 - val_accuracy: 0.5781 - val_loss: 0.7424\n",
      "\n",
      "Test Accuracy: 0.5781\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer, Conv1D, LSTM, Dense, Dropout, Activation\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Load dan Preprocess Data ---\n",
    "df = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv')\n",
    "df['smiles'] = df['smiles'].str.upper()\n",
    "\n",
    "def encode_label(cat):\n",
    "    return 1 if cat == \"inhibitor\" else 0\n",
    "\n",
    "df['target'] = df['categories'].apply(encode_label)\n",
    "print(\"Distribusi label:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Split data (stratified)\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")\n",
    "\n",
    "# --- 2. Bangun vocabulary dan encode SMILES ---\n",
    "all_chars = set(''.join(df_train['smiles']))\n",
    "charset = sorted(all_chars)\n",
    "# Tambahkan token khusus jika perlu, tapi SMILES2vec asli TIDAK pakai embedding/token khusus\n",
    "# Jadi kita langsung one-hot encode karakter asli\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "vocab_size = len(charset)\n",
    "max_len = max(len(s) for s in df['smiles'])  # gunakan seluruh data untuk max_len\n",
    "# Sesuai makalah: max_len = 250, padding ±10 di kiri/kanan → kita sederhanakan jadi padding kanan saja\n",
    "MAX_LEN = min(max_len, 250)  # batasi maks 250\n",
    "\n",
    "def smiles_to_onehot(smiles_list, char_to_int, max_len):\n",
    "    \"\"\"\n",
    "    Konversi list SMILES ke one-hot tensor: (n, max_len, vocab_size)\n",
    "    \"\"\"\n",
    "    n = len(smiles_list)\n",
    "    X = np.zeros((n, max_len, len(char_to_int)), dtype=np.float32)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        s = s[:max_len]  # truncate jika terlalu panjang\n",
    "        for t, char in enumerate(s):\n",
    "            if char in char_to_int:\n",
    "                X[i, t, char_to_int[char]] = 1.0\n",
    "            # else: biarkan 0 (tidak ada karakter unknown karena pakai train set untuk vocab)\n",
    "    return X\n",
    "\n",
    "X_train = smiles_to_onehot(df_train['smiles'], char_to_int, MAX_LEN)\n",
    "X_test = smiles_to_onehot(df_test['smiles'], char_to_int, MAX_LEN)\n",
    "y_train = df_train['target'].values.astype(np.float32)\n",
    "y_test = df_test['target'].values.astype(np.float32)\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Vocab size: {vocab_size}, Max length: {MAX_LEN}\")\n",
    "\n",
    "# --- 3. Fungsi Build Model (untuk MBO) ---\n",
    "def build_model(\n",
    "    use_conv=True,\n",
    "    conv_filters=192,\n",
    "    conv_kernel_size=3,\n",
    "    lstm_units_1=128,\n",
    "    lstm_units_2=64,\n",
    "    lstm_units_3=32,\n",
    "    dropout_rate=0.4,\n",
    "    learning_rate=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Bangun model Conv1D → LSTM (atau LSTM saja) untuk prediksi biner.\n",
    "    Input: one-hot encoded SMILES (shape: [None, MAX_LEN, vocab_size])\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(MAX_LEN, vocab_size)))\n",
    "\n",
    "    # Opsional: Conv1D layer\n",
    "    if use_conv:\n",
    "        model.add(Conv1D(\n",
    "            filters=conv_filters,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "\n",
    "    # LSTM layers\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    model.add(LSTM(lstm_units_2, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # binary classification\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 4. Contoh Penggunaan Model ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Contoh: build model dengan Conv1D\n",
    "    model = build_model(\n",
    "        use_conv=True,\n",
    "        conv_filters=192,\n",
    "        conv_kernel_size=3,\n",
    "        lstm_units_1=32,\n",
    "        lstm_units_2=16,\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # Latih model (hanya contoh, nanti diganti oleh MBO loop)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluasi\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed766176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label:\n",
      "target\n",
      "0    321\n",
      "1    315\n",
      "Name: count, dtype: int64\n",
      "Input shape: (508, 163)\n",
      "Vocab size: 27, Max length: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.4941 - loss: 0.6938 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5020 - loss: 0.6934 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.5236 - loss: 0.6932 - val_accuracy: 0.5078 - val_loss: 0.6931\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.4941 - loss: 0.6950 - val_accuracy: 0.5156 - val_loss: 0.6931\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.4921 - loss: 0.6932 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.4803 - loss: 0.6930 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.4961 - loss: 0.6933 - val_accuracy: 0.5078 - val_loss: 0.6931\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.4921 - loss: 0.6935 - val_accuracy: 0.5078 - val_loss: 0.6931\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5059 - loss: 0.6928 - val_accuracy: 0.5078 - val_loss: 0.6930\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5157 - loss: 0.6928 - val_accuracy: 0.5078 - val_loss: 0.6932\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.4961 - loss: 0.6932 - val_accuracy: 0.5234 - val_loss: 0.6933\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.4764 - loss: 0.6942 - val_accuracy: 0.5156 - val_loss: 0.6938\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5059 - loss: 0.6922 - val_accuracy: 0.5156 - val_loss: 0.6948\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4882 - loss: 0.6920 - val_accuracy: 0.5000 - val_loss: 0.6923\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5098 - loss: 0.6927 - val_accuracy: 0.5312 - val_loss: 0.6918\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.4862 - loss: 0.6919 - val_accuracy: 0.5547 - val_loss: 0.6932\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5256 - loss: 0.6879 - val_accuracy: 0.5391 - val_loss: 0.6906\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5118 - loss: 0.6876 - val_accuracy: 0.5312 - val_loss: 0.6907\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5236 - loss: 0.6877 - val_accuracy: 0.5312 - val_loss: 0.6914\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.5039 - loss: 0.6875 - val_accuracy: 0.5391 - val_loss: 0.6951\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5433 - loss: 0.6854 - val_accuracy: 0.5391 - val_loss: 0.6875\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5256 - loss: 0.6848 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5177 - loss: 0.6909 - val_accuracy: 0.5312 - val_loss: 0.6924\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5276 - loss: 0.6851 - val_accuracy: 0.5391 - val_loss: 0.6911\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5295 - loss: 0.6888 - val_accuracy: 0.5547 - val_loss: 0.6841\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.5571 - loss: 0.6817 - val_accuracy: 0.5156 - val_loss: 0.6842\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5138 - loss: 0.6801 - val_accuracy: 0.5234 - val_loss: 0.6879\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5472 - loss: 0.6799 - val_accuracy: 0.5469 - val_loss: 0.6802\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.4961 - loss: 0.6809 - val_accuracy: 0.4844 - val_loss: 0.6792\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.4980 - loss: 0.6837 - val_accuracy: 0.5703 - val_loss: 0.6919\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5453 - loss: 0.6767 - val_accuracy: 0.5078 - val_loss: 0.6919\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.5118 - loss: 0.6818 - val_accuracy: 0.5156 - val_loss: 0.6815\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.5551 - loss: 0.6752 - val_accuracy: 0.5312 - val_loss: 0.6849\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5276 - loss: 0.6723 - val_accuracy: 0.5234 - val_loss: 0.6727\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.5531 - loss: 0.6669 - val_accuracy: 0.5469 - val_loss: 0.6729\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5984 - loss: 0.6717 - val_accuracy: 0.5391 - val_loss: 0.6774\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6083 - loss: 0.6540 - val_accuracy: 0.5938 - val_loss: 0.6555\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.5965 - loss: 0.6647 - val_accuracy: 0.5859 - val_loss: 0.6591\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.5748 - loss: 0.6615 - val_accuracy: 0.5703 - val_loss: 0.6628\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5787 - loss: 0.6565 - val_accuracy: 0.6250 - val_loss: 0.6540\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.6122 - loss: 0.6542 - val_accuracy: 0.6172 - val_loss: 0.6584\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.6260 - loss: 0.6443 - val_accuracy: 0.5938 - val_loss: 0.6686\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6161 - loss: 0.6474 - val_accuracy: 0.5938 - val_loss: 0.6472\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.6299 - loss: 0.6430 - val_accuracy: 0.6719 - val_loss: 0.6404\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6240 - loss: 0.6399 - val_accuracy: 0.6328 - val_loss: 0.6415\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.6339 - loss: 0.6344 - val_accuracy: 0.6016 - val_loss: 0.6462\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.6319 - loss: 0.6447 - val_accuracy: 0.6016 - val_loss: 0.6450\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6339 - loss: 0.6399 - val_accuracy: 0.6406 - val_loss: 0.6342\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6594 - loss: 0.6223 - val_accuracy: 0.5859 - val_loss: 0.7039\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.6417 - loss: 0.6448 - val_accuracy: 0.6172 - val_loss: 0.6518\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.6496 - loss: 0.6224 - val_accuracy: 0.6094 - val_loss: 0.6689\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.6476 - loss: 0.6281 - val_accuracy: 0.5547 - val_loss: 0.7084\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6693 - loss: 0.6255 - val_accuracy: 0.6016 - val_loss: 0.6648\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6772 - loss: 0.6139 - val_accuracy: 0.6250 - val_loss: 0.6496\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.6693 - loss: 0.6000 - val_accuracy: 0.6484 - val_loss: 0.6334\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.6988 - loss: 0.5924 - val_accuracy: 0.6406 - val_loss: 0.6563\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.6713 - loss: 0.5981 - val_accuracy: 0.6250 - val_loss: 0.6629\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6791 - loss: 0.5768 - val_accuracy: 0.5703 - val_loss: 0.7234\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.6850 - loss: 0.5913 - val_accuracy: 0.6562 - val_loss: 0.6251\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.7067 - loss: 0.5958 - val_accuracy: 0.6641 - val_loss: 0.6489\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.6969 - loss: 0.5809 - val_accuracy: 0.6484 - val_loss: 0.6402\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.7362 - loss: 0.5632 - val_accuracy: 0.6562 - val_loss: 0.6268\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.7185 - loss: 0.5621 - val_accuracy: 0.6406 - val_loss: 0.6341\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7303 - loss: 0.5543 - val_accuracy: 0.6562 - val_loss: 0.6393\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.7323 - loss: 0.5541 - val_accuracy: 0.6406 - val_loss: 0.6485\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.7461 - loss: 0.5333 - val_accuracy: 0.6562 - val_loss: 0.6679\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.7657 - loss: 0.5255 - val_accuracy: 0.6406 - val_loss: 0.6984\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.7539 - loss: 0.5212 - val_accuracy: 0.6875 - val_loss: 0.6210\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7579 - loss: 0.5234 - val_accuracy: 0.6484 - val_loss: 0.6630\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.7559 - loss: 0.5327 - val_accuracy: 0.6328 - val_loss: 0.7040\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.7559 - loss: 0.5211 - val_accuracy: 0.7031 - val_loss: 0.6397\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7500 - loss: 0.5163 - val_accuracy: 0.6719 - val_loss: 0.6187\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7539 - loss: 0.5256 - val_accuracy: 0.6953 - val_loss: 0.6414\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7874 - loss: 0.4934 - val_accuracy: 0.7109 - val_loss: 0.5912\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.7559 - loss: 0.5164 - val_accuracy: 0.6797 - val_loss: 0.5816\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.7539 - loss: 0.5277 - val_accuracy: 0.6250 - val_loss: 0.7333\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.7244 - loss: 0.5492 - val_accuracy: 0.6406 - val_loss: 0.7038\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7756 - loss: 0.4974 - val_accuracy: 0.6484 - val_loss: 0.7181\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8071 - loss: 0.4785 - val_accuracy: 0.6719 - val_loss: 0.6918\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.7992 - loss: 0.4719 - val_accuracy: 0.6953 - val_loss: 0.6419\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8091 - loss: 0.4670 - val_accuracy: 0.7344 - val_loss: 0.5746\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8091 - loss: 0.4616 - val_accuracy: 0.7500 - val_loss: 0.5740\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8268 - loss: 0.4568 - val_accuracy: 0.7031 - val_loss: 0.6386\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8228 - loss: 0.4615 - val_accuracy: 0.6484 - val_loss: 0.7311\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.8150 - loss: 0.4676 - val_accuracy: 0.6641 - val_loss: 0.7601\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8130 - loss: 0.4462 - val_accuracy: 0.7266 - val_loss: 0.6014\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.8228 - loss: 0.4297 - val_accuracy: 0.7109 - val_loss: 0.6433\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8268 - loss: 0.4361 - val_accuracy: 0.7578 - val_loss: 0.5966\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8287 - loss: 0.4376 - val_accuracy: 0.7109 - val_loss: 0.5865\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8268 - loss: 0.4334 - val_accuracy: 0.7188 - val_loss: 0.6317\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8406 - loss: 0.4112 - val_accuracy: 0.7031 - val_loss: 0.6936\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8484 - loss: 0.4005 - val_accuracy: 0.7266 - val_loss: 0.6738\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8465 - loss: 0.4017 - val_accuracy: 0.7188 - val_loss: 0.6330\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7992 - loss: 0.4684 - val_accuracy: 0.7188 - val_loss: 0.6239\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7992 - loss: 0.4792 - val_accuracy: 0.7422 - val_loss: 0.5724\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8189 - loss: 0.4354 - val_accuracy: 0.7109 - val_loss: 0.5971\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8425 - loss: 0.4100 - val_accuracy: 0.7188 - val_loss: 0.6470\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8543 - loss: 0.3843 - val_accuracy: 0.7188 - val_loss: 0.6246\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8563 - loss: 0.3805 - val_accuracy: 0.7422 - val_loss: 0.6308\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.8563 - loss: 0.3841 - val_accuracy: 0.7578 - val_loss: 0.5977\n",
      "\n",
      "Test Accuracy: 0.7578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Conv1D, LSTM, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load dan Preprocess Data ---\n",
    "df = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/GSARPC3.csv')\n",
    "df['smiles'] = df['smiles'].str.upper()\n",
    "\n",
    "def encode_label(cat):\n",
    "    return 1 if cat == \"inhibitor\" else 0\n",
    "\n",
    "df['target'] = df['categories'].apply(encode_label)\n",
    "print(\"Distribusi label:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Split data (stratified)\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")\n",
    "\n",
    "# --- 2. Bangun vocabulary dan encode SMILES ke integer ---\n",
    "all_chars = set(''.join(df_train['smiles']))\n",
    "charset = sorted(all_chars)\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "vocab_size = len(charset)\n",
    "MAX_LEN = min(max(len(s) for s in df['smiles']), 250)  # batas maks 250\n",
    "\n",
    "def smiles_to_int(smiles_list, char_to_int, max_len):\n",
    "    \"\"\"\n",
    "    Konversi list SMILES ke integer sequence: (n, max_len)\n",
    "    Karakter tidak dikenal diisi 0 (bisa diganti dengan token khusus jika mau)\n",
    "    \"\"\"\n",
    "    n = len(smiles_list)\n",
    "    X = np.zeros((n, max_len), dtype=np.int32)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        s = s[:max_len]  # truncate\n",
    "        for t, char in enumerate(s):\n",
    "            X[i, t] = char_to_int.get(char, 0)  # fallback ke 0 jika unknown\n",
    "    return X\n",
    "\n",
    "X_train = smiles_to_int(df_train['smiles'], char_to_int, MAX_LEN)\n",
    "X_test = smiles_to_int(df_test['smiles'], char_to_int, MAX_LEN)\n",
    "y_train = df_train['target'].values.astype(np.float32)\n",
    "y_test = df_test['target'].values.astype(np.float32)\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Vocab size: {vocab_size}, Max length: {MAX_LEN}\")\n",
    "\n",
    "# --- 3. Fungsi Build Model dengan Embedding (untuk MBO) ---\n",
    "def build_model(\n",
    "    embedding_dim=50,\n",
    "    use_conv=True,\n",
    "    conv_filters=192,\n",
    "    conv_kernel_size=3,\n",
    "    lstm_units_1=128,\n",
    "    lstm_units_2=64,\n",
    "    dropout_rate=0.4,\n",
    "    learning_rate=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Bangun model: Embedding → [Conv1D] → LSTM → Dense\n",
    "    Input: integer-encoded SMILES (shape: [None, MAX_LEN])\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=MAX_LEN\n",
    "    ))\n",
    "\n",
    "    if use_conv:\n",
    "        model.add(Conv1D(\n",
    "            filters=conv_filters,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    model.add(LSTM(lstm_units_2, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 4. Contoh Penggunaan Model ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_model(\n",
    "        embedding_dim=50,\n",
    "        use_conv=True,\n",
    "        conv_filters=64,        # lebih kecil untuk data kecil\n",
    "        conv_kernel_size=3,\n",
    "        lstm_units_1=64,\n",
    "        lstm_units_2=32,\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
