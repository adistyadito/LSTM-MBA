{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a199422",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1426f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 23:59:23.450010: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-16 23:59:23.851087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-16 23:59:25.713879: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LSTM Hyperparameter Optimization with Monarch Butterfly Algorithm (NiaPy) - Enhanced\n",
    "# ============================================\n",
    "# Requirements:\n",
    "# pip install niapy tensorflow scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Monarch Butterfly Optimization\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "\n",
    "# prompt: import essential packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "#import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder # Atau OneHotEncoder\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem # Untuk MACCS Keys\n",
    "\n",
    "print(\"Packages imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf977c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PubchemFP0</th>\n",
       "      <th>PubchemFP1</th>\n",
       "      <th>PubchemFP2</th>\n",
       "      <th>PubchemFP3</th>\n",
       "      <th>PubchemFP4</th>\n",
       "      <th>PubchemFP5</th>\n",
       "      <th>PubchemFP6</th>\n",
       "      <th>PubchemFP7</th>\n",
       "      <th>PubchemFP8</th>\n",
       "      <th>...</th>\n",
       "      <th>PubchemFP873</th>\n",
       "      <th>PubchemFP874</th>\n",
       "      <th>PubchemFP875</th>\n",
       "      <th>PubchemFP876</th>\n",
       "      <th>PubchemFP877</th>\n",
       "      <th>PubchemFP878</th>\n",
       "      <th>PubchemFP879</th>\n",
       "      <th>PubchemFP880</th>\n",
       "      <th>acvalue</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44244736</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44244911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44245235</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10451021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44245073</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>145958114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>145950639</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>3168508</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>145952863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119.1000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>145955648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.9000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows Ã— 884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  PubchemFP0  PubchemFP1  PubchemFP2  PubchemFP3  PubchemFP4  \\\n",
       "0     44244736           1           1           1           0           0   \n",
       "1     44244911           1           1           1           0           0   \n",
       "2     44245235           1           1           1           0           0   \n",
       "3     10451021           1           1           1           0           0   \n",
       "4     44245073           1           1           1           0           0   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "631  145958114           1           1           1           0           0   \n",
       "632  145950639           1           1           1           0           0   \n",
       "633    3168508           1           1           0           0           0   \n",
       "634  145952863           1           1           1           0           0   \n",
       "635  145955648           1           1           1           0           0   \n",
       "\n",
       "     PubchemFP5  PubchemFP6  PubchemFP7  PubchemFP8  ...  PubchemFP873  \\\n",
       "0             0           0           0           0  ...             0   \n",
       "1             0           0           0           0  ...             0   \n",
       "2             0           0           0           0  ...             0   \n",
       "3             0           0           0           0  ...             0   \n",
       "4             0           0           0           0  ...             0   \n",
       "..          ...         ...         ...         ...  ...           ...   \n",
       "631           0           0           0           0  ...             0   \n",
       "632           0           0           0           0  ...             0   \n",
       "633           0           0           0           0  ...             0   \n",
       "634           0           0           0           0  ...             0   \n",
       "635           0           0           0           0  ...             0   \n",
       "\n",
       "     PubchemFP874  PubchemFP875  PubchemFP876  PubchemFP877  PubchemFP878  \\\n",
       "0               0             0             0             0             0   \n",
       "1               0             0             0             0             0   \n",
       "2               0             0             0             0             0   \n",
       "3               0             0             0             0             0   \n",
       "4               0             0             0             0             0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "631             0             0             0             0             0   \n",
       "632             0             0             0             0             0   \n",
       "633             0             0             0             0             0   \n",
       "634             0             0             0             0             0   \n",
       "635             0             0             0             0             0   \n",
       "\n",
       "     PubchemFP879  PubchemFP880   acvalue  categories  \n",
       "0               0             0    0.0030   inhibitor  \n",
       "1               0             0    0.0035   inhibitor  \n",
       "2               0             0    0.0047   inhibitor  \n",
       "3               0             0    0.0090   inhibitor  \n",
       "4               0             0    0.0180   inhibitor  \n",
       "..            ...           ...       ...         ...  \n",
       "631             0             0  100.0000     neutral  \n",
       "632             0             0  100.0000     neutral  \n",
       "633             0             0  100.0000     neutral  \n",
       "634             0             0  119.1000     neutral  \n",
       "635             0             0  122.9000     neutral  \n",
       "\n",
       "[636 rows x 884 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/full/dataFP.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33759653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "['inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:882].values  \n",
    "y = data['categories'].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3671513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754a2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98018f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting MBO search: max_evals=120, pop_size=10. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 16:02:01.591144: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval 1/120] layers=2 act=relu opt=nadam lr=1.32e-05 bs=8 l2=4e-05 -> val_auc=0.7414 (took 7.2s)\n",
      "[Eval 2/120] layers=2 act=relu opt=nadam lr=1.08e-05 bs=8 l2=2e-08 -> val_auc=0.7984 (took 7.6s)\n",
      "[Eval 3/120] layers=3 act=relu opt=rmsprop lr=3.74e-05 bs=32 l2=2e-07 -> val_auc=0.5000 (took 5.2s)\n",
      "[Eval 4/120] layers=2 act=relu opt=nadam lr=3.08e-05 bs=8 l2=2e-07 -> val_auc=0.7800 (took 9.0s)\n",
      "[Eval 5/120] layers=2 act=tanh opt=adam lr=6.11e-03 bs=16 l2=1e-07 -> val_auc=0.9520 (took 5.0s)\n",
      "[Eval 6/120] layers=1 act=relu opt=adam lr=2.14e-04 bs=32 l2=6e-06 -> val_auc=0.8463 (took 3.7s)\n",
      "[Eval 7/120] layers=2 act=selu opt=rmsprop lr=1.68e-03 bs=8 l2=1e-08 -> val_auc=0.9258 (took 5.8s)\n",
      "[Eval 8/120] layers=2 act=selu opt=adam lr=8.41e-05 bs=8 l2=8e-05 -> val_auc=0.8847 (took 7.2s)\n",
      "[Eval 9/120] layers=2 act=relu opt=rmsprop lr=2.06e-05 bs=16 l2=3e-05 -> val_auc=0.7076 (took 5.3s)\n",
      "[Eval 10/120] layers=2 act=relu opt=rmsprop lr=5.06e-05 bs=8 l2=7e-06 -> val_auc=0.7593 (took 8.7s)\n",
      "[Eval 11/120] layers=2 act=relu opt=rmsprop lr=5.06e-05 bs=8 l2=4e-05 -> val_auc=0.8005 (took 6.9s)\n",
      "[Eval 12/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9280 (took 7.0s)\n",
      "[Eval 13/120] layers=2 act=relu opt=nadam lr=2.06e-05 bs=8 l2=3e-05 -> val_auc=0.7999 (took 9.1s)\n",
      "[Eval 14/120] layers=2 act=relu opt=adam lr=3.08e-05 bs=8 l2=7e-06 -> val_auc=0.7676 (took 6.3s)\n",
      "[Eval 15/120] layers=2 act=selu opt=rmsprop lr=2.06e-05 bs=16 l2=3e-05 -> val_auc=0.7771 (took 5.1s)\n",
      "[Eval 16/120] layers=2 act=relu opt=rmsprop lr=5.06e-05 bs=32 l2=4e-05 -> val_auc=0.7306 (took 4.6s)\n",
      "[Eval 17/120] layers=3 act=relu opt=rmsprop lr=5.06e-05 bs=32 l2=4e-05 -> val_auc=0.5000 (took 6.0s)\n",
      "[Eval 18/120] layers=2 act=relu opt=nadam lr=5.06e-05 bs=32 l2=1e-02 -> val_auc=0.6003 (took 5.5s)\n",
      "[Eval 19/120] layers=2 act=selu opt=nadam lr=5.06e-05 bs=8 l2=4e-05 -> val_auc=0.8346 (took 8.3s)\n",
      "[Eval 20/120] layers=3 act=relu opt=rmsprop lr=5.06e-05 bs=32 l2=1e-02 -> val_auc=0.5000 (took 6.0s)\n",
      "[Eval 21/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9084 (took 6.4s)\n",
      "[Eval 22/120] layers=2 act=selu opt=nadam lr=5.06e-05 bs=8 l2=4e-05 -> val_auc=0.8305 (took 8.3s)\n",
      "[Eval 23/120] layers=2 act=relu opt=rmsprop lr=5.06e-05 bs=8 l2=4e-05 -> val_auc=0.8005 (took 7.3s)\n",
      "[Eval 24/120] layers=2 act=relu opt=nadam lr=2.06e-05 bs=8 l2=3e-05 -> val_auc=0.7914 (took 10.1s)\n",
      "[Eval 25/120] layers=2 act=selu opt=rmsprop lr=2.06e-05 bs=16 l2=3e-05 -> val_auc=0.7893 (took 5.6s)\n",
      "[Eval 26/120] layers=2 act=relu opt=adam lr=3.08e-05 bs=8 l2=7e-06 -> val_auc=0.7922 (took 7.0s)\n",
      "[Eval 27/120] layers=2 act=relu opt=rmsprop lr=5.06e-05 bs=32 l2=4e-05 -> val_auc=0.7347 (took 5.0s)\n",
      "[Eval 28/120] layers=2 act=relu opt=nadam lr=5.06e-05 bs=32 l2=1e-02 -> val_auc=0.6716 (took 6.3s)\n",
      "[Eval 29/120] layers=2 act=tanh opt=adam lr=6.11e-03 bs=16 l2=1e-07 -> val_auc=0.9240 (took 6.7s)\n",
      "[Eval 30/120] layers=2 act=selu opt=rmsprop lr=1.68e-03 bs=8 l2=1e-08 -> val_auc=0.9067 (took 7.6s)\n",
      "[Eval 31/120] layers=2 act=relu opt=rmsprop lr=2.06e-05 bs=8 l2=8e-05 -> val_auc=0.7461 (took 8.0s)\n",
      "[Eval 32/120] layers=2 act=selu opt=rmsprop lr=3.08e-05 bs=16 l2=8e-05 -> val_auc=0.7872 (took 6.2s)\n",
      "[Eval 33/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=32 l2=3e-05 -> val_auc=0.9219 (took 6.2s)\n",
      "[Eval 34/120] layers=2 act=relu opt=adam lr=3.08e-05 bs=8 l2=1e-08 -> val_auc=0.7520 (took 7.1s)\n",
      "[Eval 35/120] layers=2 act=relu opt=nadam lr=2.06e-05 bs=8 l2=1e-08 -> val_auc=0.7849 (took 8.5s)\n",
      "[Eval 36/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9322 (took 8.2s)\n",
      "[Eval 37/120] layers=3 act=relu opt=nadam lr=6.11e-03 bs=32 l2=1e-02 -> val_auc=0.5000 (took 9.5s)\n",
      "[Eval 38/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9482 (took 8.5s)\n",
      "[Eval 39/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9009 (took 7.4s)\n",
      "[Eval 40/120] layers=3 act=selu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9307 (took 8.6s)\n",
      "[Eval 41/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9445 (took 7.6s)\n",
      "[Eval 42/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9355 (took 6.5s)\n",
      "[Eval 43/120] layers=3 act=selu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9206 (took 8.4s)\n",
      "[Eval 44/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=32 l2=3e-05 -> val_auc=0.9379 (took 6.1s)\n",
      "[Eval 45/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9088 (took 6.6s)\n",
      "[Eval 46/120] layers=2 act=selu opt=rmsprop lr=3.08e-05 bs=16 l2=8e-05 -> val_auc=0.7990 (took 5.9s)\n",
      "[Eval 47/120] layers=2 act=relu opt=nadam lr=2.06e-05 bs=8 l2=1e-08 -> val_auc=0.8194 (took 8.4s)\n",
      "[Eval 48/120] layers=2 act=relu opt=adam lr=3.08e-05 bs=8 l2=1e-08 -> val_auc=0.7791 (took 7.1s)\n",
      "[Eval 49/120] layers=2 act=tanh opt=adam lr=6.11e-03 bs=16 l2=1e-07 -> val_auc=0.9067 (took 5.3s)\n",
      "[Eval 50/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9329 (took 7.4s)\n",
      "[Eval 51/120] layers=2 act=tanh opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9542 (took 7.8s)\n",
      "[Eval 52/120] layers=2 act=selu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9050 (took 5.9s)\n",
      "[Eval 53/120] layers=2 act=relu opt=rmsprop lr=2.06e-05 bs=8 l2=8e-05 -> val_auc=0.7398 (took 7.3s)\n",
      "[Eval 54/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9079 (took 6.7s)\n",
      "[Eval 55/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9337 (took 6.5s)\n",
      "[Eval 56/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9182 (took 7.2s)\n",
      "[Eval 57/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9067 (took 5.6s)\n",
      "[Eval 58/120] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 7.9s)\n",
      "[Eval 59/120] layers=3 act=relu opt=nadam lr=6.11e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 10.0s)\n",
      "[Eval 60/120] layers=3 act=relu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9235 (took 7.6s)\n",
      "[Eval 61/120] layers=2 act=tanh opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9391 (took 7.6s)\n",
      "[Eval 62/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.8724 (took 6.2s)\n",
      "[Eval 63/120] layers=3 act=relu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9199 (took 7.1s)\n",
      "[Eval 64/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9457 (took 8.1s)\n",
      "[Eval 65/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9069 (took 6.9s)\n",
      "[Eval 66/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9013 (took 5.5s)\n",
      "[Eval 67/120] layers=2 act=selu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9526 (took 5.7s)\n",
      "[Eval 68/120] layers=2 act=relu opt=rmsprop lr=2.06e-05 bs=8 l2=8e-05 -> val_auc=0.7387 (took 7.0s)\n",
      "[Eval 69/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9440 (took 7.6s)\n",
      "[Eval 70/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=32 l2=3e-05 -> val_auc=0.8940 (took 5.5s)\n",
      "[Eval 71/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9269 (took 6.2s)\n",
      "[Eval 72/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9420 (took 6.3s)\n",
      "[Eval 73/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9005 (took 6.1s)\n",
      "[Eval 74/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9166 (took 7.1s)\n",
      "[Eval 75/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9520 (took 6.6s)\n",
      "[Eval 76/120] layers=2 act=tanh opt=nadam lr=6.11e-03 bs=16 l2=1e-02 -> val_auc=0.8622 (took 6.2s)\n",
      "[Eval 77/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9508 (took 8.0s)\n",
      "[Eval 78/120] layers=3 act=tanh opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9427 (took 6.9s)\n",
      "[Eval 79/120] layers=3 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.8960 (took 8.8s)\n",
      "[Eval 80/120] layers=3 act=tanh opt=nadam lr=6.11e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 8.2s)\n",
      "[Eval 81/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9025 (took 6.2s)\n",
      "[Eval 82/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9482 (took 8.1s)\n",
      "[Eval 83/120] layers=3 act=tanh opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9528 (took 7.2s)\n",
      "[Eval 84/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9469 (took 6.8s)\n",
      "[Eval 85/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9501 (took 6.7s)\n",
      "[Eval 86/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9274 (took 7.2s)\n",
      "[Eval 87/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9483 (took 6.8s)\n",
      "[Eval 88/120] layers=3 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9154 (took 8.9s)\n",
      "[Eval 89/120] layers=2 act=selu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9503 (took 6.1s)\n",
      "[Eval 90/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9390 (took 8.0s)\n",
      "[Eval 91/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9004 (took 6.0s)\n",
      "[Eval 92/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9453 (took 6.9s)\n",
      "[Eval 93/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.8787 (took 6.6s)\n",
      "[Eval 94/120] layers=2 act=relu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.8029 (took 5.0s)\n",
      "[Eval 95/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9515 (took 7.8s)\n",
      "[Eval 96/120] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 7.2s)\n",
      "[Eval 97/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=32 l2=8e-05 -> val_auc=0.9089 (took 6.4s)\n",
      "[Eval 98/120] layers=3 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.8845 (took 8.4s)\n",
      "[Eval 99/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.8930 (took 7.0s)\n",
      "[Eval 100/120] layers=2 act=relu opt=nadam lr=6.11e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 7.2s)\n",
      "[Eval 101/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9462 (took 7.9s)\n",
      "[Eval 102/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9513 (took 7.0s)\n",
      "[Eval 103/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=32 l2=8e-05 -> val_auc=0.9121 (took 6.3s)\n",
      "[Eval 104/120] layers=2 act=selu opt=nadam lr=6.11e-03 bs=16 l2=3e-05 -> val_auc=0.9405 (took 6.7s)\n",
      "[Eval 105/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9028 (took 7.4s)\n",
      "[Eval 106/120] layers=3 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.8843 (took 8.4s)\n",
      "[Eval 107/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9075 (took 7.2s)\n",
      "[Eval 108/120] layers=2 act=relu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.9158 (took 6.1s)\n",
      "[Eval 109/120] layers=3 act=tanh opt=nadam lr=6.11e-03 bs=32 l2=8e-05 -> val_auc=0.9513 (took 7.4s)\n",
      "[Eval 110/120] layers=2 act=selu opt=rmsprop lr=6.11e-03 bs=16 l2=8e-05 -> val_auc=0.8623 (took 5.5s)\n",
      "[Eval 111/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9015 (took 7.0s)\n",
      "[Eval 112/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9061 (took 6.6s)\n",
      "[Eval 113/120] layers=2 act=relu opt=nadam lr=1.00e-02 bs=32 l2=8e-05 -> val_auc=0.9068 (took 6.2s)\n",
      "[Eval 114/120] layers=2 act=selu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9440 (took 7.7s)\n",
      "[Eval 115/120] layers=2 act=selu opt=nadam lr=1.00e-02 bs=32 l2=8e-05 -> val_auc=0.9092 (took 6.4s)\n",
      "[Eval 116/120] layers=2 act=selu opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9249 (took 8.1s)\n",
      "[Eval 117/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.7655 (took 6.9s)\n",
      "[Eval 118/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=32 l2=8e-05 -> val_auc=0.9225 (took 6.6s)\n",
      "[Eval 119/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.6769 (took 6.7s)\n",
      "[Eval 120/120] layers=2 act=tanh opt=nadam lr=1.00e-02 bs=16 l2=8e-05 -> val_auc=0.9545 (took 8.8s)\n",
      "\n",
      "=== ğŸ¦‹ MBO Result ===\n",
      "Best Params: {'n_layers': 3, 'units': (154, 48), 'dropout': 0.19635925798121862, 'lr': 0.006113035841699416, 'optimizer': 'nadam', 'batch_size': 32, 'l2': 7.746989065246282e-05, 'activation': 'tanh'}\n",
      "Best Validation AUC (approx): 0.9528053402900696\n",
      "\n",
      "ğŸ¯ Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - AUC: 0.5669 - loss: 0.7240 - val_AUC: 0.7276 - val_loss: 0.6682\n",
      "Epoch 2/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.7599 - loss: 0.6088 - val_AUC: 0.7734 - val_loss: 0.6255\n",
      "Epoch 3/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.8611 - loss: 0.4921 - val_AUC: 0.8201 - val_loss: 0.6053\n",
      "Epoch 4/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.8938 - loss: 0.4445 - val_AUC: 0.8452 - val_loss: 0.5642\n",
      "Epoch 5/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9292 - loss: 0.3757 - val_AUC: 0.8570 - val_loss: 0.5635\n",
      "Epoch 6/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9424 - loss: 0.3471 - val_AUC: 0.8286 - val_loss: 0.6497\n",
      "Epoch 7/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9583 - loss: 0.3009 - val_AUC: 0.8445 - val_loss: 0.6669\n",
      "Epoch 8/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9568 - loss: 0.3052 - val_AUC: 0.8262 - val_loss: 0.6868\n",
      "Final short validation AUC (from build_and_eval): 0.9359983205795288\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - AUC: 0.5758 - accuracy: 0.5336 - loss: 0.7202 - val_AUC: 0.7432 - val_accuracy: 0.6104 - val_loss: 0.6505\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.7567 - accuracy: 0.7053 - loss: 0.6165 - val_AUC: 0.7747 - val_accuracy: 0.6623 - val_loss: 0.6482\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.8647 - accuracy: 0.7889 - loss: 0.4969 - val_AUC: 0.8174 - val_accuracy: 0.7013 - val_loss: 0.6064\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9028 - accuracy: 0.8144 - loss: 0.4301 - val_AUC: 0.8438 - val_accuracy: 0.7273 - val_loss: 0.5937\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9211 - accuracy: 0.8445 - loss: 0.3942 - val_AUC: 0.8364 - val_accuracy: 0.7273 - val_loss: 0.6397\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9482 - accuracy: 0.8585 - loss: 0.3288 - val_AUC: 0.8337 - val_accuracy: 0.7532 - val_loss: 0.6720\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9501 - accuracy: 0.8747 - loss: 0.3245 - val_AUC: 0.8465 - val_accuracy: 0.7792 - val_loss: 0.6319\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9597 - accuracy: 0.8747 - loss: 0.2971 - val_AUC: 0.8360 - val_accuracy: 0.7532 - val_loss: 0.6447\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - AUC: 0.9580 - accuracy: 0.8770 - loss: 0.3069 - val_AUC: 0.8584 - val_accuracy: 0.7662 - val_loss: 0.5882\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9627 - accuracy: 0.8840 - loss: 0.2916 - val_AUC: 0.8459 - val_accuracy: 0.7273 - val_loss: 0.6736\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9551 - accuracy: 0.8701 - loss: 0.3143 - val_AUC: 0.8479 - val_accuracy: 0.7792 - val_loss: 0.7025\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9576 - accuracy: 0.8747 - loss: 0.3123 - val_AUC: 0.8018 - val_accuracy: 0.6883 - val_loss: 0.8702\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9601 - accuracy: 0.8840 - loss: 0.3076 - val_AUC: 0.8591 - val_accuracy: 0.8052 - val_loss: 0.5754\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9712 - accuracy: 0.9049 - loss: 0.2700 - val_AUC: 0.8516 - val_accuracy: 0.8182 - val_loss: 0.6159\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9772 - accuracy: 0.9142 - loss: 0.2461 - val_AUC: 0.8042 - val_accuracy: 0.7013 - val_loss: 0.9357\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9718 - accuracy: 0.8956 - loss: 0.2704 - val_AUC: 0.8445 - val_accuracy: 0.7662 - val_loss: 0.7369\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9801 - accuracy: 0.9118 - loss: 0.2315 - val_AUC: 0.8428 - val_accuracy: 0.7922 - val_loss: 0.7883\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9780 - accuracy: 0.9072 - loss: 0.2431 - val_AUC: 0.8381 - val_accuracy: 0.7662 - val_loss: 0.8124\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9786 - accuracy: 0.9095 - loss: 0.2392 - val_AUC: 0.8215 - val_accuracy: 0.7403 - val_loss: 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Results: Loss=0.5657, Acc=0.8125, AUC=0.8822\n",
      "ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  \n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    n_layers = params['n_layers']\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "    activation = params['activation']\n",
    "\n",
    "    \n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    \n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            note += \"No val_auc in history. \"\n",
    "            \n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "            note += f\"Used training AUC instead: {val_auc:.4f}. \"\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"activation\": activation,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # Print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={n_layers} act={activation} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc  # minimize negative AUC\n",
    "\n",
    "# Run optimization\n",
    "\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 1.0 # Increased from 0.6, adjust as needed\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings - Increased from 10 for better search\n",
    "MAX_EVALS = 120 # Adjust as needed, e.g., 60, 120\n",
    "POP_SIZE = 10\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"ğŸš€ Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== ğŸ¦‹ MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\nğŸ¯ Retraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "\n",
    "# IMPORTANT: Call without eval_idx=\"final\" to avoid logging issues and simplify final train\n",
    "# We still want verbose=1 to see progress during final training\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1)\n",
    "                               \n",
    "\n",
    "print(\"Final short validation AUC (from build_and_eval):\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nâœ… Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66024997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting MBO search: max_evals=180, pop_size=15. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n",
      "[Eval 1/180] layers=1 act=relu opt=rmsprop lr=6.95e-03 bs=4 l2=4e-07 -> val_auc=0.8567 (took 8.7s)\n",
      "[Eval 2/180] layers=3 act=relu opt=nadam lr=1.51e-05 bs=16 l2=1e-06 -> val_auc=0.5000 (took 9.1s)\n",
      "[Eval 3/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=2e-03 -> val_auc=0.9374 (took 8.1s)\n",
      "[Eval 4/180] layers=1 act=tanh opt=rmsprop lr=3.52e-04 bs=8 l2=3e-05 -> val_auc=0.8800 (took 7.8s)\n",
      "[Eval 5/180] layers=2 act=relu opt=rmsprop lr=8.64e-03 bs=4 l2=2e-08 -> val_auc=0.8618 (took 9.3s)\n",
      "[Eval 6/180] layers=1 act=selu opt=adam lr=1.06e-03 bs=32 l2=5e-05 -> val_auc=0.9282 (took 5.1s)\n",
      "[Eval 7/180] layers=2 act=tanh opt=adam lr=1.36e-04 bs=8 l2=0e+00 -> val_auc=0.8843 (took 7.9s)\n",
      "[Eval 8/180] layers=2 act=relu opt=nadam lr=5.04e-05 bs=32 l2=6e-08 -> val_auc=0.7331 (took 6.8s)\n",
      "[Eval 9/180] layers=2 act=tanh opt=nadam lr=1.46e-04 bs=8 l2=7e-04 -> val_auc=0.8933 (took 14.4s)\n",
      "[Eval 10/180] layers=2 act=relu opt=nadam lr=1.63e-05 bs=32 l2=5e-07 -> val_auc=0.7372 (took 8.0s)\n",
      "[Eval 11/180] layers=3 act=tanh opt=rmsprop lr=1.84e-04 bs=8 l2=9e-03 -> val_auc=0.5000 (took 9.5s)\n",
      "[Eval 12/180] layers=2 act=relu opt=rmsprop lr=1.27e-05 bs=4 l2=2e-05 -> val_auc=0.7908 (took 14.2s)\n",
      "[Eval 13/180] layers=2 act=relu opt=rmsprop lr=6.41e-04 bs=32 l2=2e-07 -> val_auc=0.8756 (took 6.8s)\n",
      "[Eval 14/180] layers=2 act=relu opt=nadam lr=8.57e-04 bs=4 l2=9e-03 -> val_auc=0.5000 (took 10.3s)\n",
      "[Eval 15/180] layers=1 act=tanh opt=rmsprop lr=4.89e-04 bs=32 l2=4e-07 -> val_auc=0.8821 (took 5.7s)\n",
      "[Eval 16/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9059 (took 23.8s)\n",
      "[Eval 17/180] layers=1 act=selu opt=nadam lr=1.84e-04 bs=4 l2=9e-03 -> val_auc=0.8894 (took 13.7s)\n",
      "[Eval 18/180] layers=2 act=tanh opt=rmsprop lr=1.06e-03 bs=32 l2=5e-07 -> val_auc=0.9167 (took 7.0s)\n",
      "[Eval 19/180] layers=1 act=relu opt=rmsprop lr=4.89e-04 bs=8 l2=5e-05 -> val_auc=0.9220 (took 7.7s)\n",
      "[Eval 20/180] layers=1 act=relu opt=rmsprop lr=1.46e-04 bs=8 l2=4e-07 -> val_auc=0.8652 (took 6.2s)\n",
      "[Eval 21/180] layers=2 act=tanh opt=rmsprop lr=1.84e-04 bs=32 l2=2e-03 -> val_auc=0.7668 (took 7.1s)\n",
      "[Eval 22/180] layers=1 act=relu opt=rmsprop lr=6.95e-03 bs=4 l2=2e-05 -> val_auc=0.8678 (took 9.9s)\n",
      "[Eval 23/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8338 (took 8.4s)\n",
      "[Eval 24/180] layers=3 act=tanh opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9095 (took 12.6s)\n",
      "[Eval 25/180] layers=3 act=relu opt=adam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.8877 (took 13.3s)\n",
      "[Eval 26/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8173 (took 10.3s)\n",
      "[Eval 27/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9255 (took 10.1s)\n",
      "[Eval 28/180] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 10.1s)\n",
      "[Eval 29/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9281 (took 20.7s)\n",
      "[Eval 30/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9378 (took 21.1s)\n",
      "[Eval 31/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9125 (took 20.9s)\n",
      "[Eval 32/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9302 (took 20.7s)\n",
      "[Eval 33/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9400 (took 10.5s)\n",
      "[Eval 34/180] layers=1 act=relu opt=rmsprop lr=4.89e-04 bs=8 l2=5e-05 -> val_auc=0.9313 (took 8.2s)\n",
      "[Eval 35/180] layers=2 act=tanh opt=rmsprop lr=1.06e-03 bs=32 l2=5e-07 -> val_auc=0.9137 (took 7.3s)\n",
      "[Eval 36/180] layers=3 act=tanh opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9124 (took 13.0s)\n",
      "[Eval 37/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9298 (took 23.9s)\n",
      "[Eval 38/180] layers=1 act=selu opt=nadam lr=1.84e-04 bs=4 l2=9e-03 -> val_auc=0.8880 (took 13.4s)\n",
      "[Eval 39/180] layers=3 act=relu opt=adam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.8934 (took 13.8s)\n",
      "[Eval 40/180] layers=1 act=relu opt=rmsprop lr=6.95e-03 bs=4 l2=2e-05 -> val_auc=0.9026 (took 12.4s)\n",
      "[Eval 41/180] layers=1 act=relu opt=rmsprop lr=1.46e-04 bs=8 l2=4e-07 -> val_auc=0.8566 (took 6.6s)\n",
      "[Eval 42/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8274 (took 8.9s)\n",
      "[Eval 43/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8093 (took 10.5s)\n",
      "[Eval 44/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=2e-03 -> val_auc=0.9362 (took 9.4s)\n",
      "[Eval 45/180] layers=1 act=selu opt=adam lr=1.06e-03 bs=32 l2=5e-05 -> val_auc=0.9156 (took 5.9s)\n",
      "[Eval 46/180] layers=1 act=relu opt=rmsprop lr=5.87e-04 bs=4 l2=4e-07 -> val_auc=0.8944 (took 11.3s)\n",
      "[Eval 47/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8207 (took 10.6s)\n",
      "[Eval 48/180] layers=1 act=relu opt=rmsprop lr=1.36e-04 bs=8 l2=9e-03 -> val_auc=0.8000 (took 6.7s)\n",
      "[Eval 49/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9078 (took 26.0s)\n",
      "[Eval 50/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9219 (took 15.3s)\n",
      "[Eval 51/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=2e-03 -> val_auc=0.5000 (took 21.5s)\n",
      "[Eval 52/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9027 (took 26.6s)\n",
      "[Eval 53/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9066 (took 10.2s)\n",
      "[Eval 54/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 9.9s)\n",
      "[Eval 55/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9384 (took 10.2s)\n",
      "[Eval 56/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9475 (took 25.3s)\n",
      "[Eval 57/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8240 (took 22.0s)\n",
      "[Eval 58/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-02 -> val_auc=0.8818 (took 17.4s)\n",
      "[Eval 59/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8524 (took 10.8s)\n",
      "[Eval 60/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7906 (took 12.6s)\n",
      "[Eval 61/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9505 (took 22.8s)\n",
      "[Eval 62/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9110 (took 9.0s)\n",
      "[Eval 63/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9216 (took 13.5s)\n",
      "[Eval 64/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9080 (took 23.7s)\n",
      "[Eval 65/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.8729 (took 9.1s)\n",
      "[Eval 66/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=4 l2=5e-05 -> val_auc=0.9165 (took 26.5s)\n",
      "[Eval 67/180] layers=1 act=relu opt=rmsprop lr=5.87e-04 bs=4 l2=4e-07 -> val_auc=0.8974 (took 12.3s)\n",
      "[Eval 68/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-02 -> val_auc=0.8840 (took 18.0s)\n",
      "[Eval 69/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8572 (took 11.1s)\n",
      "[Eval 70/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8984 (took 22.6s)\n",
      "[Eval 71/180] layers=3 act=relu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8224 (took 11.7s)\n",
      "[Eval 72/180] layers=1 act=relu opt=rmsprop lr=1.36e-04 bs=8 l2=9e-03 -> val_auc=0.7952 (took 7.1s)\n",
      "[Eval 73/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8654 (took 13.3s)\n",
      "[Eval 74/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9344 (took 10.5s)\n",
      "[Eval 75/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=2e-03 -> val_auc=0.9348 (took 9.7s)\n",
      "[Eval 76/180] layers=3 act=relu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7185 (took 20.1s)\n",
      "[Eval 77/180] layers=1 act=relu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8371 (took 7.9s)\n",
      "[Eval 78/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9294 (took 9.1s)\n",
      "[Eval 79/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9503 (took 23.2s)\n",
      "[Eval 80/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9390 (took 26.7s)\n",
      "[Eval 81/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9302 (took 27.1s)\n",
      "[Eval 82/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=4e-07 -> val_auc=0.9298 (took 18.0s)\n",
      "[Eval 83/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9188 (took 10.6s)\n",
      "[Eval 84/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9485 (took 25.7s)\n",
      "[Eval 85/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-02 -> val_auc=0.9034 (took 27.3s)\n",
      "[Eval 86/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-02 -> val_auc=0.7885 (took 11.5s)\n",
      "[Eval 87/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8687 (took 11.6s)\n",
      "[Eval 88/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8911 (took 12.2s)\n",
      "[Eval 89/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 11.2s)\n",
      "[Eval 90/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9236 (took 10.8s)\n",
      "[Eval 91/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9515 (took 23.0s)\n",
      "[Eval 92/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9426 (took 28.0s)\n",
      "[Eval 93/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9432 (took 26.1s)\n",
      "[Eval 94/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9430 (took 27.7s)\n",
      "[Eval 95/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=4e-07 -> val_auc=0.9254 (took 18.7s)\n",
      "[Eval 96/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9447 (took 10.3s)\n",
      "[Eval 97/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9311 (took 11.9s)\n",
      "[Eval 98/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.8966 (took 10.6s)\n",
      "[Eval 99/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-02 -> val_auc=0.8982 (took 27.3s)\n",
      "[Eval 100/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8941 (took 11.8s)\n",
      "[Eval 101/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8647 (took 11.5s)\n",
      "[Eval 102/180] layers=1 act=relu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8460 (took 9.3s)\n",
      "[Eval 103/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-02 -> val_auc=0.7825 (took 11.4s)\n",
      "[Eval 104/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9358 (took 27.4s)\n",
      "[Eval 105/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=2e-03 -> val_auc=0.9285 (took 11.1s)\n",
      "[Eval 106/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9198 (took 21.8s)\n",
      "[Eval 107/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9158 (took 16.0s)\n",
      "[Eval 108/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9358 (took 27.6s)\n",
      "[Eval 109/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7469 (took 20.5s)\n",
      "[Eval 110/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7953 (took 27.3s)\n",
      "[Eval 111/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=4e-07 -> val_auc=0.8097 (took 21.5s)\n",
      "[Eval 112/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8813 (took 15.4s)\n",
      "[Eval 113/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9470 (took 21.5s)\n",
      "[Eval 114/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 13.3s)\n",
      "[Eval 115/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-02 -> val_auc=0.8522 (took 11.0s)\n",
      "[Eval 116/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9121 (took 15.2s)\n",
      "[Eval 117/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9513 (took 25.9s)\n",
      "[Eval 118/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8782 (took 11.1s)\n",
      "[Eval 119/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9255 (took 12.8s)\n",
      "[Eval 120/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9539 (took 13.1s)\n",
      "[Eval 121/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9411 (took 12.4s)\n",
      "[Eval 122/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9510 (took 26.0s)\n",
      "[Eval 123/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9502 (took 21.5s)\n",
      "[Eval 124/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9313 (took 26.9s)\n",
      "[Eval 125/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9428 (took 13.2s)\n",
      "[Eval 126/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9169 (took 23.5s)\n",
      "[Eval 127/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=4 l2=1e-06 -> val_auc=0.9181 (took 17.6s)\n",
      "[Eval 128/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9242 (took 15.8s)\n",
      "[Eval 129/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.8029 (took 24.2s)\n",
      "[Eval 130/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8271 (took 10.9s)\n",
      "[Eval 131/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-02 -> val_auc=0.8510 (took 15.0s)\n",
      "[Eval 132/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=4e-07 -> val_auc=0.8460 (took 30.2s)\n",
      "[Eval 133/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7830 (took 24.2s)\n",
      "[Eval 134/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9530 (took 23.7s)\n",
      "[Eval 135/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9392 (took 11.5s)\n",
      "[Eval 136/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8149 (took 11.3s)\n",
      "[Eval 137/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5000 (took 17.5s)\n",
      "[Eval 138/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9413 (took 23.9s)\n",
      "[Eval 139/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9500 (took 13.5s)\n",
      "[Eval 140/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9259 (took 11.6s)\n",
      "[Eval 141/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9425 (took 15.3s)\n",
      "[Eval 142/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5000 (took 16.9s)\n",
      "[Eval 143/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9238 (took 10.8s)\n",
      "[Eval 144/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 11.8s)\n",
      "[Eval 145/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9521 (took 12.2s)\n",
      "[Eval 146/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 10.9s)\n",
      "[Eval 147/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8541 (took 10.6s)\n",
      "[Eval 148/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-02 -> val_auc=0.8878 (took 12.1s)\n",
      "[Eval 149/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9567 (took 12.0s)\n",
      "[Eval 150/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8940 (took 10.9s)\n",
      "[Eval 151/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9276 (took 10.5s)\n",
      "[Eval 152/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9302 (took 11.3s)\n",
      "[Eval 153/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9388 (took 12.2s)\n",
      "[Eval 154/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9349 (took 14.2s)\n",
      "[Eval 155/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9246 (took 22.6s)\n",
      "[Eval 156/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9198 (took 24.0s)\n",
      "[Eval 157/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.9312 (took 12.0s)\n",
      "[Eval 158/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8890 (took 11.7s)\n",
      "[Eval 159/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-02 -> val_auc=0.8854 (took 12.5s)\n",
      "[Eval 160/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8645 (took 10.4s)\n",
      "[Eval 161/180] layers=3 act=selu opt=nadam lr=1.36e-04 bs=32 l2=1e-06 -> val_auc=0.8146 (took 10.9s)\n",
      "[Eval 162/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5852 (took 13.0s)\n",
      "[Eval 163/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5000 (took 17.0s)\n",
      "[Eval 164/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9475 (took 26.4s)\n",
      "[Eval 165/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-06 -> val_auc=0.9595 (took 24.3s)\n",
      "[Eval 166/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=4 l2=1e-02 -> val_auc=0.8803 (took 23.6s)\n",
      "[Eval 167/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7247 (took 12.8s)\n",
      "[Eval 168/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9485 (took 13.3s)\n",
      "[Eval 169/180] layers=3 act=selu opt=nadam lr=5.87e-04 bs=32 l2=1e-06 -> val_auc=0.8791 (took 11.8s)\n",
      "[Eval 170/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=1e-06 -> val_auc=0.7191 (took 15.2s)\n",
      "[Eval 171/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9013 (took 13.8s)\n",
      "[Eval 172/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9445 (took 14.1s)\n",
      "[Eval 173/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8712 (took 12.8s)\n",
      "[Eval 174/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-06 -> val_auc=0.9370 (took 13.8s)\n",
      "[Eval 175/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 13.7s)\n",
      "[Eval 176/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 27.9s)\n",
      "[Eval 177/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8350 (took 52.5s)\n",
      "[Eval 178/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8523 (took 25.0s)\n",
      "[Eval 179/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8667 (took 53.0s)\n",
      "[Eval 180/180] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 13.3s)\n",
      "\n",
      "=== ğŸ¦‹ MBO Result ===\n",
      "Best Params: {'n_layers': 3, 'units': (252, 179), 'dropout': 0.7, 'lr': 0.0005865800946979541, 'optimizer': 'nadam', 'batch_size': 4, 'l2': 1.3166325076383184e-06, 'activation': 'selu'}\n",
      "Best Validation AUC (approx): 0.9595375061035156\n",
      "\n",
      "ğŸ¯ Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - AUC: 0.6209 - loss: 0.6720 - val_AUC: 0.7446 - val_loss: 0.6117\n",
      "Epoch 2/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - AUC: 0.7851 - loss: 0.5577 - val_AUC: 0.7818 - val_loss: 0.6068\n",
      "Epoch 3/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - AUC: 0.8665 - loss: 0.4545 - val_AUC: 0.8059 - val_loss: 0.6507\n",
      "Epoch 4/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - AUC: 0.8949 - loss: 0.4074 - val_AUC: 0.8222 - val_loss: 0.6058\n",
      "Epoch 5/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - AUC: 0.9215 - loss: 0.3499 - val_AUC: 0.8276 - val_loss: 0.7137\n",
      "Epoch 6/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - AUC: 0.9394 - loss: 0.3105 - val_AUC: 0.8310 - val_loss: 0.8182\n",
      "Epoch 7/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - AUC: 0.9471 - loss: 0.2892 - val_AUC: 0.8276 - val_loss: 0.9020\n",
      "Final short validation AUC (from build_and_eval): 0.9176012873649597\n",
      "Epoch 1/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - AUC: 0.6309 - accuracy: 0.5963 - loss: 0.6690 - val_AUC: 0.7415 - val_accuracy: 0.6234 - val_loss: 0.6115\n",
      "Epoch 2/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - AUC: 0.7913 - accuracy: 0.7100 - loss: 0.5556 - val_AUC: 0.8045 - val_accuracy: 0.6883 - val_loss: 0.5709\n",
      "Epoch 3/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - AUC: 0.8557 - accuracy: 0.7726 - loss: 0.4771 - val_AUC: 0.8337 - val_accuracy: 0.7403 - val_loss: 0.5357\n",
      "Epoch 4/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - AUC: 0.8925 - accuracy: 0.8121 - loss: 0.4136 - val_AUC: 0.8384 - val_accuracy: 0.7532 - val_loss: 0.5445\n",
      "Epoch 5/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - AUC: 0.9258 - accuracy: 0.8353 - loss: 0.3507 - val_AUC: 0.8398 - val_accuracy: 0.7662 - val_loss: 0.5931\n",
      "Epoch 6/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - AUC: 0.9436 - accuracy: 0.8538 - loss: 0.3006 - val_AUC: 0.8374 - val_accuracy: 0.7532 - val_loss: 0.6646\n",
      "Epoch 7/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - AUC: 0.9416 - accuracy: 0.8561 - loss: 0.3039 - val_AUC: 0.8286 - val_accuracy: 0.7403 - val_loss: 0.8361\n",
      "Epoch 8/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - AUC: 0.9466 - accuracy: 0.8492 - loss: 0.2971 - val_AUC: 0.8411 - val_accuracy: 0.7273 - val_loss: 0.7681\n",
      "Epoch 9/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - AUC: 0.9579 - accuracy: 0.8724 - loss: 0.2613 - val_AUC: 0.8340 - val_accuracy: 0.7143 - val_loss: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Results: Loss=0.5348, Acc=0.7578, AUC=0.8588\n",
      "ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Fix seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "# init CSV with header - Added 'activation' and 'n_layers'\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ===============================\n",
    "# Decode solution vector -> params\n",
    "# ===============================\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Build dynamic LSTM\n",
    "# ===============================\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    # Dynamic layer setup\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  # 3 layers\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# Train + evaluate function (Enhanced with fallback and safe logging)\n",
    "# ===============================\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    # Extract parameters\n",
    "    n_layers = params['n_layers']\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "    activation = params['activation']\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        # Prefer last val_auc if available\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            # Fallback: evaluate on training data (not ideal for validation, but prevents 0.0)\n",
    "            # Or just note that val_auc was missing\n",
    "            note += \"No val_auc in history. \"\n",
    "            # Let's evaluate on the training data as a very last resort (though not a true validation score)\n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "            note += f\"Used training AUC instead: {val_auc:.4f}. \"\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Log to CSV - Ensure eval_idx is safe for CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"activation\": activation,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # Print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={n_layers} act={activation} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "# ===============================\n",
    "# Define NiaPy Problem\n",
    "# ===============================\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc  # minimize negative AUC\n",
    "\n",
    "# ===============================\n",
    "# Run optimization\n",
    "# ===============================\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 1.0 # Increased from 0.6, adjust as needed\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings - Increased from 10 for better search\n",
    "MAX_EVALS = 180 # Adjust as needed, e.g., 60, 120\n",
    "POP_SIZE = 15\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"ğŸš€ Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== ğŸ¦‹ MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\nğŸ¯ Retraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "\n",
    "# IMPORTANT: Call without eval_idx=\"final\" to avoid logging issues and simplify final train\n",
    "# We still want verbose=1 to see progress during final training\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1)\n",
    "                               # Removed: eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final short validation AUC (from build_and_eval):\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nâœ… Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90793d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting MBO search: max_evals=100, pop_size=10. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 23:59:56.001064: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval 1/100] layers=1 act=tanh opt=rmsprop lr=2.19e-04 bs=32 l2=3e-08 -> val_auc=0.8600 (took 4.8s)\n",
      "[Eval 2/100] layers=2 act=selu opt=adam lr=3.95e-04 bs=8 l2=4e-05 -> val_auc=0.9201 (took 9.9s)\n",
      "[Eval 3/100] layers=2 act=relu opt=rmsprop lr=1.34e-04 bs=16 l2=2e-06 -> val_auc=0.7844 (took 6.4s)\n",
      "[Eval 4/100] layers=2 act=relu opt=rmsprop lr=1.28e-04 bs=4 l2=3e-05 -> val_auc=0.8472 (took 9.3s)\n",
      "[Eval 5/100] layers=2 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=2e-03 -> val_auc=0.9213 (took 5.5s)\n",
      "[Eval 6/100] layers=3 act=relu opt=nadam lr=5.64e-03 bs=4 l2=7e-06 -> val_auc=0.8576 (took 17.0s)\n",
      "[Eval 7/100] layers=2 act=relu opt=nadam lr=6.97e-04 bs=32 l2=2e-03 -> val_auc=0.9050 (took 5.3s)\n",
      "[Eval 8/100] layers=2 act=selu opt=adam lr=2.94e-05 bs=8 l2=0e+00 -> val_auc=0.7813 (took 6.1s)\n",
      "[Eval 9/100] layers=2 act=relu opt=rmsprop lr=1.92e-05 bs=16 l2=4e-04 -> val_auc=0.7383 (took 7.0s)\n",
      "[Eval 10/100] layers=3 act=tanh opt=rmsprop lr=5.81e-03 bs=8 l2=8e-06 -> val_auc=0.9203 (took 11.9s)\n",
      "[Eval 11/100] layers=3 act=relu opt=rmsprop lr=1.28e-04 bs=4 l2=3e-05 -> val_auc=0.7833 (took 13.9s)\n",
      "[Eval 12/100] layers=2 act=relu opt=adam lr=1.34e-04 bs=8 l2=2e-06 -> val_auc=0.9028 (took 11.3s)\n",
      "[Eval 13/100] layers=3 act=selu opt=rmsprop lr=4.41e-04 bs=4 l2=3e-05 -> val_auc=0.9175 (took 11.2s)\n",
      "[Eval 14/100] layers=2 act=selu opt=adam lr=1.34e-04 bs=8 l2=0e+00 -> val_auc=0.9053 (took 9.5s)\n",
      "[Eval 15/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9366 (took 12.6s)\n",
      "[Eval 16/100] layers=3 act=relu opt=rmsprop lr=1.28e-04 bs=32 l2=3e-05 -> val_auc=0.5000 (took 6.0s)\n",
      "[Eval 17/100] layers=3 act=relu opt=rmsprop lr=1.00e-02 bs=32 l2=3e-05 -> val_auc=0.8472 (took 6.6s)\n",
      "[Eval 18/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=3e-05 -> val_auc=0.8610 (took 13.4s)\n",
      "[Eval 19/100] layers=3 act=relu opt=rmsprop lr=1.28e-04 bs=32 l2=3e-05 -> val_auc=0.5000 (took 7.1s)\n",
      "[Eval 20/100] layers=3 act=relu opt=nadam lr=1.00e-02 bs=4 l2=3e-05 -> val_auc=0.5000 (took 17.4s)\n",
      "[Eval 21/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9365 (took 12.9s)\n",
      "[Eval 22/100] layers=3 act=selu opt=rmsprop lr=4.41e-04 bs=4 l2=3e-05 -> val_auc=0.9274 (took 12.3s)\n",
      "[Eval 23/100] layers=2 act=selu opt=adam lr=1.34e-04 bs=8 l2=0e+00 -> val_auc=0.9113 (took 9.6s)\n",
      "[Eval 24/100] layers=2 act=relu opt=adam lr=1.34e-04 bs=8 l2=2e-06 -> val_auc=0.9047 (took 11.4s)\n",
      "[Eval 25/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=4 l2=3e-05 -> val_auc=0.8012 (took 14.1s)\n",
      "[Eval 26/100] layers=3 act=relu opt=rmsprop lr=1.00e-02 bs=32 l2=3e-05 -> val_auc=0.8441 (took 7.1s)\n",
      "[Eval 27/100] layers=3 act=relu opt=rmsprop lr=1.28e-04 bs=4 l2=3e-05 -> val_auc=0.7614 (took 14.3s)\n",
      "[Eval 28/100] layers=3 act=relu opt=rmsprop lr=1.28e-04 bs=32 l2=3e-05 -> val_auc=0.5000 (took 6.4s)\n",
      "[Eval 29/100] layers=2 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=2e-03 -> val_auc=0.9204 (took 6.7s)\n",
      "[Eval 30/100] layers=3 act=tanh opt=rmsprop lr=5.81e-03 bs=8 l2=8e-06 -> val_auc=0.8566 (took 10.7s)\n",
      "[Eval 31/100] layers=3 act=tanh opt=nadam lr=4.41e-04 bs=32 l2=3e-05 -> val_auc=0.9235 (took 9.1s)\n",
      "[Eval 32/100] layers=3 act=tanh opt=nadam lr=1.00e-02 bs=8 l2=3e-05 -> val_auc=0.9027 (took 12.0s)\n",
      "[Eval 33/100] layers=3 act=selu opt=rmsprop lr=1.28e-04 bs=8 l2=3e-05 -> val_auc=0.9016 (took 13.5s)\n",
      "[Eval 34/100] layers=3 act=tanh opt=rmsprop lr=1.00e-02 bs=8 l2=8e-06 -> val_auc=0.9155 (took 8.7s)\n",
      "[Eval 35/100] layers=3 act=selu opt=rmsprop lr=1.34e-04 bs=4 l2=3e-05 -> val_auc=0.9070 (took 13.4s)\n",
      "[Eval 36/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.8399 (took 12.1s)\n",
      "[Eval 37/100] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=2e-06 -> val_auc=0.8971 (took 8.6s)\n",
      "[Eval 38/100] layers=2 act=relu opt=nadam lr=1.00e-02 bs=8 l2=1e-02 -> val_auc=0.5000 (took 7.1s)\n",
      "[Eval 39/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9522 (took 10.7s)\n",
      "[Eval 40/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9181 (took 11.3s)\n",
      "[Eval 41/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9218 (took 9.8s)\n",
      "[Eval 42/100] layers=3 act=tanh opt=nadam lr=4.41e-04 bs=32 l2=3e-05 -> val_auc=0.9293 (took 8.3s)\n",
      "[Eval 43/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9145 (took 11.2s)\n",
      "[Eval 44/100] layers=3 act=tanh opt=rmsprop lr=1.00e-02 bs=8 l2=8e-06 -> val_auc=0.9275 (took 8.9s)\n",
      "[Eval 45/100] layers=3 act=selu opt=rmsprop lr=1.34e-04 bs=4 l2=3e-05 -> val_auc=0.9169 (took 13.5s)\n",
      "[Eval 46/100] layers=3 act=tanh opt=nadam lr=1.00e-02 bs=8 l2=3e-05 -> val_auc=0.8730 (took 9.5s)\n",
      "[Eval 47/100] layers=3 act=selu opt=rmsprop lr=1.28e-04 bs=8 l2=3e-05 -> val_auc=0.8977 (took 12.0s)\n",
      "[Eval 48/100] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=2e-06 -> val_auc=0.9024 (took 8.2s)\n",
      "[Eval 49/100] layers=2 act=relu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9135 (took 11.6s)\n",
      "[Eval 50/100] layers=3 act=selu opt=rmsprop lr=4.41e-04 bs=4 l2=3e-05 -> val_auc=0.9161 (took 11.8s)\n",
      "[Eval 51/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=2e-06 -> val_auc=0.9276 (took 8.7s)\n",
      "[Eval 52/100] layers=2 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9262 (took 9.8s)\n",
      "[Eval 53/100] layers=3 act=relu opt=nadam lr=1.34e-04 bs=32 l2=8e-06 -> val_auc=0.7927 (took 8.9s)\n",
      "[Eval 54/100] layers=3 act=selu opt=nadam lr=1.28e-04 bs=8 l2=3e-05 -> val_auc=0.9079 (took 14.4s)\n",
      "[Eval 55/100] layers=3 act=relu opt=rmsprop lr=1.00e-02 bs=8 l2=2e-06 -> val_auc=0.8109 (took 8.1s)\n",
      "[Eval 56/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.8972 (took 11.3s)\n",
      "[Eval 57/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9369 (took 14.3s)\n",
      "[Eval 58/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8616 (took 10.3s)\n",
      "[Eval 59/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=2e-06 -> val_auc=0.9382 (took 8.8s)\n",
      "[Eval 60/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9467 (took 12.1s)\n",
      "[Eval 61/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9491 (took 11.7s)\n",
      "[Eval 62/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=2e-06 -> val_auc=0.9043 (took 9.0s)\n",
      "[Eval 63/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9317 (took 12.3s)\n",
      "[Eval 64/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=2e-06 -> val_auc=0.9246 (took 9.3s)\n",
      "[Eval 65/100] layers=2 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9193 (took 10.1s)\n",
      "[Eval 66/100] layers=3 act=selu opt=nadam lr=1.28e-04 bs=8 l2=3e-05 -> val_auc=0.9009 (took 14.0s)\n",
      "[Eval 67/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.8968 (took 10.9s)\n",
      "[Eval 68/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8734 (took 9.3s)\n",
      "[Eval 69/100] layers=3 act=tanh opt=nadam lr=4.41e-04 bs=32 l2=3e-05 -> val_auc=0.9259 (took 8.7s)\n",
      "[Eval 70/100] layers=3 act=tanh opt=rmsprop lr=1.00e-02 bs=8 l2=8e-06 -> val_auc=0.9065 (took 9.1s)\n",
      "[Eval 71/100] layers=3 act=selu opt=nadam lr=1.28e-04 bs=32 l2=2e-06 -> val_auc=0.8468 (took 9.0s)\n",
      "[Eval 72/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=8e-06 -> val_auc=0.9143 (took 11.4s)\n",
      "[Eval 73/100] layers=3 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=3e-05 -> val_auc=0.9344 (took 11.9s)\n",
      "[Eval 74/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=3e-05 -> val_auc=0.9358 (took 8.6s)\n",
      "[Eval 75/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.8985 (took 13.6s)\n",
      "[Eval 76/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8818 (took 8.4s)\n",
      "[Eval 77/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.9043 (took 17.0s)\n",
      "[Eval 78/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=2e-06 -> val_auc=0.9404 (took 9.0s)\n",
      "[Eval 79/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=8 l2=1e-02 -> val_auc=0.5000 (took 12.9s)\n",
      "[Eval 80/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8771 (took 8.7s)\n",
      "[Eval 81/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=2e-06 -> val_auc=0.9194 (took 8.5s)\n",
      "[Eval 82/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=3e-05 -> val_auc=0.9363 (took 9.5s)\n",
      "[Eval 83/100] layers=3 act=selu opt=rmsprop lr=4.41e-04 bs=8 l2=3e-05 -> val_auc=0.9343 (took 12.6s)\n",
      "[Eval 84/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=8e-06 -> val_auc=0.9146 (took 10.9s)\n",
      "[Eval 85/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.8955 (took 15.1s)\n",
      "[Eval 86/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.9053 (took 13.8s)\n",
      "[Eval 87/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8759 (took 8.8s)\n",
      "[Eval 88/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8690 (took 8.5s)\n",
      "[Eval 89/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9307 (took 12.0s)\n",
      "[Eval 90/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=2e-06 -> val_auc=0.9400 (took 12.4s)\n",
      "[Eval 91/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=3e-05 -> val_auc=0.9565 (took 15.4s)\n",
      "[Eval 92/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=1e-02 -> val_auc=0.9045 (took 15.5s)\n",
      "[Eval 93/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=1e-02 -> val_auc=0.8791 (took 9.3s)\n",
      "[Eval 94/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=32 l2=2e-06 -> val_auc=0.9381 (took 8.8s)\n",
      "[Eval 95/100] layers=3 act=selu opt=nadam lr=4.41e-04 bs=8 l2=8e-06 -> val_auc=0.9249 (took 10.9s)\n",
      "[Eval 96/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8230 (took 8.0s)\n",
      "[Eval 97/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 8.7s)\n",
      "[Eval 98/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=2e-06 -> val_auc=0.9290 (took 8.4s)\n",
      "[Eval 99/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 9.0s)\n",
      "[Eval 100/100] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 9.3s)\n",
      "\n",
      "=== ğŸ¦‹ MBO Result ===\n",
      "Best Params: {'n_layers': 3, 'units': (146, 230), 'dropout': 0.36150823776400254, 'lr': 0.00044090751110961723, 'optimizer': 'nadam', 'batch_size': 8, 'l2': 1.518088846131132e-06, 'activation': 'selu'}\n",
      "Best Validation AUC (approx): 0.9491212368011475\n",
      "\n",
      "ğŸ“Š Generating convergence plot...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Convergence plot saved to mbo_logs/mbo_convergence_plot.png\n",
      "\n",
      "ğŸ¯ Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - AUC: 0.6200 - loss: 0.6743 - val_AUC: 0.7398 - val_loss: 0.6240\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.7959 - loss: 0.5500 - val_AUC: 0.7991 - val_loss: 0.5664\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.8826 - loss: 0.4274 - val_AUC: 0.8198 - val_loss: 0.6049\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9156 - loss: 0.3682 - val_AUC: 0.8354 - val_loss: 0.6432\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9351 - loss: 0.3218 - val_AUC: 0.8330 - val_loss: 0.7478\n",
      "Final short validation AUC (from build_and_eval): 0.8902461528778076\n",
      "Epoch 1/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - AUC: 0.6358 - accuracy: 0.5986 - loss: 0.6728 - val_AUC: 0.7375 - val_accuracy: 0.6623 - val_loss: 0.6251\n",
      "Epoch 2/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.8090 - accuracy: 0.7332 - loss: 0.5389 - val_AUC: 0.8120 - val_accuracy: 0.6623 - val_loss: 0.5640\n",
      "Epoch 3/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.8800 - accuracy: 0.7958 - loss: 0.4352 - val_AUC: 0.8144 - val_accuracy: 0.7273 - val_loss: 0.6163\n",
      "Epoch 4/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9144 - accuracy: 0.8399 - loss: 0.3711 - val_AUC: 0.8232 - val_accuracy: 0.7273 - val_loss: 0.6759\n",
      "Epoch 5/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9352 - accuracy: 0.8631 - loss: 0.3257 - val_AUC: 0.8255 - val_accuracy: 0.7143 - val_loss: 0.7868\n",
      "Epoch 6/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9463 - accuracy: 0.8561 - loss: 0.2959 - val_AUC: 0.8266 - val_accuracy: 0.7273 - val_loss: 0.7912\n",
      "Epoch 7/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9543 - accuracy: 0.8654 - loss: 0.2717 - val_AUC: 0.8147 - val_accuracy: 0.7013 - val_loss: 0.9632\n",
      "Epoch 8/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9582 - accuracy: 0.8701 - loss: 0.2589 - val_AUC: 0.8225 - val_accuracy: 0.7143 - val_loss: 0.8556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Results: Loss=0.5895, Acc=0.6641, AUC=0.8418\n",
      "ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Fix seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "# init CSV with header - Added 'activation' and 'n_layers'\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ===============================\n",
    "# Decode solution vector -> params\n",
    "# ===============================\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Build dynamic LSTM\n",
    "# ===============================\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    # Dynamic layer setup\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  # 3 layers\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# Train + evaluate function (Enhanced with fallback and safe logging)\n",
    "# ===============================\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    # Extract parameters\n",
    "    n_layers = params['n_layers']\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "    activation = params['activation']\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        # Prefer last val_auc if available\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            # Fallback: evaluate on training data (not ideal for validation, but prevents 0.0)\n",
    "            # Or just note that val_auc was missing\n",
    "            note += \"No val_auc in history. \"\n",
    "            # Let's evaluate on the training data as a very last resort (though not a true validation score)\n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "            note += f\"Used training AUC instead: {val_auc:.4f}. \"\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Log to CSV - Ensure eval_idx is safe for CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"activation\": activation,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # Print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={n_layers} act={activation} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "# ===============================\n",
    "# Define NiaPy Problem (Updated to track convergence)\n",
    "# ===============================\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "        # Add lists to store convergence data\n",
    "        self.eval_indices = []\n",
    "        self.best_fitness_so_far = []\n",
    "        self.current_best_fit = float('inf') # Start with worst possible (for minimization)\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0 # Assign worst score on error\n",
    "\n",
    "        fitness = -val_auc # Remember, we minimize the negative AUC\n",
    "\n",
    "        # Update best fitness found so far (for minimization)\n",
    "        if fitness < self.current_best_fit:\n",
    "            self.current_best_fit = fitness\n",
    "\n",
    "        # Store data point for convergence plot\n",
    "        self.eval_indices.append(self.eval_count)\n",
    "        self.best_fitness_so_far.append(self.current_best_fit)\n",
    "\n",
    "        return fitness # Return the calculated fitness (negative AUC)\n",
    "\n",
    "# ===============================\n",
    "# Run optimization\n",
    "# ===============================\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 1.0 # Increased from 0.6, adjust as needed\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings - Increased from 10 for better search\n",
    "MAX_EVALS = 100 # Adjust as needed, e.g., 60, 120\n",
    "POP_SIZE = 10\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"ğŸš€ Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== ğŸ¦‹ MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit) # Convert back to positive AUC\n",
    "\n",
    "# ===============================\n",
    "# Generate Convergence Plot\n",
    "# ===============================\n",
    "print(\"\\nğŸ“Š Generating convergence plot...\")\n",
    "# Convert negative fitness back to positive AUC for plotting\n",
    "convergence_auc = [-f for f in problem.best_fitness_so_far]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(problem.eval_indices, convergence_auc, marker='o', linestyle='-', color='blue')\n",
    "plt.title('MBO Optimization Convergence (Best Validation AUC over Evaluations)')\n",
    "plt.xlabel('Function Evaluation Count')\n",
    "plt.ylabel('Best Validation AUC Found So Far')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "convergence_plot_path = os.path.join(LOG_DIR, \"mbo_convergence_plot.png\")\n",
    "plt.savefig(convergence_plot_path)\n",
    "plt.show() # Display the plot in the notebook\n",
    "print(f\"ğŸ’¾ Convergence plot saved to {convergence_plot_path}\")\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\nğŸ¯ Retraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "\n",
    "# IMPORTANT: Call without eval_idx=\"final\" to avoid logging issues and simplify final train\n",
    "# We still want verbose=1 to see progress during final training\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1)\n",
    "                               # Removed: eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final short validation AUC (from build_and_eval):\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nâœ… Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"ğŸ’¾ Saved final model to mbo_best_lstm_fp_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
