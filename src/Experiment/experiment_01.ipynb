{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09186939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 14:51:43.649490: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-15 14:51:43.708718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-15 14:51:45.187651: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# prompt: import essential packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "#import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder # Atau OneHotEncoder\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem # Untuk MACCS Keys\n",
    "\n",
    "print(\"Packages imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f352bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PubchemFP0</th>\n",
       "      <th>PubchemFP1</th>\n",
       "      <th>PubchemFP2</th>\n",
       "      <th>PubchemFP3</th>\n",
       "      <th>PubchemFP4</th>\n",
       "      <th>PubchemFP5</th>\n",
       "      <th>PubchemFP6</th>\n",
       "      <th>PubchemFP7</th>\n",
       "      <th>PubchemFP8</th>\n",
       "      <th>...</th>\n",
       "      <th>PubchemFP873</th>\n",
       "      <th>PubchemFP874</th>\n",
       "      <th>PubchemFP875</th>\n",
       "      <th>PubchemFP876</th>\n",
       "      <th>PubchemFP877</th>\n",
       "      <th>PubchemFP878</th>\n",
       "      <th>PubchemFP879</th>\n",
       "      <th>PubchemFP880</th>\n",
       "      <th>acvalue</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44244736</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44244911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44245235</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10451021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44245073</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>145958114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>145950639</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>3168508</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>145952863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119.1000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>145955648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.9000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows × 884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  PubchemFP0  PubchemFP1  PubchemFP2  PubchemFP3  PubchemFP4  \\\n",
       "0     44244736           1           1           1           0           0   \n",
       "1     44244911           1           1           1           0           0   \n",
       "2     44245235           1           1           1           0           0   \n",
       "3     10451021           1           1           1           0           0   \n",
       "4     44245073           1           1           1           0           0   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "631  145958114           1           1           1           0           0   \n",
       "632  145950639           1           1           1           0           0   \n",
       "633    3168508           1           1           0           0           0   \n",
       "634  145952863           1           1           1           0           0   \n",
       "635  145955648           1           1           1           0           0   \n",
       "\n",
       "     PubchemFP5  PubchemFP6  PubchemFP7  PubchemFP8  ...  PubchemFP873  \\\n",
       "0             0           0           0           0  ...             0   \n",
       "1             0           0           0           0  ...             0   \n",
       "2             0           0           0           0  ...             0   \n",
       "3             0           0           0           0  ...             0   \n",
       "4             0           0           0           0  ...             0   \n",
       "..          ...         ...         ...         ...  ...           ...   \n",
       "631           0           0           0           0  ...             0   \n",
       "632           0           0           0           0  ...             0   \n",
       "633           0           0           0           0  ...             0   \n",
       "634           0           0           0           0  ...             0   \n",
       "635           0           0           0           0  ...             0   \n",
       "\n",
       "     PubchemFP874  PubchemFP875  PubchemFP876  PubchemFP877  PubchemFP878  \\\n",
       "0               0             0             0             0             0   \n",
       "1               0             0             0             0             0   \n",
       "2               0             0             0             0             0   \n",
       "3               0             0             0             0             0   \n",
       "4               0             0             0             0             0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "631             0             0             0             0             0   \n",
       "632             0             0             0             0             0   \n",
       "633             0             0             0             0             0   \n",
       "634             0             0             0             0             0   \n",
       "635             0             0             0             0             0   \n",
       "\n",
       "     PubchemFP879  PubchemFP880   acvalue  categories  \n",
       "0               0             0    0.0030   inhibitor  \n",
       "1               0             0    0.0035   inhibitor  \n",
       "2               0             0    0.0047   inhibitor  \n",
       "3               0             0    0.0090   inhibitor  \n",
       "4               0             0    0.0180   inhibitor  \n",
       "..            ...           ...       ...         ...  \n",
       "631             0             0  100.0000     neutral  \n",
       "632             0             0  100.0000     neutral  \n",
       "633             0             0  100.0000     neutral  \n",
       "634             0             0  119.1000     neutral  \n",
       "635             0             0  122.9000     neutral  \n",
       "\n",
       "[636 rows x 884 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/dito-adistya/Dito/TA/Coding/LSTM-MBA/data/full/dataFP.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d986529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "['inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor' 'inhibitor'\n",
      " 'inhibitor' 'inhibitor' 'inhibitor' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:882].values  # ambil FP0 sampai FP880\n",
    "y = data['categories'].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d36d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9aabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd093600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff1114f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:36:47.133525: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 280ms/step - accuracy: 0.4587 - loss: 0.6934 - val_accuracy: 0.5703 - val_loss: 0.6931\n",
      "Epoch 2/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 266ms/step - accuracy: 0.5551 - loss: 0.6929 - val_accuracy: 0.4688 - val_loss: 0.6944\n",
      "Epoch 3/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 264ms/step - accuracy: 0.5138 - loss: 0.6926 - val_accuracy: 0.4688 - val_loss: 0.6948\n",
      "Epoch 4/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 266ms/step - accuracy: 0.5157 - loss: 0.6924 - val_accuracy: 0.4688 - val_loss: 0.6925\n",
      "Epoch 5/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - accuracy: 0.5000 - loss: 0.6916 - val_accuracy: 0.5703 - val_loss: 0.6907\n",
      "Epoch 6/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 283ms/step - accuracy: 0.5354 - loss: 0.6903 - val_accuracy: 0.5156 - val_loss: 0.6912\n",
      "Epoch 7/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - accuracy: 0.5098 - loss: 0.6903 - val_accuracy: 0.4219 - val_loss: 0.6922\n",
      "Epoch 8/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 271ms/step - accuracy: 0.5551 - loss: 0.6866 - val_accuracy: 0.4297 - val_loss: 0.6968\n",
      "Epoch 9/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.5630 - loss: 0.6866 - val_accuracy: 0.4609 - val_loss: 0.7020\n",
      "Epoch 10/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5472 - loss: 0.6874 - val_accuracy: 0.5703 - val_loss: 0.6878\n",
      "Epoch 11/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 272ms/step - accuracy: 0.5315 - loss: 0.6862 - val_accuracy: 0.4453 - val_loss: 0.6914\n",
      "Epoch 12/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 273ms/step - accuracy: 0.5669 - loss: 0.6839 - val_accuracy: 0.4531 - val_loss: 0.6993\n",
      "Epoch 13/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.5551 - loss: 0.6891 - val_accuracy: 0.5000 - val_loss: 0.7011\n",
      "Epoch 14/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 247ms/step - accuracy: 0.5610 - loss: 0.6872 - val_accuracy: 0.4688 - val_loss: 0.6924\n",
      "Epoch 15/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253ms/step - accuracy: 0.5295 - loss: 0.6863 - val_accuracy: 0.5703 - val_loss: 0.6877\n",
      "Epoch 16/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.5650 - loss: 0.6836 - val_accuracy: 0.4609 - val_loss: 0.6932\n",
      "Epoch 17/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - accuracy: 0.6063 - loss: 0.6785 - val_accuracy: 0.4766 - val_loss: 0.7123\n",
      "Epoch 18/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.5787 - loss: 0.6770 - val_accuracy: 0.4297 - val_loss: 0.7054\n",
      "Epoch 19/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - accuracy: 0.5610 - loss: 0.6802 - val_accuracy: 0.5391 - val_loss: 0.6981\n",
      "Epoch 20/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 264ms/step - accuracy: 0.5709 - loss: 0.6798 - val_accuracy: 0.5234 - val_loss: 0.6963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73bd71246fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03741746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">242,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_4 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │       \u001b[38;5;34m242,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">254,625</span> (994.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m254,625\u001b[0m (994.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">254,625</span> (994.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m254,625\u001b[0m (994.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - AUC: 0.6277 - accuracy: 0.5944 - loss: 0.6749 - val_AUC: 0.7509 - val_accuracy: 0.6013 - val_loss: 0.6449\n",
      "Epoch 2/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.7709 - accuracy: 0.6817 - loss: 0.5893 - val_AUC: 0.8025 - val_accuracy: 0.7255 - val_loss: 0.5578\n",
      "Epoch 3/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8462 - accuracy: 0.7493 - loss: 0.4968 - val_AUC: 0.8227 - val_accuracy: 0.7451 - val_loss: 0.5161\n",
      "Epoch 4/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9019 - accuracy: 0.8141 - loss: 0.4027 - val_AUC: 0.8335 - val_accuracy: 0.7582 - val_loss: 0.5091\n",
      "Epoch 5/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9260 - accuracy: 0.8535 - loss: 0.3535 - val_AUC: 0.8383 - val_accuracy: 0.7582 - val_loss: 0.5194\n",
      "Epoch 6/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9452 - accuracy: 0.8789 - loss: 0.3056 - val_AUC: 0.8530 - val_accuracy: 0.7451 - val_loss: 0.5163\n",
      "Epoch 7/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9545 - accuracy: 0.8873 - loss: 0.2772 - val_AUC: 0.8578 - val_accuracy: 0.7712 - val_loss: 0.5221\n",
      "Epoch 8/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9602 - accuracy: 0.9014 - loss: 0.2544 - val_AUC: 0.8654 - val_accuracy: 0.7712 - val_loss: 0.5258\n",
      "Epoch 9/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9619 - accuracy: 0.9014 - loss: 0.2502 - val_AUC: 0.8617 - val_accuracy: 0.7516 - val_loss: 0.5573\n",
      "Epoch 10/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9682 - accuracy: 0.8958 - loss: 0.2297 - val_AUC: 0.8577 - val_accuracy: 0.7255 - val_loss: 0.6507\n",
      "Epoch 11/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9641 - accuracy: 0.9014 - loss: 0.2405 - val_AUC: 0.8584 - val_accuracy: 0.7320 - val_loss: 0.7360\n",
      "Epoch 12/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9753 - accuracy: 0.9099 - loss: 0.2019 - val_AUC: 0.8597 - val_accuracy: 0.7124 - val_loss: 0.8658\n",
      "Epoch 13/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9756 - accuracy: 0.9042 - loss: 0.2036 - val_AUC: 0.8583 - val_accuracy: 0.7320 - val_loss: 0.7595\n",
      "Epoch 14/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9831 - accuracy: 0.9296 - loss: 0.1720 - val_AUC: 0.8617 - val_accuracy: 0.7516 - val_loss: 0.6867\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8401 - accuracy: 0.7656 - loss: 0.4882 \n",
      "Baseline 1 Test Accuracy: 0.7656, AUC: 0.8401\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape # Tambah Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.random.set_seed(42)\n",
    "# Assume nBits is the size of your fingerprint (e.g., 1024)\n",
    "# Ini harus sama dengan ukuran fingerprint yang lu bikin pake RDKit\n",
    "\n",
    "# Hyperparameters (sesuai kode lu, vocab_size dan embedding_dim nggak kepake lagi)\n",
    "# vocab_size = len(tokenizer.word_index) + 1 # Not needed\n",
    "# embedding_dim = 64 # Not needed\n",
    "units = [64, 32] # Ukuran hidden state LSTM\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_1 = Sequential()\n",
    "\n",
    "# Layer Input: Langsung terima vektor fingerprint\n",
    "# Bentuk inputnya adalah (panjang_fingerprint,)\n",
    "model_baseline_1.add(tf.keras.Input(shape=(X_train.shape[1], 1))) # Define input shape\n",
    "\n",
    "# Tambahkan layer Reshape untuk mengubah (nBits,) menjadi (1, nBits)\n",
    "# Agar cocok dengan input shape LSTM yang butuh 3D (samples, timesteps, features)\n",
    "model_baseline_1.add(Reshape((1, X_train.shape[1]))) # Ubah (features,) jadi (1, features)\n",
    "\n",
    "# Layer LSTM: Sekarang inputnya udah 3D (samples, 1, nBits)\n",
    "# Return sequences False karena kita cuma punya 1 timestep dan mau output single vector\n",
    "model_baseline_1.add(LSTM(units[0], activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model_baseline_1.add(LSTM(units[1], activation='tanh', recurrent_activation='sigmoid', return_sequences=False))\n",
    "\n",
    "# Layer Dropout\n",
    "model_baseline_1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Layer Output: Dense dengan 1 unit dan activation sigmoid (kalau task-nya binary classification)\n",
    "# Kalau task-nya regresi, ganti activation jadi 'linear' atau jangan pakai activation\n",
    "model_baseline_1.add(Dense(1, activation='sigmoid')) # Ganti kalau task-nya regresi\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_1.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy', 'AUC'])\n",
    "\n",
    "model_baseline_1.summary() # Penting buat ngecek arsitektur dan shape\n",
    "\n",
    "# Train the model\n",
    "history_baseline_1 = model_baseline_1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy, auc = model_baseline_1.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 1 Test Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e667b",
   "metadata": {},
   "source": [
    "| Model | Units | Dropout | LR | Optimizer | Batch | L2 Reg |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| Baseline 1 | (64, 32) | 0.3 | 0.001 | Adam | 8 | 0 |\n",
    "| Baseline 2 | (128, 64) | 0.4 | 0.0005 | RMSprop | 8 | 1.00E-04 |\n",
    "| Baseline 3 | (256, 128) | 0.5 | 0.001 | Nadam | 16 | 0 |\n",
    "| Baseline 4 | (128, 32) | 0.3 | 0.001 | RMSprop | 4 | 0 |\n",
    "| Baseline 5 | (64, 32, 16) | 0.4 | 0.0005 | Adam | 16 | 1.00E-04 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9632c8",
   "metadata": {},
   "source": [
    "# Baseline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b0699af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">242,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_10 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_22 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │       \u001b[38;5;34m242,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">254,625</span> (994.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m254,625\u001b[0m (994.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">254,625</span> (994.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m254,625\u001b[0m (994.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - AUC: 0.6050 - accuracy: 0.5887 - loss: 0.6776 - val_AUC: 0.7471 - val_accuracy: 0.6013 - val_loss: 0.6479\n",
      "Epoch 2/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.7628 - accuracy: 0.6789 - loss: 0.5954 - val_AUC: 0.8002 - val_accuracy: 0.7386 - val_loss: 0.5692\n",
      "Epoch 3/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.8465 - accuracy: 0.7577 - loss: 0.4931 - val_AUC: 0.8207 - val_accuracy: 0.7647 - val_loss: 0.5199\n",
      "Epoch 4/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.8960 - accuracy: 0.8113 - loss: 0.4162 - val_AUC: 0.8408 - val_accuracy: 0.7843 - val_loss: 0.5041\n",
      "Epoch 5/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9231 - accuracy: 0.8479 - loss: 0.3579 - val_AUC: 0.8485 - val_accuracy: 0.7778 - val_loss: 0.5108\n",
      "Epoch 6/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9402 - accuracy: 0.8592 - loss: 0.3147 - val_AUC: 0.8541 - val_accuracy: 0.7516 - val_loss: 0.5212\n",
      "Epoch 7/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9503 - accuracy: 0.8761 - loss: 0.2861 - val_AUC: 0.8589 - val_accuracy: 0.7843 - val_loss: 0.5170\n",
      "Epoch 8/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9566 - accuracy: 0.8930 - loss: 0.2673 - val_AUC: 0.8626 - val_accuracy: 0.7908 - val_loss: 0.5317\n",
      "Epoch 9/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9569 - accuracy: 0.8958 - loss: 0.2660 - val_AUC: 0.8629 - val_accuracy: 0.7516 - val_loss: 0.5359\n",
      "Epoch 10/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9620 - accuracy: 0.8958 - loss: 0.2502 - val_AUC: 0.8610 - val_accuracy: 0.7516 - val_loss: 0.6279\n",
      "Epoch 11/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9632 - accuracy: 0.8845 - loss: 0.2459 - val_AUC: 0.8636 - val_accuracy: 0.7255 - val_loss: 0.7309\n",
      "Epoch 12/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9725 - accuracy: 0.9070 - loss: 0.2139 - val_AUC: 0.8610 - val_accuracy: 0.7451 - val_loss: 0.7580\n",
      "Epoch 13/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9831 - accuracy: 0.9268 - loss: 0.1734 - val_AUC: 0.8667 - val_accuracy: 0.7516 - val_loss: 0.6735\n",
      "Epoch 14/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9847 - accuracy: 0.9352 - loss: 0.1611 - val_AUC: 0.8659 - val_accuracy: 0.7516 - val_loss: 0.6910\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8428 - accuracy: 0.7812 - loss: 0.4776 \n",
      "Baseline 1 Test Accuracy: 0.7812, AUC: 0.8428\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape # Tambah Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.random.set_seed(42)\n",
    "# Assume nBits is the size of your fingerprint (e.g., 1024)\n",
    "# Ini harus sama dengan ukuran fingerprint yang lu bikin pake RDKit\n",
    "\n",
    "# Hyperparameters (sesuai kode lu, vocab_size dan embedding_dim nggak kepake lagi)\n",
    "# vocab_size = len(tokenizer.word_index) + 1 # Not needed\n",
    "# embedding_dim = 64 # Not needed\n",
    "units = [64, 32] # Ukuran hidden state LSTM\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_1 = Sequential()\n",
    "\n",
    "# Layer Input: Langsung terima vektor fingerprint\n",
    "# Bentuk inputnya adalah (panjang_fingerprint,)\n",
    "model_baseline_1.add(tf.keras.Input(shape=(X_train.shape[1], 1))) # Define input shape\n",
    "\n",
    "# Tambahkan layer Reshape untuk mengubah (nBits,) menjadi (1, nBits)\n",
    "# Agar cocok dengan input shape LSTM yang butuh 3D (samples, timesteps, features)\n",
    "model_baseline_1.add(Reshape((1, X_train.shape[1]))) # Ubah (features,) jadi (1, features)\n",
    "\n",
    "# Layer LSTM: Sekarang inputnya udah 3D (samples, 1, nBits)\n",
    "# Return sequences False karena kita cuma punya 1 timestep dan mau output single vector\n",
    "model_baseline_1.add(LSTM(units[0], activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model_baseline_1.add(LSTM(units[1], activation='tanh', recurrent_activation='sigmoid', return_sequences=False))\n",
    "\n",
    "# Layer Dropout\n",
    "model_baseline_1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Layer Output: Dense dengan 1 unit dan activation sigmoid (kalau task-nya binary classification)\n",
    "# Kalau task-nya regresi, ganti activation jadi 'linear' atau jangan pakai activation\n",
    "model_baseline_1.add(Dense(1, activation='sigmoid')) # Ganti kalau task-nya regresi\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_1.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy', 'AUC'])\n",
    "\n",
    "model_baseline_1.summary() # Penting buat ngecek arsitektur dan shape\n",
    "\n",
    "# Train the model\n",
    "history_baseline_1 = model_baseline_1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_baseline_1, acc_baseline_1, auc_baseline_1 = model_baseline_1.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 1 Test Accuracy: {acc_baseline_1:.4f}, AUC: {auc_baseline_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db6685",
   "metadata": {},
   "source": [
    "# Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1c998ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">517,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_11 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_24 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │       \u001b[38;5;34m517,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_25 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">566,593</span> (2.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m566,593\u001b[0m (2.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">566,593</span> (2.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m566,593\u001b[0m (2.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - AUC: 0.5579 - accuracy: 0.5521 - loss: 0.7689 - val_AUC: 0.7642 - val_accuracy: 0.5752 - val_loss: 0.7572\n",
      "Epoch 2/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.7033 - accuracy: 0.6282 - loss: 0.7297 - val_AUC: 0.7676 - val_accuracy: 0.6471 - val_loss: 0.7133\n",
      "Epoch 3/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.7608 - accuracy: 0.6592 - loss: 0.6769 - val_AUC: 0.7832 - val_accuracy: 0.6863 - val_loss: 0.6591\n",
      "Epoch 4/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8156 - accuracy: 0.7352 - loss: 0.6134 - val_AUC: 0.8042 - val_accuracy: 0.7320 - val_loss: 0.6299\n",
      "Epoch 5/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8538 - accuracy: 0.7690 - loss: 0.5596 - val_AUC: 0.8231 - val_accuracy: 0.7451 - val_loss: 0.6063\n",
      "Epoch 6/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8809 - accuracy: 0.7887 - loss: 0.5151 - val_AUC: 0.8417 - val_accuracy: 0.7516 - val_loss: 0.5718\n",
      "Epoch 7/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8993 - accuracy: 0.8141 - loss: 0.4791 - val_AUC: 0.8480 - val_accuracy: 0.7320 - val_loss: 0.5900\n",
      "Epoch 8/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9116 - accuracy: 0.8197 - loss: 0.4524 - val_AUC: 0.8519 - val_accuracy: 0.7451 - val_loss: 0.5875\n",
      "Epoch 9/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9194 - accuracy: 0.8310 - loss: 0.4311 - val_AUC: 0.8520 - val_accuracy: 0.7451 - val_loss: 0.5891\n",
      "Epoch 10/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9294 - accuracy: 0.8310 - loss: 0.4111 - val_AUC: 0.8532 - val_accuracy: 0.7451 - val_loss: 0.6074\n",
      "Epoch 11/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9340 - accuracy: 0.8366 - loss: 0.3953 - val_AUC: 0.8492 - val_accuracy: 0.7516 - val_loss: 0.6401\n",
      "Epoch 12/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9395 - accuracy: 0.8592 - loss: 0.3815 - val_AUC: 0.8550 - val_accuracy: 0.7647 - val_loss: 0.6267\n",
      "Epoch 13/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9453 - accuracy: 0.8704 - loss: 0.3666 - val_AUC: 0.8547 - val_accuracy: 0.7582 - val_loss: 0.6533\n",
      "Epoch 14/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9464 - accuracy: 0.8648 - loss: 0.3586 - val_AUC: 0.8555 - val_accuracy: 0.7712 - val_loss: 0.6379\n",
      "Epoch 15/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9504 - accuracy: 0.8817 - loss: 0.3469 - val_AUC: 0.8636 - val_accuracy: 0.7778 - val_loss: 0.6201\n",
      "Epoch 16/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9562 - accuracy: 0.8732 - loss: 0.3311 - val_AUC: 0.8527 - val_accuracy: 0.7778 - val_loss: 0.6519\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8504 - accuracy: 0.7266 - loss: 0.5769 \n",
      "Baseline 2 Test Accuracy: 0.7266, AUC: 0.8504\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters (Model M2)\n",
    "units = [128, 64]\n",
    "dropout_rate = 0.4\n",
    "learning_rate = 0.0005\n",
    "l2_reg = 1e-4\n",
    "batch_size = 8\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_2 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_baseline_2.add(tf.keras.Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Reshape layer: (features,) → (1, features)\n",
    "model_baseline_2.add(Reshape((1, X_train.shape[1])))\n",
    "\n",
    "# LSTM Layers with L2 Regularization\n",
    "model_baseline_2.add(\n",
    "    LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_2.add(\n",
    "    LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropout\n",
    "model_baseline_2.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output Layer (binary classification)\n",
    "model_baseline_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_2.compile(\n",
    "    optimizer=RMSprop(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model_baseline_2.summary()\n",
    "\n",
    "# Train the model\n",
    "history_baseline_2 = model_baseline_2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_baseline_2, acc_baseline_2, auc_baseline_2 = model_baseline_2.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 2 Test Accuracy: {acc_baseline_2:.4f}, AUC: {auc_baseline_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24a40d",
   "metadata": {},
   "source": [
    "# Baseline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "307e634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,165,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_12 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_26 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │     \u001b[38;5;34m1,165,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_27 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,362,561</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,362,561\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,362,561</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,362,561\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - AUC: 0.6072 - accuracy: 0.5803 - loss: 0.6772 - val_AUC: 0.7507 - val_accuracy: 0.6209 - val_loss: 0.6414\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.7668 - accuracy: 0.6845 - loss: 0.5789 - val_AUC: 0.7910 - val_accuracy: 0.7255 - val_loss: 0.5558\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.8603 - accuracy: 0.7887 - loss: 0.4689 - val_AUC: 0.8295 - val_accuracy: 0.7320 - val_loss: 0.5117\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9081 - accuracy: 0.8197 - loss: 0.3882 - val_AUC: 0.8458 - val_accuracy: 0.7516 - val_loss: 0.5066\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.9277 - accuracy: 0.8479 - loss: 0.3442 - val_AUC: 0.8490 - val_accuracy: 0.7778 - val_loss: 0.5214\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - AUC: 0.9494 - accuracy: 0.8761 - loss: 0.2895 - val_AUC: 0.8542 - val_accuracy: 0.7712 - val_loss: 0.5321\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.9554 - accuracy: 0.8676 - loss: 0.2742 - val_AUC: 0.8574 - val_accuracy: 0.7582 - val_loss: 0.5397\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9620 - accuracy: 0.8930 - loss: 0.2491 - val_AUC: 0.8612 - val_accuracy: 0.7712 - val_loss: 0.5564\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - AUC: 0.9690 - accuracy: 0.9155 - loss: 0.2242 - val_AUC: 0.8563 - val_accuracy: 0.7647 - val_loss: 0.6039\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9719 - accuracy: 0.9127 - loss: 0.2130 - val_AUC: 0.8619 - val_accuracy: 0.7712 - val_loss: 0.6360\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9769 - accuracy: 0.9042 - loss: 0.1959 - val_AUC: 0.8600 - val_accuracy: 0.7582 - val_loss: 0.6436\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9808 - accuracy: 0.9183 - loss: 0.1797 - val_AUC: 0.8491 - val_accuracy: 0.7386 - val_loss: 0.7844\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - AUC: 0.9856 - accuracy: 0.9296 - loss: 0.1557 - val_AUC: 0.8509 - val_accuracy: 0.7516 - val_loss: 0.8332\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9877 - accuracy: 0.9268 - loss: 0.1456 - val_AUC: 0.8448 - val_accuracy: 0.7516 - val_loss: 0.8593\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8707 - accuracy: 0.7812 - loss: 0.4647 \n",
      "Baseline 3 Test Accuracy: 0.7812, AUC: 0.8707\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters (Model M3)\n",
    "units = [256, 128]\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "l2_reg = 0.0  # No regularization\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_3 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_baseline_3.add(tf.keras.Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Reshape: (features,) → (1, features)\n",
    "model_baseline_3.add(Reshape((1, X_train.shape[1])))\n",
    "\n",
    "# LSTM Layers (tanpa regularizer)\n",
    "model_baseline_3.add(\n",
    "    LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True\n",
    "    )\n",
    ")\n",
    "model_baseline_3.add(\n",
    "    LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropout\n",
    "model_baseline_3.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output Layer (binary classification)\n",
    "model_baseline_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_3.compile(\n",
    "    optimizer=Nadam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model_baseline_3.summary()\n",
    "\n",
    "# Train the model\n",
    "history_baseline_3 = model_baseline_3.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_baseline_3, acc_baseline_3, auc_baseline_3 = model_baseline_3.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 3 Test Accuracy: {acc_baseline_3:.4f}, AUC: {auc_baseline_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb481e0",
   "metadata": {},
   "source": [
    "# Baseline 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29cd5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">517,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_13 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_28 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │       \u001b[38;5;34m517,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_29 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m20,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,761</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m537,761\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,761</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m537,761\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - AUC: 0.5872 - accuracy: 0.5577 - loss: 0.6782 - val_AUC: 0.7595 - val_accuracy: 0.6797 - val_loss: 0.6248\n",
      "Epoch 2/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.7571 - accuracy: 0.6592 - loss: 0.5844 - val_AUC: 0.8126 - val_accuracy: 0.7255 - val_loss: 0.5458\n",
      "Epoch 3/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.8436 - accuracy: 0.7634 - loss: 0.4908 - val_AUC: 0.8314 - val_accuracy: 0.7190 - val_loss: 0.5199\n",
      "Epoch 4/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.8795 - accuracy: 0.8028 - loss: 0.4349 - val_AUC: 0.8383 - val_accuracy: 0.7255 - val_loss: 0.5271\n",
      "Epoch 5/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.8994 - accuracy: 0.8282 - loss: 0.3995 - val_AUC: 0.8457 - val_accuracy: 0.7386 - val_loss: 0.5138\n",
      "Epoch 6/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9192 - accuracy: 0.8451 - loss: 0.3620 - val_AUC: 0.8563 - val_accuracy: 0.7451 - val_loss: 0.5110\n",
      "Epoch 7/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9286 - accuracy: 0.8507 - loss: 0.3396 - val_AUC: 0.8589 - val_accuracy: 0.7516 - val_loss: 0.5231\n",
      "Epoch 8/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9379 - accuracy: 0.8704 - loss: 0.3172 - val_AUC: 0.8561 - val_accuracy: 0.7516 - val_loss: 0.5260\n",
      "Epoch 9/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9432 - accuracy: 0.8761 - loss: 0.3038 - val_AUC: 0.8568 - val_accuracy: 0.7712 - val_loss: 0.5308\n",
      "Epoch 10/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9483 - accuracy: 0.8817 - loss: 0.2892 - val_AUC: 0.8565 - val_accuracy: 0.7712 - val_loss: 0.5437\n",
      "Epoch 11/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9546 - accuracy: 0.8789 - loss: 0.2692 - val_AUC: 0.8531 - val_accuracy: 0.7712 - val_loss: 0.5940\n",
      "Epoch 12/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9580 - accuracy: 0.8761 - loss: 0.2657 - val_AUC: 0.8583 - val_accuracy: 0.7712 - val_loss: 0.5659\n",
      "Epoch 13/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9621 - accuracy: 0.9042 - loss: 0.2522 - val_AUC: 0.8578 - val_accuracy: 0.7647 - val_loss: 0.5718\n",
      "Epoch 14/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9657 - accuracy: 0.9014 - loss: 0.2386 - val_AUC: 0.8541 - val_accuracy: 0.7647 - val_loss: 0.5975\n",
      "Epoch 15/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9684 - accuracy: 0.8958 - loss: 0.2313 - val_AUC: 0.8563 - val_accuracy: 0.7712 - val_loss: 0.6102\n",
      "Epoch 16/100\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9733 - accuracy: 0.9155 - loss: 0.2130 - val_AUC: 0.8537 - val_accuracy: 0.7647 - val_loss: 0.6114\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8794 - accuracy: 0.7656 - loss: 0.4952 \n",
      "Baseline 4 Test Accuracy: 0.7656, AUC: 0.8794\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters (Model M4)\n",
    "units = [128, 32]\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "l2_reg = 0.0  # No regularization\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_4 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_baseline_4.add(tf.keras.Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Reshape: (features,) → (1, features)\n",
    "model_baseline_4.add(Reshape((1, X_train.shape[1])))\n",
    "\n",
    "# LSTM Layers (tanpa regularizer)\n",
    "model_baseline_4.add(\n",
    "    LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True\n",
    "    )\n",
    ")\n",
    "model_baseline_4.add(\n",
    "    LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropout\n",
    "model_baseline_4.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output Layer (binary classification)\n",
    "model_baseline_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_4.compile(\n",
    "    optimizer=RMSprop(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model_baseline_4.summary()\n",
    "\n",
    "# Train the model\n",
    "history_baseline_4 = model_baseline_4.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_baseline_4, acc_baseline_4, auc_baseline_4 = model_baseline_4.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 4 Test Accuracy: {acc_baseline_4:.4f}, AUC: {auc_baseline_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e250c",
   "metadata": {},
   "source": [
    "# Baseline 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d25a1350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">881</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">242,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_15 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m881\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_33 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │       \u001b[38;5;34m242,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_34 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_35 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">257,745</span> (1006.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m257,745\u001b[0m (1006.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">257,745</span> (1006.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m257,745\u001b[0m (1006.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - AUC: 0.5727 - accuracy: 0.5634 - loss: 0.7385 - val_AUC: 0.7294 - val_accuracy: 0.6078 - val_loss: 0.7297\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.7126 - accuracy: 0.6254 - loss: 0.7207 - val_AUC: 0.7485 - val_accuracy: 0.6209 - val_loss: 0.7112\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.7529 - accuracy: 0.6648 - loss: 0.6944 - val_AUC: 0.7703 - val_accuracy: 0.6536 - val_loss: 0.6797\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.7709 - accuracy: 0.6873 - loss: 0.6535 - val_AUC: 0.7840 - val_accuracy: 0.7190 - val_loss: 0.6360\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.7894 - accuracy: 0.7211 - loss: 0.6096 - val_AUC: 0.7934 - val_accuracy: 0.7320 - val_loss: 0.5995\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8251 - accuracy: 0.7493 - loss: 0.5597 - val_AUC: 0.8031 - val_accuracy: 0.7190 - val_loss: 0.5769\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8558 - accuracy: 0.7944 - loss: 0.5133 - val_AUC: 0.8110 - val_accuracy: 0.7320 - val_loss: 0.5565\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8702 - accuracy: 0.8085 - loss: 0.4873 - val_AUC: 0.8160 - val_accuracy: 0.7190 - val_loss: 0.5488\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8838 - accuracy: 0.8366 - loss: 0.4529 - val_AUC: 0.8229 - val_accuracy: 0.7516 - val_loss: 0.5435\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9080 - accuracy: 0.8563 - loss: 0.4136 - val_AUC: 0.8289 - val_accuracy: 0.7778 - val_loss: 0.5391\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9212 - accuracy: 0.8648 - loss: 0.3855 - val_AUC: 0.8361 - val_accuracy: 0.7778 - val_loss: 0.5335\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9160 - accuracy: 0.8817 - loss: 0.3716 - val_AUC: 0.8384 - val_accuracy: 0.7712 - val_loss: 0.5353\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9342 - accuracy: 0.8873 - loss: 0.3397 - val_AUC: 0.8437 - val_accuracy: 0.7647 - val_loss: 0.5471\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9382 - accuracy: 0.8873 - loss: 0.3324 - val_AUC: 0.8398 - val_accuracy: 0.7712 - val_loss: 0.5588\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9360 - accuracy: 0.8986 - loss: 0.3240 - val_AUC: 0.8501 - val_accuracy: 0.7451 - val_loss: 0.5763\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9337 - accuracy: 0.8761 - loss: 0.3397 - val_AUC: 0.8406 - val_accuracy: 0.7451 - val_loss: 0.5840\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9320 - accuracy: 0.8901 - loss: 0.3375 - val_AUC: 0.8313 - val_accuracy: 0.7124 - val_loss: 0.6155\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9305 - accuracy: 0.8873 - loss: 0.3486 - val_AUC: 0.8468 - val_accuracy: 0.7320 - val_loss: 0.5759\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9448 - accuracy: 0.8789 - loss: 0.3217 - val_AUC: 0.8548 - val_accuracy: 0.7843 - val_loss: 0.5390\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9606 - accuracy: 0.8986 - loss: 0.2855 - val_AUC: 0.8648 - val_accuracy: 0.7974 - val_loss: 0.5242\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9677 - accuracy: 0.9127 - loss: 0.2540 - val_AUC: 0.8640 - val_accuracy: 0.7908 - val_loss: 0.5361\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9766 - accuracy: 0.9183 - loss: 0.2302 - val_AUC: 0.8630 - val_accuracy: 0.7843 - val_loss: 0.5707\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9817 - accuracy: 0.9324 - loss: 0.2100 - val_AUC: 0.8617 - val_accuracy: 0.7778 - val_loss: 0.5924\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9739 - accuracy: 0.9380 - loss: 0.2241 - val_AUC: 0.8604 - val_accuracy: 0.7974 - val_loss: 0.6001\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9801 - accuracy: 0.9268 - loss: 0.2081 - val_AUC: 0.8544 - val_accuracy: 0.7843 - val_loss: 0.6249\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9767 - accuracy: 0.9408 - loss: 0.2160 - val_AUC: 0.8512 - val_accuracy: 0.7843 - val_loss: 0.6369\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9790 - accuracy: 0.9324 - loss: 0.2091 - val_AUC: 0.8428 - val_accuracy: 0.7843 - val_loss: 0.6707\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9757 - accuracy: 0.9155 - loss: 0.2202 - val_AUC: 0.8431 - val_accuracy: 0.7582 - val_loss: 0.6758\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9771 - accuracy: 0.9127 - loss: 0.2169 - val_AUC: 0.8442 - val_accuracy: 0.7582 - val_loss: 0.6900\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9778 - accuracy: 0.9155 - loss: 0.2215 - val_AUC: 0.8429 - val_accuracy: 0.7712 - val_loss: 0.6743\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8806 - accuracy: 0.8125 - loss: 0.5373 \n",
      "Baseline 5 Test Accuracy: 0.8125, AUC: 0.8806\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters (Model M5)\n",
    "units = [64, 32, 16]\n",
    "dropout_rate = 0.4\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "l2_reg = 1e-4  # Regularization\n",
    "\n",
    "# Build the model for Fingerprint input\n",
    "model_baseline_5 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_baseline_5.add(tf.keras.Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Reshape: (features,) → (1, features)\n",
    "model_baseline_5.add(Reshape((1, X_train.shape[1])))\n",
    "\n",
    "# LSTM Layers with L2 regularization\n",
    "model_baseline_5.add(\n",
    "    LSTM(\n",
    "        units[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(\n",
    "    LSTM(\n",
    "        units[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "model_baseline_5.add(\n",
    "    LSTM(\n",
    "        units[2],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=l2(l2_reg)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropout\n",
    "model_baseline_5.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output Layer (binary classification)\n",
    "model_baseline_5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_baseline_5.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model_baseline_5.summary()\n",
    "\n",
    "# Train the model\n",
    "history_baseline_5 = model_baseline_5.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_baseline_5, acc_baseline_5, auc_baseline_5 = model_baseline_5.evaluate(X_test, y_test)\n",
    "print(f\"Baseline 5 Test Accuracy: {acc_baseline_5:.4f}, AUC: {auc_baseline_5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4a345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4608b59",
   "metadata": {},
   "source": [
    "# Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b8eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Evaluation Summary =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hidden Layers (Units)</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>L2 Reg</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline 1</td>\n",
       "      <td>(128, 64)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>Adam</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.477632</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.842770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline 2</td>\n",
       "      <td>(128, 64)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.850368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline 3</td>\n",
       "      <td>(256, 128)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.464748</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.870711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baseline 4</td>\n",
       "      <td>(128, 32)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.495220</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.879412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baseline 5</td>\n",
       "      <td>(64, 32, 16)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>Adam</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.537321</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.880637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model Hidden Layers (Units)  Dropout  Learning Rate Optimizer  \\\n",
       "0  Baseline 1             (128, 64)      0.4         0.0005      Adam   \n",
       "1  Baseline 2             (128, 64)      0.4         0.0005   RMSprop   \n",
       "2  Baseline 3            (256, 128)      0.5         0.0010     Nadam   \n",
       "3  Baseline 4             (128, 32)      0.3         0.0010   RMSprop   \n",
       "4  Baseline 5          (64, 32, 16)      0.4         0.0005      Adam   \n",
       "\n",
       "   Batch Size  L2 Reg  Test Loss  Test Accuracy  Test AUC  \n",
       "0           8  0.0000   0.477632       0.781250  0.842770  \n",
       "1           8  0.0001   0.576852       0.726562  0.850368  \n",
       "2          16  0.0000   0.464748       0.781250  0.870711  \n",
       "3           4  0.0000   0.495220       0.765625  0.879412  \n",
       "4          16  0.0001   0.537321       0.812500  0.880637  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model Hidden Layers (Units)  Dropout  Learning Rate Optimizer  Batch Size  L2 Reg  Test Loss  Test Accuracy  Test AUC\n",
      "Baseline 1             (128, 64)      0.4         0.0005      Adam           8  0.0000   0.477632       0.781250  0.842770\n",
      "Baseline 2             (128, 64)      0.4         0.0005   RMSprop           8  0.0001   0.576852       0.726562  0.850368\n",
      "Baseline 3            (256, 128)      0.5         0.0010     Nadam          16  0.0000   0.464748       0.781250  0.870711\n",
      "Baseline 4             (128, 32)      0.3         0.0010   RMSprop           4  0.0000   0.495220       0.765625  0.879412\n",
      "Baseline 5          (64, 32, 16)      0.4         0.0005      Adam          16  0.0001   0.537321       0.812500  0.880637\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simpan hasil evaluasi dari masing-masing model (isi sesuai hasil evaluate)\n",
    "results = {\n",
    "    \"Model\": [\"Baseline 1\", \"Baseline 2\", \"Baseline 3\", \"Baseline 4\", \"Baseline 5\"],\n",
    "    \"Hidden Layers (Units)\": [\"(128, 64)\", \"(128, 64)\", \"(256, 128)\", \"(128, 32)\", \"(64, 32, 16)\"],\n",
    "    \"Dropout\": [0.4, 0.4, 0.5, 0.3, 0.4],\n",
    "    \"Learning Rate\": [0.0005, 0.0005, 0.001, 0.001, 0.0005],\n",
    "    \"Optimizer\": [\"Adam\", \"RMSprop\", \"Nadam\", \"RMSprop\", \"Adam\"],\n",
    "    \"Batch Size\": [8, 8, 16, 4, 16],\n",
    "    \"L2 Reg\": [0, 1e-4, 0, 0, 1e-4],\n",
    "    \n",
    "    # Ganti nilai di bawah sesuai hasil evaluate (loss, acc, auc)\n",
    "    \"Test Loss\": [\n",
    "        loss_baseline_1,\n",
    "        loss_baseline_2,\n",
    "        loss_baseline_3,\n",
    "        loss_baseline_4,\n",
    "        loss_baseline_5\n",
    "    ],\n",
    "    \"Test Accuracy\": [\n",
    "        acc_baseline_1,\n",
    "        acc_baseline_2,\n",
    "        acc_baseline_3,\n",
    "        acc_baseline_4,\n",
    "        acc_baseline_5\n",
    "    ],\n",
    "    \"Test AUC\": [\n",
    "        auc_baseline_1,\n",
    "        auc_baseline_2,\n",
    "        auc_baseline_3,\n",
    "        auc_baseline_4,\n",
    "        auc_baseline_5\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Convert ke DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Tampilkan tabel hasil evaluasi\n",
    "print(\"\\n===== Model Evaluation Summary =====\")\n",
    "display(df_results)\n",
    "\n",
    "# Atau kalau lo mau tampil rapi di console\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d0063",
   "metadata": {},
   "source": [
    "Top 3 yg bakal dipake di laporan kayanya Baseline 1, 3, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9d5c6",
   "metadata": {},
   "source": [
    "# MBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb627d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MBO search: max_evals=120, pop_size=10. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 14:52:33.366356: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval 1/120] units=(138, 41) opt=rmsprop lr=4.13e-04 bs=8 l2=0e+00 -> val_auc=0.9172 (took 5.6s)\n",
      "[Eval 2/120] units=(188, 25) opt=adam lr=3.73e-04 bs=32 l2=3e-04 -> val_auc=0.8976 (took 5.0s)\n",
      "[Eval 3/120] units=(254, 16) opt=adam lr=5.78e-04 bs=4 l2=4e-03 -> val_auc=0.8933 (took 14.7s)\n",
      "[Eval 4/120] units=(63, 78) opt=rmsprop lr=3.45e-03 bs=16 l2=0e+00 -> val_auc=0.8976 (took 4.1s)\n",
      "[Eval 5/120] units=(234, 24) opt=adam lr=3.26e-03 bs=32 l2=3e-04 -> val_auc=0.9377 (took 5.5s)\n",
      "[Eval 6/120] units=(178, 84) opt=rmsprop lr=1.86e-03 bs=8 l2=1e-04 -> val_auc=0.9463 (took 7.2s)\n",
      "[Eval 7/120] units=(222, 26) opt=rmsprop lr=3.56e-03 bs=32 l2=2e-03 -> val_auc=0.9020 (took 5.0s)\n",
      "[Eval 8/120] units=(97, 25) opt=rmsprop lr=1.38e-03 bs=4 l2=1e-07 -> val_auc=0.9185 (took 7.3s)\n",
      "[Eval 9/120] units=(49, 77) opt=nadam lr=5.49e-03 bs=4 l2=1e-05 -> val_auc=0.9331 (took 8.3s)\n",
      "[Eval 10/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9558 (took 13.0s)\n",
      "[Eval 11/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=3e-04 -> val_auc=0.9293 (took 4.4s)\n",
      "[Eval 12/120] units=(204, 78) opt=rmsprop lr=3.73e-04 bs=4 l2=0e+00 -> val_auc=0.9318 (took 10.1s)\n",
      "[Eval 13/120] units=(234, 78) opt=nadam lr=4.13e-04 bs=32 l2=3e-04 -> val_auc=0.9063 (took 6.5s)\n",
      "[Eval 14/120] units=(222, 25) opt=adam lr=3.45e-03 bs=4 l2=0e+00 -> val_auc=0.9440 (took 12.2s)\n",
      "[Eval 15/120] units=(138, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9322 (took 4.6s)\n",
      "[Eval 16/120] units=(32, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8522 (took 4.2s)\n",
      "[Eval 17/120] units=(32, 29) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8559 (took 4.4s)\n",
      "[Eval 18/120] units=(54, 25) opt=rmsprop lr=1.00e-02 bs=32 l2=3e-04 -> val_auc=0.8062 (took 4.0s)\n",
      "[Eval 19/120] units=(35, 25) opt=rmsprop lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 4.2s)\n",
      "[Eval 20/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=3e-04 -> val_auc=0.9209 (took 4.3s)\n",
      "[Eval 21/120] units=(222, 25) opt=adam lr=3.45e-03 bs=4 l2=0e+00 -> val_auc=0.9499 (took 12.0s)\n",
      "[Eval 22/120] units=(138, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9101 (took 4.9s)\n",
      "[Eval 23/120] units=(204, 78) opt=rmsprop lr=3.73e-04 bs=4 l2=0e+00 -> val_auc=0.9161 (took 10.5s)\n",
      "[Eval 24/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=3e-04 -> val_auc=0.9239 (took 4.5s)\n",
      "[Eval 25/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=3e-04 -> val_auc=0.9280 (took 4.7s)\n",
      "[Eval 26/120] units=(234, 78) opt=nadam lr=4.13e-04 bs=32 l2=3e-04 -> val_auc=0.9117 (took 6.6s)\n",
      "[Eval 27/120] units=(32, 29) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8225 (took 4.6s)\n",
      "[Eval 28/120] units=(32, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8397 (took 4.4s)\n",
      "[Eval 29/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9479 (took 13.3s)\n",
      "[Eval 30/120] units=(178, 84) opt=rmsprop lr=1.86e-03 bs=8 l2=1e-04 -> val_auc=0.9207 (took 7.2s)\n",
      "[Eval 31/120] units=(138, 25) opt=rmsprop lr=3.45e-03 bs=32 l2=3e-04 -> val_auc=0.9249 (took 5.1s)\n",
      "[Eval 32/120] units=(138, 75) opt=nadam lr=3.26e-03 bs=4 l2=0e+00 -> val_auc=0.9373 (took 10.4s)\n",
      "[Eval 33/120] units=(138, 25) opt=rmsprop lr=4.13e-04 bs=32 l2=3e-04 -> val_auc=0.8521 (took 5.0s)\n",
      "[Eval 34/120] units=(32, 25) opt=nadam lr=4.13e-04 bs=4 l2=3e-04 -> val_auc=0.9309 (took 8.5s)\n",
      "[Eval 35/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8254 (took 4.7s)\n",
      "[Eval 36/120] units=(222, 25) opt=adam lr=3.45e-03 bs=32 l2=4e-06 -> val_auc=0.9494 (took 5.9s)\n",
      "[Eval 37/120] units=(222, 25) opt=adam lr=3.45e-03 bs=4 l2=1e-02 -> val_auc=0.5826 (took 14.3s)\n",
      "[Eval 38/120] units=(78, 77) opt=nadam lr=3.45e-03 bs=4 l2=0e+00 -> val_auc=0.9563 (took 9.1s)\n",
      "[Eval 39/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9604 (took 5.4s)\n",
      "[Eval 40/120] units=(222, 77) opt=adam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5000 (took 14.4s)\n",
      "[Eval 41/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9286 (took 5.4s)\n",
      "[Eval 42/120] units=(78, 77) opt=nadam lr=3.45e-03 bs=4 l2=0e+00 -> val_auc=0.9403 (took 9.2s)\n",
      "[Eval 43/120] units=(222, 25) opt=adam lr=3.45e-03 bs=32 l2=4e-06 -> val_auc=0.9505 (took 6.1s)\n",
      "[Eval 44/120] units=(138, 75) opt=nadam lr=3.26e-03 bs=4 l2=0e+00 -> val_auc=0.9552 (took 10.6s)\n",
      "[Eval 45/120] units=(32, 25) opt=nadam lr=4.13e-04 bs=4 l2=3e-04 -> val_auc=0.9259 (took 8.5s)\n",
      "[Eval 46/120] units=(138, 25) opt=rmsprop lr=3.45e-03 bs=32 l2=3e-04 -> val_auc=0.9211 (took 5.3s)\n",
      "[Eval 47/120] units=(138, 25) opt=rmsprop lr=4.13e-04 bs=32 l2=3e-04 -> val_auc=0.8564 (took 5.4s)\n",
      "[Eval 48/120] units=(63, 25) opt=rmsprop lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8474 (took 5.0s)\n",
      "[Eval 49/120] units=(222, 25) opt=adam lr=3.45e-03 bs=4 l2=0e+00 -> val_auc=0.9349 (took 12.5s)\n",
      "[Eval 50/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9518 (took 13.8s)\n",
      "[Eval 51/120] units=(78, 75) opt=rmsprop lr=4.13e-04 bs=32 l2=0e+00 -> val_auc=0.8482 (took 4.9s)\n",
      "[Eval 52/120] units=(32, 75) opt=adam lr=4.13e-04 bs=32 l2=4e-06 -> val_auc=0.8326 (took 5.5s)\n",
      "[Eval 53/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=4 l2=3e-05 -> val_auc=0.9479 (took 9.0s)\n",
      "[Eval 54/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=32 l2=3e-05 -> val_auc=0.9571 (took 5.6s)\n",
      "[Eval 55/120] units=(138, 25) opt=nadam lr=3.26e-03 bs=4 l2=0e+00 -> val_auc=0.9510 (took 10.6s)\n",
      "[Eval 56/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8057 (took 5.6s)\n",
      "[Eval 57/120] units=(32, 21) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8251 (took 5.6s)\n",
      "[Eval 58/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9632 (took 5.5s)\n",
      "[Eval 59/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8182 (took 5.4s)\n",
      "[Eval 60/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.7184 (took 5.3s)\n",
      "[Eval 61/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9566 (took 5.6s)\n",
      "[Eval 62/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=32 l2=3e-05 -> val_auc=0.9378 (took 5.8s)\n",
      "[Eval 63/120] units=(138, 25) opt=nadam lr=3.26e-03 bs=4 l2=0e+00 -> val_auc=0.9456 (took 10.9s)\n",
      "[Eval 64/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=4 l2=3e-05 -> val_auc=0.9229 (took 9.2s)\n",
      "[Eval 65/120] units=(78, 75) opt=rmsprop lr=4.13e-04 bs=32 l2=0e+00 -> val_auc=0.8491 (took 5.2s)\n",
      "[Eval 66/120] units=(32, 75) opt=adam lr=4.13e-04 bs=32 l2=4e-06 -> val_auc=0.8285 (took 5.8s)\n",
      "[Eval 67/120] units=(32, 21) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.7655 (took 5.5s)\n",
      "[Eval 68/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8817 (took 5.8s)\n",
      "[Eval 69/120] units=(138, 75) opt=nadam lr=3.26e-03 bs=4 l2=0e+00 -> val_auc=0.9431 (took 11.2s)\n",
      "[Eval 70/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9526 (took 14.3s)\n",
      "[Eval 71/120] units=(204, 75) opt=adam lr=1.00e-02 bs=4 l2=1e-02 -> val_auc=0.5000 (took 13.6s)\n",
      "[Eval 72/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=32 l2=4e-06 -> val_auc=0.9427 (took 5.5s)\n",
      "[Eval 73/120] units=(32, 75) opt=nadam lr=4.13e-04 bs=32 l2=0e+00 -> val_auc=0.8656 (took 5.4s)\n",
      "[Eval 74/120] units=(32, 75) opt=adam lr=7.45e-04 bs=4 l2=4e-06 -> val_auc=0.9323 (took 7.9s)\n",
      "[Eval 75/120] units=(32, 75) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9623 (took 5.3s)\n",
      "[Eval 76/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8897 (took 5.6s)\n",
      "[Eval 77/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8226 (took 5.1s)\n",
      "[Eval 78/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9421 (took 5.3s)\n",
      "[Eval 79/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.7749 (took 5.2s)\n",
      "[Eval 80/120] units=(32, 24) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9561 (took 5.3s)\n",
      "[Eval 81/120] units=(32, 75) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9359 (took 5.3s)\n",
      "[Eval 82/120] units=(32, 24) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9274 (took 5.3s)\n",
      "[Eval 83/120] units=(32, 25) opt=nadam lr=3.26e-03 bs=32 l2=4e-06 -> val_auc=0.9501 (took 5.5s)\n",
      "[Eval 84/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9334 (took 5.2s)\n",
      "[Eval 85/120] units=(32, 75) opt=adam lr=7.45e-04 bs=4 l2=4e-06 -> val_auc=0.9439 (took 7.8s)\n",
      "[Eval 86/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.9022 (took 5.7s)\n",
      "[Eval 87/120] units=(32, 75) opt=nadam lr=4.13e-04 bs=32 l2=0e+00 -> val_auc=0.8676 (took 5.8s)\n",
      "[Eval 88/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.9138 (took 5.5s)\n",
      "[Eval 89/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9538 (took 5.1s)\n",
      "[Eval 90/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9554 (took 12.6s)\n",
      "[Eval 91/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9474 (took 5.2s)\n",
      "[Eval 92/120] units=(32, 25) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8377 (took 5.4s)\n",
      "[Eval 93/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8898 (took 5.3s)\n",
      "[Eval 94/120] units=(32, 30) opt=adam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9332 (took 5.0s)\n",
      "[Eval 95/120] units=(32, 30) opt=nadam lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8988 (took 5.4s)\n",
      "[Eval 96/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9456 (took 5.3s)\n",
      "[Eval 97/120] units=(32, 75) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9379 (took 5.2s)\n",
      "[Eval 98/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9575 (took 5.4s)\n",
      "[Eval 99/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9245 (took 5.4s)\n",
      "[Eval 100/120] units=(32, 38) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9409 (took 5.6s)\n",
      "[Eval 101/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9636 (took 5.8s)\n",
      "[Eval 102/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9577 (took 5.7s)\n",
      "[Eval 103/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9471 (took 6.1s)\n",
      "[Eval 104/120] units=(32, 38) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9497 (took 5.8s)\n",
      "[Eval 105/120] units=(32, 75) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9479 (took 6.0s)\n",
      "[Eval 106/120] units=(32, 30) opt=adam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9214 (took 5.9s)\n",
      "[Eval 107/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9269 (took 5.9s)\n",
      "[Eval 108/120] units=(32, 30) opt=nadam lr=3.26e-03 bs=32 l2=1e-02 -> val_auc=0.8856 (took 6.1s)\n",
      "[Eval 109/120] units=(204, 75) opt=adam lr=7.45e-04 bs=4 l2=3e-05 -> val_auc=0.9455 (took 14.4s)\n",
      "[Eval 110/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9493 (took 6.1s)\n",
      "[Eval 111/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=3e-05 -> val_auc=0.9613 (took 6.3s)\n",
      "[Eval 112/120] units=(32, 75) opt=nadam lr=7.45e-04 bs=32 l2=0e+00 -> val_auc=0.8972 (took 6.3s)\n",
      "[Eval 113/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=4 l2=0e+00 -> val_auc=0.8942 (took 9.0s)\n",
      "[Eval 114/120] units=(32, 75) opt=nadam lr=3.26e-03 bs=32 l2=0e+00 -> val_auc=0.9554 (took 5.6s)\n",
      "[Eval 115/120] units=(32, 30) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9089 (took 5.5s)\n",
      "[Eval 116/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8782 (took 5.8s)\n",
      "[Eval 117/120] units=(75, 75) opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8410 (took 5.8s)\n",
      "[Eval 118/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9421 (took 5.6s)\n",
      "[Eval 119/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9461 (took 5.5s)\n",
      "[Eval 120/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.9290 (took 5.4s)\n",
      "\n",
      "=== MBO result ===\n",
      "best params: {'units': (32, 75), 'dropout': 0.6, 'lr': 0.01, 'optimizer': 'nadam', 'batch_size': 32, 'l2': 0.0}\n",
      "best validation AUC (approx): 0.9636170268058777\n",
      "\n",
      "Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - AUC: 0.5851 - loss: 0.6820 - val_AUC: 0.7222 - val_loss: 0.6461\n",
      "Epoch 2/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.7680 - loss: 0.5862 - val_AUC: 0.7656 - val_loss: 0.5694\n",
      "Epoch 3/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8604 - loss: 0.4636 - val_AUC: 0.7974 - val_loss: 0.6047\n",
      "Epoch 4/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8877 - loss: 0.4153 - val_AUC: 0.8188 - val_loss: 0.6208\n",
      "Epoch 5/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9208 - loss: 0.3570 - val_AUC: 0.8174 - val_loss: 0.5987\n",
      "[Eval final/120] units=(32, 75) opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.8698 (took 5.3s)\n",
      "Final short validation AUC: 0.8698170781135559\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - AUC: 0.5777 - accuracy: 0.5592 - loss: 0.6845 - val_AUC: 0.7270 - val_accuracy: 0.5714 - val_loss: 0.6594\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.7632 - accuracy: 0.6821 - loss: 0.5816 - val_AUC: 0.7564 - val_accuracy: 0.6753 - val_loss: 0.5856\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8623 - accuracy: 0.7796 - loss: 0.4626 - val_AUC: 0.8174 - val_accuracy: 0.7273 - val_loss: 0.5714\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9077 - accuracy: 0.8306 - loss: 0.3794 - val_AUC: 0.8357 - val_accuracy: 0.7532 - val_loss: 0.5810\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9232 - accuracy: 0.8399 - loss: 0.3502 - val_AUC: 0.8343 - val_accuracy: 0.7273 - val_loss: 0.5429\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9448 - accuracy: 0.8561 - loss: 0.2931 - val_AUC: 0.8411 - val_accuracy: 0.7532 - val_loss: 0.5875\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9480 - accuracy: 0.8631 - loss: 0.2848 - val_AUC: 0.8144 - val_accuracy: 0.7273 - val_loss: 0.6426\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9598 - accuracy: 0.8724 - loss: 0.2557 - val_AUC: 0.8181 - val_accuracy: 0.7403 - val_loss: 0.7279\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9647 - accuracy: 0.8747 - loss: 0.2386 - val_AUC: 0.8310 - val_accuracy: 0.7273 - val_loss: 0.6761\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9610 - accuracy: 0.8840 - loss: 0.2533 - val_AUC: 0.8133 - val_accuracy: 0.7532 - val_loss: 0.7752\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9646 - accuracy: 0.8886 - loss: 0.2374 - val_AUC: 0.8293 - val_accuracy: 0.7792 - val_loss: 0.8149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.5130, Test Acc: 0.7812, Test AUC: 0.8665\n",
      "Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# MBO hyperparameter search for LSTM (fixed + progress logging + CSV logger)\n",
    "# Requirements: pip install NiaPy tensorflow scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NiaPy imports\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# NOTE: Ensure X_train, X_test, y_train, y_test already prepared\n",
    "# e.g. uncomment and adapt:\n",
    "# data = pd.read_csv('dataFP.csv')\n",
    "# X = data.iloc[:, 1:882].values\n",
    "# y = data['categories'].values\n",
    "# le = LabelEncoder(); y = le.fit_transform(y)\n",
    "# scaler = StandardScaler(); X = scaler.fit_transform(X)\n",
    "# X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# ---------------------------\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ---------------------------\n",
    "# Logging / output setup\n",
    "# ---------------------------\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "# init CSV with header\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"units\", \"dropout\", \"lr\", \"optimizer\", \"batch_size\", \"l2\",\n",
    "    \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# decode_solution (maps continuous vector -> hyperparams)\n",
    "# ---------------------------\n",
    "def decode_solution(x):\n",
    "    u1 = int(np.clip(round(x[0]), 32, 256))\n",
    "    u2 = int(np.clip(round(x[1]), 16, 128))\n",
    "    dropout = float(np.clip(x[2], 0.2, 0.6))\n",
    "    lr = float(10 ** np.clip(x[3], -4, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[4])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[5])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[6], -8, -2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    # if l2 very small, set to 0 to avoid unnecessary regularization\n",
    "    if l2_val < 1e-7:\n",
    "        l2_val = 0.0\n",
    "    return {\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val)\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# build_and_eval: trains quickly, returns val_auc (and logs)\n",
    "# ---------------------------\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Choose optimizer\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(X_train_local.shape[1], 1)))\n",
    "    model.add(Reshape((1, X_train_local.shape[1])))\n",
    "\n",
    "    kr = l2(l2_reg) if l2_reg > 0 else None\n",
    "\n",
    "    model.add(LSTM(units[0], activation='tanh', recurrent_activation='sigmoid',\n",
    "                   return_sequences=True, kernel_regularizer=kr))\n",
    "    model.add(LSTM(units[1], activation='tanh', recurrent_activation='sigmoid',\n",
    "                   return_sequences=False, kernel_regularizer=kr))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    # EarlyStopping to keep runs short\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local, validation_split=val_split,\n",
    "                            epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        # prefer last val_auc if available\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            # fallback evaluate on a small holdout (not ideal but safe)\n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    # clean up\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Logging to CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] units={units} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Problem for NiaPy with internal eval counting and progress printing\n",
    "# ---------------------------\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=120, epochs_small=8):\n",
    "        lower = [32, 16, 0.2, -4.0, 0.0, 0.0, -8.0]\n",
    "        upper = [256, 128, 0.6, -2.0, 2.0, 3.0, -2.0]\n",
    "        super().__init__(dimension=7, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        # print brief info about candidate (could be verbose)\n",
    "        # Evaluate (we return -val_auc because NiaPy minimizes)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc\n",
    "\n",
    "# ---------------------------\n",
    "# Run MBO search (with progress printed by build_and_eval)\n",
    "# ---------------------------\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 1.0\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings\n",
    "MAX_EVALS = 120\n",
    "POP_SIZE = 10\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== MBO result ===\")\n",
    "print(\"best params:\", best_params)\n",
    "print(\"best validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ---------------------------\n",
    "# Retrain final model with best params on full training set\n",
    "# ---------------------------\n",
    "print(\"\\nRetraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train, epochs=final_epochs, val_split=0.15, verbose=1, eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final short validation AUC:\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "def build_model_from_params(params):\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(X_train.shape[1], 1)))\n",
    "    model.add(Reshape((1, X_train.shape[1])))\n",
    "    kr = l2(l2_reg) if l2_reg > 0 else None\n",
    "    model.add(LSTM(units[0], activation='tanh', recurrent_activation='sigmoid', return_sequences=True, kernel_regularizer=kr))\n",
    "    model.add(LSTM(units[1], activation='tanh', recurrent_activation='sigmoid', return_sequences=False, kernel_regularizer=kr))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "\n",
    "best_model = build_model_from_params(final_params)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "best_model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {loss:.4f}, Test Acc: {acc:.4f}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"Saved final model to mbo_best_lstm_fp_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ee806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting MBO search: max_evals=10, pop_size=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7086c59aac00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dito-adistya/miniconda3/envs/py311/lib/python3.11/weakref.py\", line 369, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval 1/10] layers=1 act=relu opt=nadam lr=8.45e-03 bs=4 -> val_auc=0.0000 (8.1s)\n",
      "[Eval 2/10] layers=2 act=selu opt=rmsprop lr=2.74e-03 bs=16 -> val_auc=0.0000 (7.6s)\n",
      "[Eval 3/10] layers=2 act=relu opt=rmsprop lr=2.93e-03 bs=8 -> val_auc=0.0000 (7.1s)\n",
      "[Eval 4/10] layers=2 act=tanh opt=rmsprop lr=4.74e-05 bs=8 -> val_auc=0.0000 (8.5s)\n",
      "[Eval 5/10] layers=1 act=tanh opt=nadam lr=9.75e-04 bs=8 -> val_auc=0.0000 (7.1s)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LSTM Hyperparameter Optimization with Monarch Butterfly Algorithm (NiaPy)\n",
    "# ============================================\n",
    "# Requirements:\n",
    "# pip install niapy tensorflow scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Monarch Butterfly Optimization\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "\n",
    "# Fix seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# Load / prepare data\n",
    "# ===============================\n",
    "# Example (uncomment and adapt to your dataset)\n",
    "# data = pd.read_csv('dataFP.csv')\n",
    "# X = data.iloc[:, 1:882].values\n",
    "# y = data['categories'].values\n",
    "# le = LabelEncoder(); y = le.fit_transform(y)\n",
    "# scaler = StandardScaler(); X = scaler.fit_transform(X)\n",
    "# X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ===============================\n",
    "# Decode solution vector -> params\n",
    "# ===============================\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Build dynamic LSTM\n",
    "# ===============================\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    # Dynamic layer setup\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  # 3 layers\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# Train + evaluate function\n",
    "# ===============================\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        if 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Log to CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx,\n",
    "        \"n_layers\": params['n_layers'],\n",
    "        \"units\": json.dumps(params['units']),\n",
    "        \"dropout\": params['dropout'],\n",
    "        \"lr\": params['lr'],\n",
    "        \"optimizer\": params['optimizer'],\n",
    "        \"batch_size\": params['batch_size'],\n",
    "        \"l2\": params['l2'],\n",
    "        \"activation\": params['activation'],\n",
    "        \"val_auc\": val_auc,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    if eval_idx and max_evals:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={params['n_layers']} act={params['activation']} opt={params['optimizer']} \"\n",
    "              f\"lr={lr:.2e} bs={batch_size} -> val_auc={val_auc:.4f} ({elapsed:.1f}s)\")\n",
    "\n",
    "    return val_auc\n",
    "\n",
    "# ===============================\n",
    "# Define NiaPy Problem\n",
    "# ===============================\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15, verbose=0,\n",
    "                                     eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc  # minimize negative AUC\n",
    "\n",
    "# ===============================\n",
    "# Run optimization\n",
    "# ===============================\n",
    "take_frac = 0.6\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "MAX_EVALS = 10\n",
    "POP_SIZE = 10\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"🚀 Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}\")\n",
    "\n",
    "best_x, best_fit = algo.run(task)\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== 🦋 MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\n🎯 Retraining final model on full training set...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50\n",
    "\n",
    "val_auc_final = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1,\n",
    "                               eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final Validation AUC:\", val_auc_final)\n",
    "\n",
    "# Build for test evaluation\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"💾 Saved final model to mbo_best_lstm_fp_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b948225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting MBO search: max_evals=60, pop_size=10. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n",
      "[Eval 1/60] layers=2 act=selu opt=nadam lr=1.58e-05 bs=16 l2=6e-07 -> val_auc=0.6906 (took 6.5s)\n",
      "[Eval 2/60] layers=1 act=relu opt=nadam lr=1.28e-05 bs=16 l2=5e-08 -> val_auc=0.7033 (took 5.8s)\n",
      "[Eval 3/60] layers=2 act=relu opt=rmsprop lr=6.52e-04 bs=4 l2=4e-06 -> val_auc=0.9244 (took 11.3s)\n",
      "[Eval 4/60] layers=3 act=relu opt=rmsprop lr=5.26e-03 bs=16 l2=2e-08 -> val_auc=0.8901 (took 7.7s)\n",
      "[Eval 5/60] layers=3 act=relu opt=rmsprop lr=4.44e-04 bs=4 l2=1e-04 -> val_auc=0.8920 (took 13.3s)\n",
      "[Eval 6/60] layers=3 act=selu opt=rmsprop lr=1.14e-04 bs=16 l2=0e+00 -> val_auc=0.7773 (took 6.6s)\n",
      "[Eval 7/60] layers=2 act=relu opt=rmsprop lr=1.20e-04 bs=32 l2=1e-05 -> val_auc=0.7733 (took 6.1s)\n",
      "[Eval 8/60] layers=2 act=relu opt=rmsprop lr=6.28e-03 bs=16 l2=3e-03 -> val_auc=0.8905 (took 5.9s)\n",
      "[Eval 9/60] layers=2 act=relu opt=nadam lr=1.90e-04 bs=16 l2=1e-03 -> val_auc=0.7967 (took 6.6s)\n",
      "[Eval 10/60] layers=3 act=relu opt=adam lr=1.53e-04 bs=32 l2=8e-04 -> val_auc=0.5000 (took 6.9s)\n",
      "[Eval 11/60] layers=2 act=relu opt=nadam lr=1.20e-04 bs=16 l2=1e-05 -> val_auc=0.8511 (took 8.3s)\n",
      "[Eval 12/60] layers=2 act=relu opt=rmsprop lr=1.20e-04 bs=16 l2=3e-03 -> val_auc=0.7719 (took 7.1s)\n",
      "[Eval 13/60] layers=3 act=relu opt=rmsprop lr=1.58e-05 bs=16 l2=6e-07 -> val_auc=0.5000 (took 7.8s)\n",
      "[Eval 14/60] layers=1 act=relu opt=rmsprop lr=1.20e-04 bs=16 l2=6e-07 -> val_auc=0.7954 (took 5.0s)\n",
      "[Eval 15/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9368 (took 14.4s)\n",
      "[Eval 16/60] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=1e-05 -> val_auc=0.9222 (took 8.7s)\n",
      "[Eval 17/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9027 (took 8.4s)\n",
      "[Eval 18/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.9078 (took 7.4s)\n",
      "[Eval 19/60] layers=3 act=selu opt=nadam lr=1.20e-04 bs=32 l2=1e-05 -> val_auc=0.8459 (took 8.8s)\n",
      "[Eval 20/60] layers=3 act=selu opt=nadam lr=1.20e-04 bs=32 l2=1e-02 -> val_auc=0.7965 (took 8.8s)\n",
      "[Eval 21/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9214 (took 14.5s)\n",
      "[Eval 22/60] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=1e-05 -> val_auc=0.8979 (took 8.8s)\n",
      "[Eval 23/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.8671 (took 7.5s)\n",
      "[Eval 24/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9302 (took 9.0s)\n",
      "[Eval 25/60] layers=2 act=relu opt=nadam lr=1.20e-04 bs=16 l2=1e-05 -> val_auc=0.8577 (took 8.5s)\n",
      "[Eval 26/60] layers=3 act=selu opt=nadam lr=1.20e-04 bs=32 l2=1e-05 -> val_auc=0.8560 (took 9.0s)\n",
      "[Eval 27/60] layers=3 act=selu opt=nadam lr=1.20e-04 bs=32 l2=1e-02 -> val_auc=0.7624 (took 9.1s)\n",
      "[Eval 28/60] layers=1 act=relu opt=rmsprop lr=1.20e-04 bs=16 l2=6e-07 -> val_auc=0.8085 (took 5.1s)\n",
      "[Eval 29/60] layers=2 act=relu opt=rmsprop lr=6.52e-04 bs=4 l2=4e-06 -> val_auc=0.9410 (took 12.5s)\n",
      "[Eval 30/60] layers=3 act=relu opt=rmsprop lr=4.44e-04 bs=4 l2=1e-04 -> val_auc=0.8769 (took 14.2s)\n",
      "[Eval 31/60] layers=1 act=relu opt=nadam lr=1.20e-04 bs=4 l2=1e-05 -> val_auc=0.9221 (took 11.9s)\n",
      "[Eval 32/60] layers=2 act=relu opt=nadam lr=1.20e-04 bs=16 l2=1e-05 -> val_auc=0.8407 (took 9.3s)\n",
      "[Eval 33/60] layers=2 act=relu opt=nadam lr=1.20e-04 bs=16 l2=1e-05 -> val_auc=0.7926 (took 7.3s)\n",
      "[Eval 34/60] layers=3 act=relu opt=nadam lr=1.20e-04 bs=4 l2=1e-05 -> val_auc=0.9173 (took 17.4s)\n",
      "[Eval 35/60] layers=3 act=selu opt=nadam lr=6.52e-04 bs=4 l2=1e-05 -> val_auc=0.9336 (took 17.5s)\n",
      "[Eval 36/60] layers=3 act=selu opt=nadam lr=4.44e-04 bs=32 l2=1e-05 -> val_auc=0.9384 (took 9.2s)\n",
      "[Eval 37/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9094 (took 15.0s)\n",
      "[Eval 38/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9552 (took 9.1s)\n",
      "[Eval 39/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9558 (took 9.2s)\n",
      "[Eval 40/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9308 (took 18.3s)\n",
      "[Eval 41/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9246 (took 9.9s)\n",
      "[Eval 42/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9278 (took 9.2s)\n",
      "[Eval 43/60] layers=3 act=selu opt=nadam lr=4.44e-04 bs=32 l2=1e-05 -> val_auc=0.9324 (took 9.2s)\n",
      "[Eval 44/60] layers=3 act=selu opt=nadam lr=6.52e-04 bs=4 l2=1e-05 -> val_auc=0.9419 (took 17.2s)\n",
      "[Eval 45/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9403 (took 18.2s)\n",
      "[Eval 46/60] layers=1 act=relu opt=nadam lr=1.20e-04 bs=4 l2=1e-05 -> val_auc=0.9216 (took 12.4s)\n",
      "[Eval 47/60] layers=3 act=relu opt=nadam lr=1.20e-04 bs=4 l2=1e-05 -> val_auc=0.9119 (took 17.3s)\n",
      "[Eval 48/60] layers=3 act=relu opt=nadam lr=4.44e-04 bs=4 l2=1e-05 -> val_auc=0.9060 (took 15.6s)\n",
      "[Eval 49/60] layers=2 act=relu opt=rmsprop lr=6.52e-04 bs=4 l2=4e-06 -> val_auc=0.9355 (took 12.9s)\n",
      "[Eval 50/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9336 (took 9.6s)\n",
      "[Eval 51/60] layers=3 act=relu opt=nadam lr=1.20e-04 bs=32 l2=1e-05 -> val_auc=0.8011 (took 9.4s)\n",
      "[Eval 52/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=4 l2=1e-05 -> val_auc=0.8463 (took 15.5s)\n",
      "[Eval 53/60] layers=3 act=relu opt=nadam lr=1.20e-04 bs=4 l2=1e-05 -> val_auc=0.9188 (took 18.5s)\n",
      "[Eval 54/60] layers=1 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9555 (took 7.1s)\n",
      "[Eval 55/60] layers=3 act=selu opt=nadam lr=4.44e-04 bs=32 l2=1e-05 -> val_auc=0.9411 (took 9.3s)\n",
      "[Eval 56/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.5000 (took 9.7s)\n",
      "[Eval 57/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9318 (took 10.3s)\n",
      "[Eval 58/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9131 (took 9.7s)\n",
      "[Eval 59/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.9486 (took 10.0s)\n",
      "[Eval 60/60] layers=3 act=relu opt=nadam lr=1.00e-02 bs=32 l2=1e-05 -> val_auc=0.8888 (took 9.3s)\n",
      "\n",
      "=== 🦋 MBO Result ===\n",
      "Best Params: {'n_layers': 3, 'units': (230, 62), 'dropout': 0.5208805354148626, 'lr': 0.0006519422035174858, 'optimizer': 'nadam', 'batch_size': 4, 'l2': 1.087869518552619e-05, 'activation': 'selu'}\n",
      "Best Validation AUC (approx): 0.9419460296630859\n",
      "\n",
      "🎯 Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - AUC: 0.6319 - loss: 0.6706 - val_AUC: 0.7364 - val_loss: 0.6153\n",
      "Epoch 2/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.7985 - loss: 0.5636 - val_AUC: 0.7737 - val_loss: 0.5908\n",
      "Epoch 3/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.8592 - loss: 0.4799 - val_AUC: 0.8089 - val_loss: 0.5771\n",
      "Epoch 4/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - AUC: 0.8925 - loss: 0.4221 - val_AUC: 0.8137 - val_loss: 0.6478\n",
      "Epoch 5/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - AUC: 0.9178 - loss: 0.3702 - val_AUC: 0.8259 - val_loss: 0.6621\n",
      "Epoch 6/50\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9353 - loss: 0.3280 - val_AUC: 0.8228 - val_loss: 0.7492\n",
      "Final short validation AUC (from build_and_eval): 0.9079529047012329\n",
      "Epoch 1/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - AUC: 0.6165 - accuracy: 0.5777 - loss: 0.6789 - val_AUC: 0.7493 - val_accuracy: 0.6623 - val_loss: 0.6183\n",
      "Epoch 2/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - AUC: 0.7913 - accuracy: 0.7239 - loss: 0.5621 - val_AUC: 0.7788 - val_accuracy: 0.6883 - val_loss: 0.5796\n",
      "Epoch 3/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.8519 - accuracy: 0.7471 - loss: 0.4833 - val_AUC: 0.8113 - val_accuracy: 0.6883 - val_loss: 0.5789\n",
      "Epoch 4/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.8771 - accuracy: 0.7958 - loss: 0.4488 - val_AUC: 0.8283 - val_accuracy: 0.7273 - val_loss: 0.5862\n",
      "Epoch 5/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9130 - accuracy: 0.8190 - loss: 0.3779 - val_AUC: 0.8279 - val_accuracy: 0.7273 - val_loss: 0.6742\n",
      "Epoch 6/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - AUC: 0.9278 - accuracy: 0.8422 - loss: 0.3482 - val_AUC: 0.8337 - val_accuracy: 0.7273 - val_loss: 0.7193\n",
      "Epoch 7/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - AUC: 0.9436 - accuracy: 0.8747 - loss: 0.3107 - val_AUC: 0.8391 - val_accuracy: 0.7403 - val_loss: 0.7108\n",
      "Epoch 8/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9517 - accuracy: 0.8701 - loss: 0.2865 - val_AUC: 0.8428 - val_accuracy: 0.7532 - val_loss: 0.8048\n",
      "Epoch 9/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9587 - accuracy: 0.8863 - loss: 0.2658 - val_AUC: 0.8296 - val_accuracy: 0.7143 - val_loss: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Results: Loss=0.5674, Acc=0.7422, AUC=0.8539\n",
      "💾 Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LSTM Hyperparameter Optimization with Monarch Butterfly Algorithm (NiaPy) - Enhanced\n",
    "# ============================================\n",
    "# Requirements:\n",
    "# pip install niapy tensorflow scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Monarch Butterfly Optimization\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "\n",
    "# Fix seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# Load / prepare data\n",
    "# ===============================\n",
    "# Example (uncomment and adapt to your dataset)\n",
    "# data = pd.read_csv('dataFP.csv')\n",
    "# X = data.iloc[:, 1:882].values\n",
    "# y = data['categories'].values\n",
    "# le = LabelEncoder(); y = le.fit_transform(y)\n",
    "# scaler = StandardScaler(); X = scaler.fit_transform(X)\n",
    "# X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "# init CSV with header - Added 'activation' and 'n_layers'\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ===============================\n",
    "# Decode solution vector -> params\n",
    "# ===============================\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Build dynamic LSTM\n",
    "# ===============================\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    # Dynamic layer setup\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  # 3 layers\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# Train + evaluate function (Enhanced with fallback and safe logging)\n",
    "# ===============================\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    # Extract parameters\n",
    "    n_layers = params['n_layers']\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "    activation = params['activation']\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        # Prefer last val_auc if available\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            # Fallback: evaluate on training data (not ideal for validation, but prevents 0.0)\n",
    "            # Or just note that val_auc was missing\n",
    "            note += \"No val_auc in history. \"\n",
    "            # Let's evaluate on the training data as a very last resort (though not a true validation score)\n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "            note += f\"Used training AUC instead: {val_auc:.4f}. \"\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Log to CSV - Ensure eval_idx is safe for CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"activation\": activation,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # Print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={n_layers} act={activation} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "# ===============================\n",
    "# Define NiaPy Problem\n",
    "# ===============================\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc  # minimize negative AUC\n",
    "\n",
    "# ===============================\n",
    "# Run optimization\n",
    "# ===============================\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 0.8 # Increased from 0.6, adjust as needed\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings - Increased from 10 for better search\n",
    "MAX_EVALS = 60 # Adjust as needed, e.g., 60, 120\n",
    "POP_SIZE = 10\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"🚀 Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== 🦋 MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\n🎯 Retraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "\n",
    "# IMPORTANT: Call without eval_idx=\"final\" to avoid logging issues and simplify final train\n",
    "# We still want verbose=1 to see progress during final training\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1)\n",
    "                               # Removed: eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final short validation AUC (from build_and_eval):\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"💾 Saved final model to mbo_best_lstm_fp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb92b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting MBO search: max_evals=60, pop_size=15. Logs -> mbo_logs/mbo_eval_log.csv\n",
      "\n",
      "[Eval 1/60] layers=1 act=tanh opt=adam lr=3.35e-05 bs=16 l2=5e-04 -> val_auc=0.7423 (took 6.5s)\n",
      "[Eval 2/60] layers=2 act=relu opt=nadam lr=6.50e-05 bs=8 l2=3e-05 -> val_auc=0.7986 (took 8.9s)\n",
      "[Eval 3/60] layers=2 act=selu opt=adam lr=6.19e-05 bs=8 l2=5e-04 -> val_auc=0.8452 (took 9.6s)\n",
      "[Eval 4/60] layers=3 act=relu opt=nadam lr=4.27e-03 bs=16 l2=5e-03 -> val_auc=0.5000 (took 9.2s)\n",
      "[Eval 5/60] layers=2 act=relu opt=nadam lr=4.72e-05 bs=8 l2=0e+00 -> val_auc=0.7988 (took 9.1s)\n",
      "[Eval 6/60] layers=2 act=relu opt=nadam lr=4.37e-05 bs=8 l2=0e+00 -> val_auc=0.8219 (took 13.2s)\n",
      "[Eval 7/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=9e-06 -> val_auc=0.9186 (took 9.8s)\n",
      "[Eval 8/60] layers=2 act=selu opt=rmsprop lr=1.11e-05 bs=32 l2=3e-04 -> val_auc=0.6999 (took 7.4s)\n",
      "[Eval 9/60] layers=3 act=tanh opt=rmsprop lr=4.32e-05 bs=8 l2=2e-05 -> val_auc=0.6687 (took 9.4s)\n",
      "[Eval 10/60] layers=3 act=relu opt=nadam lr=4.53e-05 bs=16 l2=0e+00 -> val_auc=0.7178 (took 11.8s)\n",
      "[Eval 11/60] layers=2 act=tanh opt=adam lr=5.11e-03 bs=16 l2=0e+00 -> val_auc=0.9528 (took 9.6s)\n",
      "[Eval 12/60] layers=2 act=tanh opt=rmsprop lr=8.79e-04 bs=32 l2=0e+00 -> val_auc=0.9045 (took 8.9s)\n",
      "[Eval 13/60] layers=1 act=selu opt=nadam lr=8.64e-03 bs=8 l2=0e+00 -> val_auc=0.9089 (took 17.4s)\n",
      "[Eval 14/60] layers=2 act=relu opt=rmsprop lr=5.90e-03 bs=32 l2=2e-03 -> val_auc=0.8811 (took 13.3s)\n",
      "[Eval 15/60] layers=2 act=relu opt=rmsprop lr=4.95e-04 bs=4 l2=0e+00 -> val_auc=0.9259 (took 16.4s)\n",
      "[Eval 16/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9405 (took 11.6s)\n",
      "[Eval 17/60] layers=2 act=tanh opt=nadam lr=4.72e-05 bs=4 l2=0e+00 -> val_auc=0.8518 (took 16.7s)\n",
      "[Eval 18/60] layers=2 act=relu opt=rmsprop lr=6.50e-05 bs=8 l2=0e+00 -> val_auc=0.7674 (took 10.0s)\n",
      "[Eval 19/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=8 l2=0e+00 -> val_auc=0.9336 (took 13.5s)\n",
      "[Eval 20/60] layers=1 act=tanh opt=rmsprop lr=3.35e-05 bs=16 l2=0e+00 -> val_auc=0.7863 (took 7.1s)\n",
      "[Eval 21/60] layers=1 act=tanh opt=rmsprop lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.9174 (took 6.7s)\n",
      "[Eval 22/60] layers=1 act=relu opt=nadam lr=5.11e-03 bs=16 l2=0e+00 -> val_auc=0.9264 (took 7.5s)\n",
      "[Eval 23/60] layers=3 act=relu opt=nadam lr=5.07e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 12.2s)\n",
      "[Eval 24/60] layers=3 act=selu opt=adam lr=1.00e-02 bs=16 l2=0e+00 -> val_auc=0.9269 (took 10.8s)\n",
      "[Eval 25/60] layers=2 act=relu opt=nadam lr=1.00e-02 bs=16 l2=1e-02 -> val_auc=0.5000 (took 10.0s)\n",
      "[Eval 26/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 11.2s)\n",
      "[Eval 27/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.9476 (took 10.5s)\n",
      "[Eval 28/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9218 (took 10.2s)\n",
      "[Eval 29/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9586 (took 10.8s)\n",
      "[Eval 30/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9335 (took 10.0s)\n",
      "[Eval 31/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9479 (took 9.1s)\n",
      "[Eval 32/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.9575 (took 9.0s)\n",
      "[Eval 33/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9377 (took 10.1s)\n",
      "[Eval 34/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=8 l2=0e+00 -> val_auc=0.9499 (took 12.1s)\n",
      "[Eval 35/60] layers=2 act=relu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9317 (took 10.5s)\n",
      "[Eval 36/60] layers=3 act=selu opt=adam lr=1.00e-02 bs=16 l2=0e+00 -> val_auc=0.9093 (took 11.7s)\n",
      "[Eval 37/60] layers=1 act=relu opt=nadam lr=5.11e-03 bs=16 l2=0e+00 -> val_auc=0.9452 (took 9.8s)\n",
      "[Eval 38/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9481 (took 12.0s)\n",
      "[Eval 39/60] layers=1 act=tanh opt=rmsprop lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.8930 (took 6.9s)\n",
      "[Eval 40/60] layers=2 act=tanh opt=nadam lr=4.72e-05 bs=4 l2=0e+00 -> val_auc=0.8424 (took 16.6s)\n",
      "[Eval 41/60] layers=1 act=tanh opt=rmsprop lr=3.35e-05 bs=16 l2=0e+00 -> val_auc=0.7574 (took 7.2s)\n",
      "[Eval 42/60] layers=2 act=relu opt=rmsprop lr=6.50e-05 bs=8 l2=0e+00 -> val_auc=0.7759 (took 8.7s)\n",
      "[Eval 43/60] layers=3 act=relu opt=nadam lr=5.07e-03 bs=16 l2=1e-02 -> val_auc=0.5000 (took 11.0s)\n",
      "[Eval 44/60] layers=2 act=tanh opt=adam lr=5.11e-03 bs=16 l2=0e+00 -> val_auc=0.9193 (took 9.0s)\n",
      "[Eval 45/60] layers=2 act=relu opt=rmsprop lr=4.95e-04 bs=4 l2=0e+00 -> val_auc=0.9167 (took 17.0s)\n",
      "[Eval 46/60] layers=1 act=relu opt=adam lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.9361 (took 7.2s)\n",
      "[Eval 47/60] layers=2 act=relu opt=rmsprop lr=5.07e-03 bs=4 l2=0e+00 -> val_auc=0.9262 (took 12.8s)\n",
      "[Eval 48/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9443 (took 9.8s)\n",
      "[Eval 49/60] layers=1 act=relu opt=adam lr=3.35e-05 bs=8 l2=0e+00 -> val_auc=0.8116 (took 8.9s)\n",
      "[Eval 50/60] layers=2 act=selu opt=nadam lr=5.11e-03 bs=16 l2=0e+00 -> val_auc=0.9506 (took 9.4s)\n",
      "[Eval 51/60] layers=1 act=tanh opt=rmsprop lr=5.07e-03 bs=32 l2=0e+00 -> val_auc=0.9430 (took 6.7s)\n",
      "[Eval 52/60] layers=1 act=selu opt=adam lr=5.11e-03 bs=4 l2=0e+00 -> val_auc=0.9208 (took 12.4s)\n",
      "[Eval 53/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=32 l2=1e-02 -> val_auc=0.9177 (took 9.3s)\n",
      "[Eval 54/60] layers=3 act=selu opt=nadam lr=1.00e-02 bs=16 l2=0e+00 -> val_auc=0.9332 (took 12.5s)\n",
      "[Eval 55/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=16 l2=1e-02 -> val_auc=0.8927 (took 10.4s)\n",
      "[Eval 56/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=32 l2=1e-02 -> val_auc=0.9263 (took 10.8s)\n",
      "[Eval 57/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=0e+00 -> val_auc=0.9314 (took 10.9s)\n",
      "[Eval 58/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=32 l2=0e+00 -> val_auc=0.8767 (took 10.2s)\n",
      "[Eval 59/60] layers=2 act=selu opt=nadam lr=5.07e-03 bs=16 l2=1e-02 -> val_auc=0.9112 (took 11.6s)\n",
      "[Eval 60/60] layers=2 act=selu opt=nadam lr=1.00e-02 bs=16 l2=0e+00 -> val_auc=0.9505 (took 10.8s)\n",
      "\n",
      "=== 🦋 MBO Result ===\n",
      "Best Params: {'n_layers': 2, 'units': (185, 180), 'dropout': 0.7, 'lr': 0.00507262288369937, 'optimizer': 'nadam', 'batch_size': 32, 'l2': 0.0, 'activation': 'selu'}\n",
      "Best Validation AUC (approx): 0.9575363993644714\n",
      "\n",
      "🎯 Retraining final model on full training set with best hyperparams...\n",
      "Epoch 1/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - AUC: 0.5531 - loss: 0.7149 - val_AUC: 0.7575 - val_loss: 0.6157\n",
      "Epoch 2/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.7583 - loss: 0.5741 - val_AUC: 0.7818 - val_loss: 0.6577\n",
      "Epoch 3/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - AUC: 0.8660 - loss: 0.4463 - val_AUC: 0.8249 - val_loss: 0.6094\n",
      "Epoch 4/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9095 - loss: 0.3739 - val_AUC: 0.8371 - val_loss: 0.6385\n",
      "Epoch 5/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.9278 - loss: 0.3368 - val_AUC: 0.8367 - val_loss: 0.6444\n",
      "Epoch 6/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9484 - loss: 0.2850 - val_AUC: 0.8438 - val_loss: 0.6388\n",
      "Final short validation AUC (from build_and_eval): 0.9111483097076416\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - AUC: 0.5465 - accuracy: 0.5499 - loss: 0.7250 - val_AUC: 0.7283 - val_accuracy: 0.5844 - val_loss: 0.6366\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.7625 - accuracy: 0.6961 - loss: 0.5763 - val_AUC: 0.7886 - val_accuracy: 0.6753 - val_loss: 0.5921\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.8618 - accuracy: 0.7703 - loss: 0.4566 - val_AUC: 0.8347 - val_accuracy: 0.7403 - val_loss: 0.5457\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.9098 - accuracy: 0.8167 - loss: 0.3730 - val_AUC: 0.8432 - val_accuracy: 0.7273 - val_loss: 0.5585\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - AUC: 0.9253 - accuracy: 0.8329 - loss: 0.3409 - val_AUC: 0.8499 - val_accuracy: 0.7532 - val_loss: 0.6520\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - AUC: 0.9465 - accuracy: 0.8561 - loss: 0.2920 - val_AUC: 0.8516 - val_accuracy: 0.7273 - val_loss: 0.7068\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9591 - accuracy: 0.8747 - loss: 0.2505 - val_AUC: 0.8449 - val_accuracy: 0.7532 - val_loss: 0.7817\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - AUC: 0.9624 - accuracy: 0.8886 - loss: 0.2513 - val_AUC: 0.8421 - val_accuracy: 0.7662 - val_loss: 0.6330\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - AUC: 0.9628 - accuracy: 0.8817 - loss: 0.2392 - val_AUC: 0.8445 - val_accuracy: 0.7403 - val_loss: 0.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Results: Loss=0.5772, Acc=0.7188, AUC=0.8491\n",
      "💾 Saved final model to mbo_best_lstm_fp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LSTM Hyperparameter Optimization with Monarch Butterfly Algorithm (NiaPy) - Enhanced\n",
    "# ============================================\n",
    "# Requirements:\n",
    "# pip install niapy tensorflow scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Monarch Butterfly Optimization\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "\n",
    "# Fix seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# Load / prepare data\n",
    "# ===============================\n",
    "# Example (uncomment and adapt to your dataset)\n",
    "# data = pd.read_csv('dataFP.csv')\n",
    "# X = data.iloc[:, 1:882].values\n",
    "# y = data['categories'].values\n",
    "# le = LabelEncoder(); y = le.fit_transform(y)\n",
    "# scaler = StandardScaler(); X = scaler.fit_transform(X)\n",
    "# X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals(), \"Define X_train, y_train, X_test, y_test before running.\"\n",
    "\n",
    "# ===============================\n",
    "# Logging setup\n",
    "# ===============================\n",
    "LOG_DIR = \"mbo_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "eval_log_csv = os.path.join(LOG_DIR, \"mbo_eval_log.csv\")\n",
    "\n",
    "# init CSV with header - Added 'activation' and 'n_layers'\n",
    "pd.DataFrame(columns=[\n",
    "    \"eval_idx\", \"n_layers\", \"units\", \"dropout\", \"lr\", \"optimizer\",\n",
    "    \"batch_size\", \"l2\", \"activation\", \"val_auc\", \"elapsed_sec\", \"note\"\n",
    "]).to_csv(eval_log_csv, index=False)\n",
    "\n",
    "# ===============================\n",
    "# Decode solution vector -> params\n",
    "# ===============================\n",
    "def decode_solution(x):\n",
    "    n_layers = int(np.clip(round(x[0]), 1, 3))\n",
    "    u1 = int(np.clip(round(x[1]), 16, 256))\n",
    "    u2 = int(np.clip(round(x[2]), 16, 256))\n",
    "    dropout = float(np.clip(x[3], 0.1, 0.7))\n",
    "    lr = float(10 ** np.clip(x[4], -5, -2))\n",
    "    opt_idx = int(np.clip(int(round(x[5])), 0, 2))\n",
    "    batch_idx = int(np.clip(int(round(x[6])), 0, 3))\n",
    "    l2_val = float(10 ** np.clip(x[7], -9, -2))\n",
    "    act_idx = int(np.clip(int(round(x[8])), 0, 2))\n",
    "\n",
    "    optim_map = {0: 'adam', 1: 'rmsprop', 2: 'nadam'}\n",
    "    batch_map = {0: 4, 1: 8, 2: 16, 3: 32}\n",
    "    act_map = {0: 'tanh', 1: 'relu', 2: 'selu'}\n",
    "\n",
    "    if l2_val < 1e-8:\n",
    "        l2_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'n_layers': n_layers,\n",
    "        'units': (u1, u2),\n",
    "        'dropout': dropout,\n",
    "        'lr': lr,\n",
    "        'optimizer': optim_map[opt_idx],\n",
    "        'batch_size': batch_map[batch_idx],\n",
    "        'l2': float(l2_val),\n",
    "        'activation': act_map[act_idx]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Build dynamic LSTM\n",
    "# ===============================\n",
    "def build_lstm_dynamic(params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(Reshape((1, input_shape[0])))\n",
    "\n",
    "    kr = l2(params['l2']) if params['l2'] > 0 else None\n",
    "    n_layers = params['n_layers']\n",
    "    u1, u2 = params['units']\n",
    "    act = params['activation']\n",
    "\n",
    "    # Dynamic layer setup\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "    else:  # 3 layers\n",
    "        model.add(LSTM(u1, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(u2, activation=act, return_sequences=True, kernel_regularizer=kr))\n",
    "        model.add(LSTM(max(16, u2 // 2), activation=act, return_sequences=False, kernel_regularizer=kr))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# Train + evaluate function (Enhanced with fallback and safe logging)\n",
    "# ===============================\n",
    "def build_and_eval(params, X_train_local, y_train_local, epochs=8, val_split=0.15, verbose=0, eval_idx=None, max_evals=None):\n",
    "    start = time.time()\n",
    "    # Extract parameters\n",
    "    n_layers = params['n_layers']\n",
    "    units = params['units']\n",
    "    dropout_rate = params['dropout']\n",
    "    lr = params['lr']\n",
    "    opt_name = params['optimizer']\n",
    "    l2_reg = params['l2']\n",
    "    batch_size = params['batch_size']\n",
    "    activation = params['activation']\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "\n",
    "    # Build model\n",
    "    model = build_lstm_dynamic(params, (X_train_local.shape[1], 1))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = None\n",
    "    val_auc = 0.0\n",
    "    note = \"\"\n",
    "    try:\n",
    "        history = model.fit(X_train_local, y_train_local,\n",
    "                            validation_split=val_split, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[es], verbose=verbose)\n",
    "        # Prefer last val_auc if available\n",
    "        if history and 'val_auc' in history.history:\n",
    "            val_auc = float(history.history['val_auc'][-1])\n",
    "        else:\n",
    "            # Fallback: evaluate on training data (not ideal for validation, but prevents 0.0)\n",
    "            # Or just note that val_auc was missing\n",
    "            note += \"No val_auc in history. \"\n",
    "            # Let's evaluate on the training data as a very last resort (though not a true validation score)\n",
    "            loss, auc = model.evaluate(X_train_local, y_train_local, verbose=0)\n",
    "            val_auc = float(auc)\n",
    "            note += f\"Used training AUC instead: {val_auc:.4f}. \"\n",
    "    except Exception as e:\n",
    "        note = f\"error:{e}\"\n",
    "        val_auc = 0.0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Log to CSV - Ensure eval_idx is safe for CSV\n",
    "    log_row = {\n",
    "        \"eval_idx\": eval_idx if isinstance(eval_idx, (int, np.integer)) else str(eval_idx),\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units\": json.dumps(units),\n",
    "        \"dropout\": float(dropout_rate),\n",
    "        \"lr\": float(lr),\n",
    "        \"optimizer\": opt_name,\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"l2\": float(l2_reg),\n",
    "        \"activation\": activation,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "        \"note\": note\n",
    "    }\n",
    "    pd.DataFrame([log_row]).to_csv(eval_log_csv, mode='a', header=False, index=False)\n",
    "\n",
    "    # Print lightweight progress line\n",
    "    if eval_idx is not None and max_evals is not None:\n",
    "        print(f\"[Eval {eval_idx}/{max_evals}] layers={n_layers} act={activation} opt={opt_name} lr={lr:.2e} bs={batch_size} l2={l2_reg:.0e} -> val_auc={val_auc:.4f} (took {elapsed:.1f}s)\")\n",
    "\n",
    "    return float(val_auc)\n",
    "\n",
    "# ===============================\n",
    "# Define NiaPy Problem\n",
    "# ===============================\n",
    "class LSTMHyperProblem(Problem):\n",
    "    def __init__(self, X_train_local, y_train_local, max_evals=100, epochs_small=8):\n",
    "        lower = [1, 16, 16, 0.1, -5.0, 0.0, 0.0, -9.0, 0.0]\n",
    "        upper = [3, 256, 256, 0.7, -2.0, 2.0, 3.0, -2.0, 2.0]\n",
    "        super().__init__(dimension=9, lower=lower, upper=upper)\n",
    "        self.X_train_local = X_train_local\n",
    "        self.y_train_local = y_train_local\n",
    "        self.epochs_small = epochs_small\n",
    "        self.eval_count = 0\n",
    "        self.max_evals = max_evals\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        self.eval_count += 1\n",
    "        params = decode_solution(x)\n",
    "        try:\n",
    "            val_auc = build_and_eval(params, self.X_train_local, self.y_train_local,\n",
    "                                     epochs=self.epochs_small, val_split=0.15,\n",
    "                                     verbose=0, eval_idx=self.eval_count, max_evals=self.max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"[Eval {self.eval_count}] ERROR during eval: {e}\")\n",
    "            val_auc = 0.0\n",
    "        return -val_auc  # minimize negative AUC\n",
    "\n",
    "# ===============================\n",
    "# Run optimization\n",
    "# ===============================\n",
    "# Use subset of training data to speed up optimization\n",
    "take_frac = 1.0 # Increased from 0.6, adjust as needed\n",
    "idx = np.random.choice(len(X_train), size=int(len(X_train) * take_frac), replace=False)\n",
    "X_train_sub = X_train[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Settings - Increased from 10 for better search\n",
    "MAX_EVALS = 60 # Adjust as needed, e.g., 60, 120\n",
    "POP_SIZE = 15\n",
    "\n",
    "problem = LSTMHyperProblem(X_train_sub, y_train_sub, max_evals=MAX_EVALS, epochs_small=8)\n",
    "task = Task(problem=problem, max_evals=MAX_EVALS)\n",
    "algo = MonarchButterflyOptimization(population_size=POP_SIZE)\n",
    "\n",
    "print(f\"🚀 Starting MBO search: max_evals={MAX_EVALS}, pop_size={POP_SIZE}. Logs -> {eval_log_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    best_x, best_fit = algo.run(task)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization interrupted by user. Using best-so-far from task.\")\n",
    "    best_x = task.best_solution\n",
    "    best_fit = task.best_fitness\n",
    "\n",
    "best_params = decode_solution(best_x)\n",
    "\n",
    "print(\"\\n=== 🦋 MBO Result ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation AUC (approx):\", -best_fit)\n",
    "\n",
    "# ===============================\n",
    "# Retrain final model\n",
    "# ===============================\n",
    "print(\"\\n🎯 Retraining final model on full training set with best hyperparams...\")\n",
    "final_params = best_params\n",
    "final_epochs = 50  # longer training for final model\n",
    "\n",
    "# IMPORTANT: Call without eval_idx=\"final\" to avoid logging issues and simplify final train\n",
    "# We still want verbose=1 to see progress during final training\n",
    "final_val_auc = build_and_eval(final_params, X_train, y_train,\n",
    "                               epochs=final_epochs, val_split=0.15, verbose=1)\n",
    "                               # Removed: eval_idx=\"final\", max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Final short validation AUC (from build_and_eval):\", final_val_auc)\n",
    "\n",
    "# Build & train final model for real evaluation on X_test\n",
    "model = build_lstm_dynamic(final_params, (X_train.shape[1], 1))\n",
    "if final_params['optimizer'] == 'adam':\n",
    "    opt = Adam(learning_rate=final_params['lr'])\n",
    "elif final_params['optimizer'] == 'rmsprop':\n",
    "    opt = RMSprop(learning_rate=final_params['lr'])\n",
    "else:\n",
    "    opt = Nadam(learning_rate=final_params['lr'])\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.15, epochs=100, batch_size=final_params['batch_size'], callbacks=[es], verbose=1)\n",
    "\n",
    "loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Test Results: Loss={loss:.4f}, Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "model.save('mbo_best_lstm_fp_model.h5')\n",
    "print(\"💾 Saved final model to mbo_best_lstm_fp_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
